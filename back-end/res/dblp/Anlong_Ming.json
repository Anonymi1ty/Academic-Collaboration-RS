{"id": "52/3276", "name": "Anlong Ming", "Article": {"conf/aaai/TangKLLM22": {"title": "Transfer Learning for Color Constancy via Statistic Perspective.", "url": "https://ojs.aaai.org/index.php/AAAI/article/view/20135", "year": "2022", "author": {"Yuxiang Tang": "264/5178", "Xuejing Kang": "33/8493", "Chunxiao Li": "30/1926", "Zhaowen Lin": "65/10473", "Anlong Ming": "52/3276"}, "abstract": "Color Constancy aims to correct image color casts caused by scene illumination. Recently, although the deep learning approaches have remarkably improved on single-camera data, these models still suffer from the seriously insufficient data problem, resulting in shallow model capacity and degradation in multi-camera settings. In this paper, to alleviate this problem, we present a Transfer Learning Color Constancy (TLCC) method that leverages cross-camera RAW data and massive unlabeled sRGB data to support training. Specifically, TLCC consists of the Statistic Estimation Scheme (SE-Scheme) and Color-Guided Adaption Branch (CGA-Branch). SE-Scheme builds a statistic perspective to map the camera-related illumination labels into camera-agnostic form and produce pseudo labels for sRGB data, which greatly expands data for joint training. Then, CGA-Branch further promotes efficient transfer learning from sRGB to RAW data by extracting color information to regularize the backbone's features adaptively. Experimental results show the TLCC has overcome the data limitation and model degradation, outperforming the state-of-the-art performance on popular benchmarks. Moreover, the experiments also prove the TLCC is capable of learning new scenes information from sRGB data to improve accuracy on the RAW images with similar scenes.\n\t\t\t\t"}, "conf/icra/ShengXCLM22": {"title": "Monocular Depth Distribution Alignment with Low Computation.", "url": "https://doi.org/10.1109/ICRA46639.2022.9811937", "year": "2022", "author": {"Fei Sheng": "145/6237", "Feng Xue": "03/517", "Yicong Chang": "315/3985", "Wenteng Liang": "308/0557", "Anlong Ming": "52/3276"}, "abstract": " Abstract:The performance of monocular depth estimation generally depends on the amount of parameters and computational cost. It leads to a large accuracy contrast between light-weight networks and heavy-weight networks, which limits their application in the real world. In this paper, we model the majority of accuracy contrast between them as the difference of depth distribution, which we call 'Distribution drift'. To this end, a distribution alignment network (DANet) is proposed. We firstly design a pyramid scene transformer (PST) module to capture inter-region interaction in multiple scales. By perceiving the difference of depth features between every two regions, DANet tends to predict a reasonable scene structure, which fits the shape of distribution to ground truth. Then, we propose a local-global optimization (LGO) scheme to realize the supervision of global range of scene depth. Thanks to the alignment of depth distribution shape and scene depth range, DANet sharply alleviates the distribution drift, and achieves a comparable performance with prior heavy-weight methods, but uses only 1% floating-point operations per second (FLOPs) of them. The experiments on two datasets, namely the widely used NYUDv2 dataset and the more challenging iBims-1 dataset, demonstrate the effectiveness of our method. The source code is available at https://github.com/YiLiM1/DANet."}, "conf/icra/ChangXSLM22": {"title": "Fast Road Segmentation via Uncertainty-aware Symmetric Network.", "url": "https://doi.org/10.1109/ICRA46639.2022.9812452", "year": "2022", "author": {"Yicong Chang": "315/3985", "Feng Xue": "03/517", "Fei Sheng": "145/6237", "Wenteng Liang": "308/0557", "Anlong Ming": "52/3276"}, "abstract": " Abstract:The high performance of RGB-D based road segmentation methods contrasts with their rare application in commercial autonomous driving, which is owing to two reasons: 1) the prior methods cannot achieve high inference speed and high accuracy in both ways; 2) the different properties of RGB and depth data are not well-exploited, limiting the reliability of predicted road. In this paper, based on the evidence theory, an uncertainty-aware symmetric network (USNet) is proposed to achieve a trade-off between speed and accuracy by fully fusing RGB and depth data. Firstly, cross-modal feature fusion operations, which are indispensable in the prior RGB-D based methods, are abandoned. We instead separately adopt two light-weight subnetworks to learn road representations from RGB and depth inputs. The light-weight structure guarantees the real-time inference of our method. Moreover, a multi-scale evidence collection (MEC) module is designed to collect evidence in multiple scales for each modality, which provides sufficient evidence for pixel class determination. Finally, in uncertainty-aware fusion (UAF) module, the uncertainty of each modality is perceived to guide the fusion of the two sub-networks. Experimental results demonstrate that our method achieves a state-of-the-art accuracy with real-time inference speed of \n43+43+\n FPS. The source code is available at https://github.com/morancyc/USNet."}, "conf/ijcai/HeZXJM22": {"title": "Rethinking Image Aesthetics Assessment: Models, Datasets and Benchmarks.", "url": "https://doi.org/10.24963/ijcai.2022/132", "year": "2022", "author": {"Shuai He": "87/7883", "Yongchang Zhang": "119/3836", "Rui Xie": "86/2228", "Dongxiang Jiang": "16/1096", "Anlong Ming": "52/3276"}, "abstract": ""}, "conf/ijcai/ZhangKM22": {"title": "Domain Adversarial Learning for Color Constancy.", "url": "https://doi.org/10.24963/ijcai.2022/236", "year": "2022", "author": {"Zhifeng Zhang": "03/2567", "Xuejing Kang": "33/8493", "Anlong Ming": "52/3276"}, "abstract": ""}, "conf/iccv/FengSZLZFWLKM21": {"title": "MT-ORL: Multi-Task Occlusion Relationship Learning.", "url": "https://doi.org/10.1109/ICCV48922.2021.00923", "year": "2021", "author": {"Panhe Feng": "254/2722", "Qi She": "171/7773", "Lei Zhu": "99/549", "Jiaxin Li": "69/327", "Lin Zhang": "37/1629", "Zijian Feng": "45/10114", "Changhu Wang": "30/3393", "Chunpeng Li": "78/5072", "Xuejing Kang": "33/8493", "Anlong Ming": "52/3276"}, "abstract": " Abstract:Retrieving occlusion relation among objects in a single image is challenging due to sparsity of boundaries in image. We observe two key issues in existing works: firstly, lack of an architecture which can exploit the limited amount of coupling in the decoder stage between the two subtasks, namely occlusion boundary extraction and occlusion orientation prediction, and secondly, improper representation of occlusion orientation. In this paper, we propose a novel architecture called Occlusion-shared and Path-separated Network (OPNet), which solves the first issue by exploiting rich occlusion cues in shared high-level features and structured spatial information in task-specific low-level features. We then design a simple but effective orthogonal occlusion representation (OOR) to tackle the second issue. Our method surpasses the state-of-the-art methods by 6.1%/8.3% Boundary-AP and 6.5%/10% Orientation-AP on standard PIOD/BSDS ownership datasets. Code is available at https://github.com/fengpanhe/MT-ORL."}, "conf/mm/ZhangZLCM21": {"title": "HDA-Net: Horizontal Deformable Attention Network for Stereo Matching.", "url": "https://doi.org/10.1145/3474085.3475273", "year": "2021", "author": {"Qi Zhang": "52/323", "Xuesong Zhang": "85/2286", "Baoping Li": "13/6460", "Yuzhong Chen": "32/3775", "Anlong Ming": "52/3276"}, "abstract": "\n\t\tStereo matching is a fundamental and challenging task which has various applications in autonomous driving, dense reconstruction and other depth related tasks. Contextual information with discriminative features is crucial for accurate stereo matching in the ill-posed regions (textureless, occlusion, etc.). In this paper, we propose an efficient horizontal attention module to adaptively capture the global correspondence clues. Compared with the popular non-local attention, our horizontal attention is more effective for stereo matching with better performance and lower consumption of computation and memory. We further introduce a deformable module to refine the contextual information in the disparity discontinuous areas such as the boundary of objects. Learning-based method is adopted to construct the cost volume by concatenating the features of two branches. In order to offer explicit similarity measure to guide learning-based volume for obtaining more reasonable unimodal matching cost distribution we additionally combine the learning-based volume with the improved zero-centered group-wise correlation volume. Finally, we regularize the 4D joint cost volume by a 3D CNN module and generate the final output by disparity regression. The experimental results show that our proposed HDA-Net achieves the state-of-the-art performance on the Scene Flow dataset and obtains competitive performance on the KITTI datasets compared with the relevant networks.\n\t"}, "conf/icmcs/LiZJCZM20": {"title": "High Accuracy Compressive Chromo-Tomography Reconstruction via Convolutional Sparse Coding.", "url": "https://doi.org/10.1109/ICME46284.2020.9102835", "year": "2020", "author": {"Baoping Li": "13/6460", "Xuesong Zhang": "85/2286", "Jing Jiang": "68/1974", "Yuzhong Chen": "32/3775", "Qi Zhang": "52/323", "Anlong Ming": "52/3276"}, "abstract": " Abstract:Over the last decade various compressive snapshot hyperspectral imaging methods have been proposed. The limited reconstruction quality from severely compressed measurements, however, has been a practical barrier to real applications. This paper proposes a compressive chromo-tomography framework that incorporates the convolutional sparse coding (CSC) prior into the classical total variation and L\n1\n regularization functionals. Such a combination allows excellent high-frequency recovery capabilities of CSC, while effectively suppressing ghost artifacts in tomographic reconstructions. Since nondifferentiable regularizers are employed, we propose a preconditioned alternating direction method of multipliers (ADMM) for flexible and efficient solutions, both for the reconstruction task and for hyperspectral convolutional dictionary learning. We demonstrate in our numerical experiments that just 25 learned 3D CSC filters can fulfill a rather effective hyperspectral imagery representation and that the proposed method is capable of high accuracy reconstructions."}, "conf/icpr/ZhangKM20": {"title": "BP-net: deep learning-based superpixel segmentation for RGB-D image.", "url": "https://doi.org/10.1109/ICPR48806.2021.9412929", "year": "2020", "author": {"Bin Zhang": "13/5236", "Xuejing Kang": "33/8493", "Anlong Ming": "52/3276"}, "abstract": " Abstract:In this paper, we propose a deep learning-based su-perpixel segmentation algorithm for RGB-D image. The proposed deep neural network called BP-net is composed of boundary detection network (B-net) that exploits multiscale information from depth image to extract the geometry edge of objects, and pixel labeling network (P-net) that extracts pixel features and generates superpixels. A boundary pass filter is proposed to combine the edge information and pixel features and ensures superpixels adhere better to geometry edges. To generate regular superpixels, we design a loss function which takes the shape regularity error and superpixel accuracy into account. In addition, for providing reasonable initial seeds, a new seeds initialization strategy is proposed, in which the density of seeds is investigated from a 2-manifolds space to reduce the number of superpixels that cover multiple objects in the region of rich texture. Experimental results demonstrate that our algorithm outperforms the existing state-of-the-art algorithms in terms of accuracy and shape regularity on the RGB-D dataset."}, "conf/icassp/DuanMZK19": {"title": "A Novel Super-resolution Method Based on Patch Reconstruction with Simk Clustering and Nonlinear Mapping.", "url": "https://doi.org/10.1109/ICASSP.2019.8683656", "year": "2019", "author": {"Peiqi Duan": "214/9772", "Anlong Ming": "52/3276", "Xuesong Zhang": "85/2286", "Xuejing Kang": "33/8493"}, "abstract": " Abstract:In this paper, we propose a patch-wise super-resolution (SR) method that combines an external-sample classification tree and a nonlinear-mapping learning stage to simultaneously guarantees reconstruction quality and speed at the stage of patch representation and mapping. We use the low-resolution (LR) to high-resolution (HR) mapping kernel of each patch-pair sample (called SIMK) to complete classification by binary tree branching and provide reasonable training sets for mapping-learning. Then a high accuracy but low cost lightweight network is learned for each tree node to choose the reasonable branch path for the testing LR patches. In the mapping-learning stage, the nonlinear mapping for each class is represented as a full-connected network, which provides satisfying generalization ability for LR patch reconstruction. Comparing with state-of-the-art methods, our approach achieves real-time (>24fps) SR of realistic vision and high quality for different upscaling factors."}, "conf/icassp/Zhang0MKA19": {"title": "Spatio-spectral Modulation Using a Binary Photomask for Compressive Chromotomography.", "url": "https://doi.org/10.1109/ICASSP.2019.8682609", "year": "2019", "author": {"Xuesong Zhang": "85/2286", "Jing Jiang 0017": "68/1974-17", "Anlong Ming": "52/3276", "Xuejing Kang": "33/8493", "Gonzalo R. Arce": "78/1427"}, "abstract": " Abstract:Recent advances in compressive spectral imagers have demonstrated the potential of spatio-spectral modulation (SSM) for improved reconstruction performance. Existing SSM techniques, however, use either a color filter array or a complex optical arrangement, both of which can only provide limited modulation bandwidth in the spectral dimension. This paper proposes a practical SSM method to help address the \"missing cone\" problem of chromo-tomography. A high-resolution binary coded aperture is used to modulate the dispersed images, which in the Fourier domain fulfills a 3D convolution of the probed spectrum with the aperture's wide spectrum. This spectrum spreading process facilitates the compressed sensing strategy determined by the Fourier Slice Theorem and we demonstrate the advantages of the proposed approach with numerical experiments."}, "conf/iccv/LuXZM019": {"title": "Occlusion-Shared and Feature-Separated Network for Occlusion Relationship Reasoning.", "url": "https://doi.org/10.1109/ICCV.2019.01044", "year": "2019", "author": {"Rui Lu": "33/2451", "Feng Xue": "03/517", "Menghan Zhou": "226/2572", "Anlong Ming": "52/3276", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:Occlusion relationship reasoning demands closed contour to express the object, and orientation of each contour pixel to describe the order relationship between objects. Current CNN-based methods neglect two critical issues of the task: (1) simultaneous existence of the relevance and distinction for the two elements, i.e, occlusion edge and occlusion orientation; and (2) inadequate exploration to the orientation features. For the reasons above, we propose the Occlusion-shared and Feature-separated Network (OFNet). On one hand, considering the relevance between edge and orientation, two sub-networks are designed to share the occlusion cue. On the other hand, the whole network is split into two paths to learn the high semantic features separately. Moreover, a contextual feature for orientation prediction is extracted, which represents the bilateral cue of the foreground and background areas. The bilateral cue is then fused with the occlusion cue to precisely locate the object regions. Finally, a stripe convolution is designed to further aggregate features from surrounding scenes of the occlusion edge. The proposed OFNet remarkably advances the state-of-the-art approaches on PIOD and BSDS ownership dataset."}, "conf/icip/LiZLM19": {"title": "ACPNP: an Efficient Solution for Absolute Camera Pose Estimation from Two Affine Correspondences.", "url": "https://doi.org/10.1109/ICIP.2019.8803834", "year": "2019", "author": {"Dong Li": "47/4826", "Xuesong Zhang": "85/2286", "Hengsong Li": "254/8115", "Anlong Ming": "52/3276"}, "abstract": " Abstract:In this paper, a novel algorithm to estimate the absolute camera pose is proposed using two affine correspondences (ACs). Exploring the relationship between the affine transformation and the projection equation, six linear constraints are derived, and only two ACs are sufficient to recover the pose. Even though perspective cameras are assumed, the constraints can straightforwardly be generalized to other camera models since they describe the relationship between local affinities and projection. Benefiting from the requirement of less correspondences, the proposed algorithm needs less sampling times when robust estimators like RANSAC are applied, and still performs stably with rather limited number of correspondences. For the improvement of robustness, the affine transformation is further optimized via photometric and epipolar constraints. The proposed method was validated on both synthetic and real-world datasets, which demonstrates that the proposed method yields results superior to the state-of-the-art in terms of accuracy."}, "conf/icip/WangZLM19": {"title": "Real-Time Light Field Depth Estimation via GPU-Accelerated Muti-View Semi-Global Matching.", "url": "https://doi.org/10.1109/ICIP.2019.8803400", "year": "2019", "author": {"Yuanqi Wang": "227/4195", "Xuesong Zhang": "85/2286", "Hengsong Li": "254/8115", "Anlong Ming": "52/3276"}, "abstract": " Abstract:The structured and redundant imagery of light field cameras can provide more robust depth estimation results while on the other hand demands a huge computation power, which limits its real-time applications, such as online industrial monitoring, 3D endoscopic surgery etc. This paper extends the classical SGM(Semi-global matching) [1] algorithm to light filed multi-view stereo framework, which can acquire sub-pixel level disparity estimation to cope with the micro-baseline of light field cameras. The whole algorithm is tailored for parallelization on GPU exploiting multi-stream asynchronization, multi-thread allocation, and multi-type memory management. The results show that our method’s execution time is less than 50ms on NVIDIA 1080Ti, which to our knowledge is the fastest among reported geometry based methods while keeping a comparative accuracy performance."}, "conf/icip/YeZKM19": {"title": "Adaptive Occlusion Boundary Extraction for Depth Inference.", "url": "https://doi.org/10.1109/ICIP.2019.8803427", "year": "2019", "author": {"Lizhu Ye": "254/2716", "Lei Zhu": "99/549", "Xuejing Kang": "33/8493", "Anlong Ming": "52/3276"}, "abstract": " Abstract:In this paper, we propose an adaptive occlusion boundary extraction method for depth inference based on an adaptive segmentation and classification. First, an Adaptive DRW is proposed to generate more precise seeds and adaptive segmentation results, which can improve the feature quality and lower the boundary imbalance degree. Then, to deal with the imbalanced classification, we design a cost-sensitive boosting method-Adaptive AdaCost to better classify the imbalanced boundary, which can further improve overall performance and lower the cumulative misclassification cost and cost upper bound. Benefited from our Adaptive DRW and AdaCost, we extract more reliable and precise occlusion boundaries and use them for depth inference. The experiment results demonstrate that the combination of our Adaptive DRW and Adaptive AdaCost can produce more precise occlusion boundaries, and the depth inference result with our occlusion boundaries can be greatly improved."}, "conf/icip/LiZWM19": {"title": "A Unified Unsupervised Learning Framework for Stereo Matching and Ego-Motion Estimation.", "url": "https://doi.org/10.1109/ICIP.2019.8803550", "year": "2019", "author": {"Hengsong Li": "254/8115", "Xuesong Zhang": "85/2286", "Yuanqi Wang": "227/4195", "Anlong Ming": "52/3276"}, "abstract": " Abstract:Learning to estimate depth and ego-motion from video sequences via deep convolutional networks is attracting significant attention for potentially wide computer vision applications. Most prior work in unsupervised depth learning use monocular video sequences as the input of their networks. However, their results need a scale factor that is computed frame-to-frame to maintain a stable relative scale. In this paper, we propose an unsupervised learning framework for the task of joint depth and ego-motion estimation from stereo sequences. The usage of stereo sequences can provide both spatial (left to right) and temporal (forward to back-ward) photometric warping constrains for supervised learning and allow for an absolute scale factor for the scene depth and camera pose, which is of great significance for vision guidance. Experiments on the KITTI driving dataset reveal that our framework outperforms state-of-the-art results employing unsupervised neural networks."}, "conf/icmcs/LuZMZ19": {"title": "Context-Constrained Accurate Contour Extraction for Occlusion Edge Detection.", "url": "https://doi.org/10.1109/ICME.2019.00263", "year": "2019", "author": {"Rui Lu": "33/2451", "Menghan Zhou": "226/2572", "Anlong Ming": "52/3276", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:Occlusion edge detection requires both accurate locations and context constraints of the contour. Existing CNN-based pipeline does not utilize adaptive methods to filter the noise introduced by low-level features. To address this dilemma, we propose a novel Context-constrained accurate Contour Extraction Network (CCENet). Spatial details are retained and contour-sensitive context is augmented through two extraction blocks, respectively. Then, an elaborately designed fusion module is available to integrate features, which plays a complementary role to restore details and remove clutter. Weight response of attention mechanism is eventually utilized to enhance occluded contours and suppress noise. The proposed CCENet significantly surpasses state-of-the-art methods on PIOD and BSDS ownership dataset of object edge detection and occlusion orientation detection."}, "conf/icra/XueMZZ19": {"title": "A Novel Multi-layer Framework for Tiny Obstacle Discovery.", "url": "https://doi.org/10.1109/ICRA.2019.8794279", "year": "2019", "author": {"Feng Xue": "03/517", "Anlong Ming": "52/3276", "Menghan Zhou": "226/2572", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:For tiny obstacle discovery in a monocular image, edge is a fundamental visual element. Nevertheless, because of various reasons, e.g., noise and similar color distribution with background, it is still difficult to detect the edges of tiny (b) obstacles at long distance. In this paper, we propose an obstacle-aware discovery method to recover the missing contours of these obstacles, which helps to obtain obstacle proposals as much as possible. First, by using visual cues in monocular images, several multi-layer regions are elaborately inferred to reveal the distances from the camera. Second, several novel obstacle-aware occlusion edge maps are constructed to well capture the contours of tiny obstacles, which combines cues from each layer. Third, to ensure the existence of the tiny obstacle proposals, the maps from all layers are used for proposals extraction. Finally, based on these proposals containing tiny obstacles, a novel obstacle-aware regressor is proposed to generate an obstacle occupied probability map with high confidence. The convincing experimental results with comparisons on the Lost and Found dataset demonstrate the effectiveness of our approach, achieving around 9.5% improvement on the accuracy than FPHT and PHT, it even gets comparable performance to MergeNet. Moreover, our method outperforms the state-of-the-art algorithms and significantly improves the discovery ability for tiny obstacles at long distance."}, "conf/accv/ZhuKMZ18": {"title": "Dynamic Random Walk for Superpixel Segmentation.", "url": "https://doi.org/10.1007/978-3-030-20876-9_34", "year": "2018", "author": {"Lei Zhu": "99/549", "Xuejing Kang": "33/8493", "Anlong Ming": "52/3276", "Xuesong Zhang": "85/2286"}, "abstract": "In this paper, we present a novel Random Walk model called Dynamic Random Walk (DRW) for superpixel segmentation. The proposed DRW adds a new type of node called dynamic node to enrich the features of labels and reduce redundant calculation. By greedily optimizing the Weighted Random Walk Entropy (WRWE), our DRW can consider the features of both seed nodes and dynamic nodes, which enhances the boundary adherence. In addition, a new seed initialization strategy, which can evenly distribute seed nodes in both 2D and 3D space, is proposed to extend our DRW for superpixel segmentation. With this strategy, our DRW can generate superpixels in only one iteration without updating seed nodes. The experiment results show that our DRW is faster than existing RW models, and better than the state-of-the-art superpixel segmentation algorithms in both efficiency and the performance."}, "conf/icip/ZhouMMZ18": {"title": "Objectness-Aware Tracking via Double-Layer Model.", "url": "https://doi.org/10.1109/ICIP.2018.8451130", "year": "2018", "author": {"Menghan Zhou": "226/2572", "Jianxiang Ma": "177/6660", "Anlong Ming": "52/3276", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:The prediction drifts to the non-object backgrounds is a critical issue in conversional correlation filter (CF) based trackers. The key insight of this paper is to propose a double-layer model to address this problem. Specifically, the first layer is a CF tracker, which is employed to predict a rough position of the target, and the objectness layer, which is regarded as the second layer, is utilized to reveal the object characteristics of the predicted target. The novel objectness layer firstly constructs a set of target-related object proposals, which satisfy both the spatial and temporal constraints. And then an objectness classifier is learned upon the proposal set to best separate the target from the noise background proposals. The convincing experimental results on the challenging OTB100 and TC128 dataset demonstrate the effectiveness of the presented approach."}, "conf/icpr/ZhouMMB18": {"title": "Learning Training Samples for Occlusion Edge Detection and Its Application in Depth Ordering Inference.", "url": "https://doi.org/10.1109/ICPR.2018.8545622", "year": "2018", "author": {"Yu Zhou 0016": "36/2728-16", "Jianxiang Ma": "177/6660", "Anlong Ming": "52/3276", "Xiang Bai": "59/2741"}, "abstract": " Abstract:This paper studies the problem of occlusion edge detection, which is applied to infer the depth order of objects in a monocular image. The key observation is that, given the fixed regression objective, the accuracy of occlusion edge detection is effectively boosted by selecting appropriate training samples in a discriminative feature subspace. Specifically, the ℓ\n1\n-regularized logistic regression is employed to learn a more sparse yet discriminative feature subspace, while the training sample selection is formulated as a quadratic optimization with the robust Huber loss. The presented formulation avoids the noises efficiently, and hence the desirable occlusion edges can be detected. We validate the effectiveness of our approach on depth order inference problem. Experiments are conducted on two famous datasets, i.e., the Cornell depth-order dataset and the NYU2 dataset. Promising results demonstrate the superiority of our approach over the state-of-the-art approaches."}, "conf/icpr/DuanMKY18": {"title": "A New Single Image Super-resolution Method Using SIMK-based Classification and ISRM Technique.", "url": "https://doi.org/10.1109/ICPR.2018.8546087", "year": "2018", "author": {"Peiqi Duan": "214/9772", "Anlong Ming": "52/3276", "Xuejing Kang": "33/8493", "Chao Yao": "99/6747"}, "abstract": " Abstract:Single image super-resolution (SR) technique is widely used to estimate high-resolution (HR) images from low-resolution (LR) ones. As a research hotspot, many example-based SR methods achieve superior results by learning class-mapping-kernels from classified external LR-HR patch-pair samples. However, in these methods, the classification of samples is generally based on the features of LR patch, and the interference of ill-samples to learn class-mapping-kernels is ignored as well. In this paper, we propose a new SR method with Sample Individual Mapping-Kernel (SIMK) based classification and Ill-Sample Removal Mechanism (ISRM) in learning LR-HR mapping. In the proposed sample classification, we use the SIMK feature which is the LR-to-HR mapping kernel of each sample, to classify samples and obtain more reasonable sample sets for mapping-learning. To prevent overfitting and reduce the complexity of SIMK-based-classification, samples are pre-categorized by relative pixel values of LR patch. In the mapping-learning process, the ill-samples which are far away from the classification center are removed to improve the validity of class-mapping-kernels. In addition, for each testing LR patch, the optimal class is assigned reasonably based on a probabilistic decision model learned from Naive Bayes Classifier. Comparing with state-of-the-art methods, our SR method achieves both visual and performance improvement."}, "conf/igta/SunMYK18": {"title": "Fall Detection System Based on Mobile Robot.", "url": "https://doi.org/10.1007/978-981-13-1702-6_26", "year": "2018", "author": {"Pengfei Sun": "88/6707", "Anlong Ming": "52/3276", "Chao Yao": "99/6747", "Xuejing Kang": "33/8493"}, "abstract": "This paper proposed an accurate fall detection algorithm based on the feature of whole human body. The feature is extracted from convolutional neural network. The implementation of algorithm is integrated into a hardware system based on a visual mobile robot platform. To ensure the robustness and flexibility of algorithm in actual situation, a set of systemic strategies was applied on mobile robot. Finally, sufficient experiments on public dataset were conduct on our algorithm. Moreover, in a real indoor scene, experiment results proved the efficiency and precision of the designed fall detection system.KeywordsFall detectionConvolutional neural networkMobile robotDeep learning"}, "conf/es/FengMYZ17": {"title": "Learning Action Pattern for Activity Recognition.", "url": "https://doi.org/10.1109/ES.2017.9", "year": "2017", "author": {"Jingyi Feng": "174/4737", "Anlong Ming": "52/3276", "Chao Yao": "99/6747", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:This paper studies the challenging activity recognition task and a novel discriminative action pattern is proposed. We construct action cube pool and propose a novel scheme to represent action cubes as action patterns. Candidates of discriminative patterns are selected among action patterns according to motion energy of corresponding action cube. Spatial constraint clustering algorithm clusters these candidates into several disjoint clusters. Discriminative patterns are learned with the help of negative patterns and multi-class SVM. The presented approach is validated on the MSRAction3D dataset and the MSRDailyActivity3D dataset. The experimental results prove the effectiveness of our approach."}, "conf/iccv/MaMHWZ17": {"title": "Object-Level Proposals.", "url": "https://doi.org/10.1109/ICCV.2017.527", "year": "2017", "author": {"Jianxiang Ma": "177/6660", "Anlong Ming": "52/3276", "Zilong Huang": "185/9199", "Xinggang Wang": "95/3056", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:Edge and surface are two fundamental visual elements of an object. The majority of existing object proposal approaches utilize edge or edge-like cues to rank candidates, while we consider that the surface cue containing the 3D characteristic of objects should be captured effectively for proposals, which has been rarely discussed before. In this paper, an object-level proposal model is presented, which constructs an occlusion-based objectness taking the surface cue into account. Specifically, the better detection of occlusion edges is focused on to enrich the surface cue into proposals, namely, the occlusion-dominated fusion and normalization criterion are designed to obtain the approximately overall contour information, to enhance the occlusion edge map at utmost and thus boost proposals. Experimental results on the PASCAL VOC 2007 and MS COCO 2014 dataset demonstrate the effectiveness of our approach, which achieves around 6% improvement on the average recall than Edge Boxes at 1000 proposals and also leads to a modest gain on the performance of object detection."}, "conf/icce-tw/DangSM16": {"title": "Chinese paper-cut generated from human portrait.", "url": "https://doi.org/10.1109/ICCE-TW.2016.7520943", "year": "2016", "author": {"Kun Dang": "287/6732", "Guiling Song": "307/4040", "Anlong Ming": "52/3276"}, "abstract": " Abstract:This paper presents a software package that renders artistic paper-cut of human portrait. It is an application software package suitable for the mobile devices, e.g., the iPad. The face fidelity is an essential factor in creating a portrait, while in the traditional algorithms, rendering portrait paper-cut is considerd as an inhomogeneous binarization process. Specifically, the binarization process is inhomogeneous for different facial areas, which always cannot correspond strictly to the grayscale, and lead to the edge being unsmoothed. In order to address this issue and make the portrait more similar to the person shown in a photograph, this paper presents an improved paper-cut algorithm. We first use a dynamic thresholding method to binarize the facial components, such as nose, eyes. Then, the principle of temple matching is employed to find a template for every facial component from a pre-prepared template library, which is closest to the real component. In addition, the software package on the iPad is not a simple migration from that on PC. Compared with the software on PC, our package has a friendly interface and is easier for operating. Experimental results show that we can get a more realistic portrait paper-cut using the package on the iPad."}, "conf/indin/MingLRPT16": {"title": "A energy efficient multi-dimension model for system control in smart environment systems.", "url": "https://doi.org/10.1109/INDIN.2016.7819297", "year": "2016", "author": {"Anlong Ming": "52/3276", "Hong Luo": "67/876", "Yanchen Ren": "179/8927", "Zhibo Pang": "75/9776", "Kim Fung Tsang": "39/1442"}, "abstract": " Abstract:A smart environment system should automatically control the devices according to the sensing information and users' requirements so as to keep the environmental elements (e.g., temperature, light) within the desired range. System control with minimum power is one key issue in such a system. In this paper, we propose a multi-dimension model for system control. In this model, each environmental element is abstracted into a dimension, such that a service with conditions and targets can be formulated as a multi-dimensional service space, and a smart environment with many services may map to a comprehensive multi-dimensional service space through space computation. Based on this model, we propose a minimum power adjustment algorithm for energy-efficient scheduling in smart environment, which transforms the optimal control problem into the problem of the shortest weighted distance of point-to-polygonal in multi-dimensional space. Theoretical analysis and experimental results show that the proposed model is effective and efficient in energy-efficient system control. It is important to point out that the proposed algorithms are scalable when the number of dimensions or services increases."}, "conf/isie/ZhouMS16": {"title": "Single target tracking in smart glass by reliable point set.", "url": "https://doi.org/10.1109/ISIE.2016.7745048", "year": "2016", "author": {"Yu Zhou 0016": "36/2728-16", "Anlong Ming": "52/3276", "Yanfei Shen": "43/3659"}, "abstract": " Abstract:This paper aims at presenting a simple and efficient single target tracking approach, which is suitable for the applications for modern smart glasses. Specifically, a novel set-to-set similarity measurement is proposed, which takes the critical appearance cues, the contextual structure of the points inside the set and the points' coordinate related to the bounding box into consideration jointly. In contrast to the conventional holistic template matching based tracking approaches, a set of reliable feature points is employed to express the target precisely, and we incorporate the feature selection and similarity measure to an uniform framework. In addition, an automatic deterministic scale selection method is also utilized. Extensive experiments validate the accuracy and robustness of the proposed method, and demonstrate the improved performance on handling occlusions, clutter, and changes of appearance."}, "conf/wcsp/ZhouM16": {"title": "Semi-Supervised Multiple Instance Learning and its application in visual tracking.", "url": "https://doi.org/10.1109/WCSP.2016.7752532", "year": "2016", "author": {"Yu Zhou 0016": "36/2728-16", "Anlong Ming": "52/3276"}, "abstract": " Abstract:In this paper, a novel Semi-Supervised Multiple Instance Learning (Semi-MIL) approach is presented. Compared with conventional approaches, we utilize a kind of “bag of instances” representation in the semi-supervised learning process, which provides an effective way to use the unlabeled data in multiple instance learning problem. We formulate the problem with a graph model based on the Minimax kernel. In addition, the Semi-MIL algorithm is readily applied for visual tracking, which can resolve the ambiguities during the tracking process. The presented approach is validated on several benchmark videos for visual tracking and MUSKs dataset for classification, the competitive experimental results demonstrate the effectiveness of our approach."}, "conf/icip/MingXNGZ15": {"title": "Learning discriminative occlusion feature for depth ordering inference on monocular image.", "url": "https://doi.org/10.1109/ICIP.2015.7351257", "year": "2015", "author": {"Anlong Ming": "52/3276", "Baofeng Xun": "172/9915", "Jia Ni": "65/2653", "Mingfei Gao": "67/6825", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:In this paper, a novel depth ordering inference approach is presented. Our main insight is to integrate the discriminative feature selection, occlusion feature learning and same-layer (S-L) relationship judgement into a uniform sparsity based classification objective, which cannot only supply the precise segmentation for the occlusion edge, but also reduce the solution space for the depth ordering inference efficiently. In addition, a novel triple descriptor is adopted to judge the foreground relationship, which is more discriminative than conversional local cues and can further reduce the solution space. The inference is executed by finding a valid path on a directed graph model. We validate our approach on the Cornell depth-order dataset and the NYU 2 dataset, and the convincing experimental results demonstrate the effectiveness of our approach."}, "conf/isvc/MingLLY14": {"title": "Real-Time 3D Reconstruction of Traffic Scenes Under an Images-to-Model Framework.", "url": "https://doi.org/10.1007/978-3-319-14249-4_73", "year": "2014", "author": {"Anlong Ming": "52/3276", "Liang Liu 0001": "10/6178-1", "Pengjie Li": "88/5445", "Qin Yang": "41/6432"}, "abstract": "Timely extraction and intuitive display of vehicle information are two challenges for intelligent traffic surveillance. This paper presents a system for real-time 3D reconstruction of traffic scenes from video frames. We first propose a images-to-model (I2M) framework for images based 3D modeling. Under this framework, we take the traffic scene model apart into the static background model and dynamic vehicle models. Compared to the conventional multi-view stereo methods for object reconstruction, we design a scheme, which uses combination of vehicle information extraction and vehicle model retrieval, to reconstruct the moving vehicles in real-time. We further study how to integrate the retrieved vehicle models into the static background model to generate the desired traffic scene model. We also evaluate our system using three real traffic scenes, and the experimental results show that our proposed methods are effective.KeywordsVehicle ModelModel RetrievalConcave RegionScene ModelConvex PointThese keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves."}, "conf/spac/LiZM14": {"title": "Integrating joint and surface for human action recognition in indoor environments.", "url": "https://doi.org/10.1109/SPAC.2014.6982665", "year": "2014", "author": {"Qingyang Li": "70/11398", "Yu Zhou 0016": "36/2728-16", "Anlong Ming": "52/3276"}, "abstract": " Abstract:Action recognition has a long research history, despite several contributed approaches have been introduced, it remains a challenging task in computer vision. In this paper, we present a uniform fusion framework for action recognition, which integrates not only the local depth cues but also the global depth cues. Firstly, the action recognition task is formulated as the maximize the posterior probability, and then the observation for the original action is decomposed into the sub-observations for each individual feature representation strategy of the original action. For the local depth cues, the joints inside the human skeleton is employed to model the local variation of the human motion. In addition, the normal of the depth surface is utilized as the global cue to capture the holistic structure of the human motion. Rather than using the original feature directly, the support vector machine model learning both the discriminative local cue (i.e., the joint) and the discriminative global cue (i.e., the depth surface), respectively. The presented approach is validated on the famous MSR Daily Activity 3D Dataset. And the experimental results demonstrate that our fusion approach can outperform the baseline approaches."}, "conf/icpr/LeiWPMY12": {"title": "Robust tracking by accounting for hard negatives explicitly.", "url": "https://ieeexplore.ieee.org/document/6460578/", "year": "2012", "author": {"Peng Lei": "22/8752", "Tianfu Wu": "08/4148", "Mingtao Pei": "77/7398", "Anlong Ming": "52/3276", "Zhenyu Yao": "126/0951"}, "abstract": " Abstract:In this paper, we present a method of robust tracking by accounting for hard negatives (i.e., distractors) of the tracking target explicitly. Our method extends the recently proposed Tracking-Learning-Detection (TLD) approach [7] in two aspects: (i) When learning the on-line fern detector, instead of using a set of features which are first randomly generated and then fixed throughout the tracking, we utilize a feature selection stage which constantly improves the performance of the detector, especially in tracking articulated objects (e.g., pedestrians); (ii) To address the diversity of distractors, instead of tracking a target against the whole set of collected negative examples, we account for the hard negatives explicitly, so that tracking drifts are largely prevented when multiple resembled targets appear in videos (e.g., people with white skirts and jeans). Experiments on a series of diverse videos show that our method outperforms TLD."}, "conf/infocom/LiuMMZ12": {"title": "A binary-classification-tree based framework for distributed target classification in multimedia sensor networks.", "url": "https://doi.org/10.1109/INFCOM.2012.6195802", "year": "2012", "author": {"Liang Liu 0001": "10/6178-1", "Anlong Ming": "52/3276", "Huadong Ma": "04/6217", "Xi Zhang 0005": "87/1222-5"}, "abstract": " Abstract:With rapid improvements and miniaturization in hardware, sensor nodes equipped with acoustic and visual information collection modules promise an unprecedented opportunity for target surveillance applications. This paper investigates a critical task of target surveillance, multi-class classification, in distributed multimedia sensor networks. We first analyze the procedure of target classification utilizing the acoustic and visual information. Then, we propose a binary classification tree based framework for distributed target classification in multimedia sensor networks. The proposed framework includes three main components: Generation of binary classification tree, Division of binary classification tree, and Selection of multimedia sensor nodes. Finally, we conduct an experimental application of target classification and extensive simulations to validate and evaluate our proposed framework and related schemes."}, "conf/icip/LiMM11": {"title": "View-based 3D model retrieval using two-level spatial structure.", "url": "https://doi.org/10.1109/ICIP.2011.6116511", "year": "2011", "author": {"Pengjie Li": "88/5445", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": " Abstract:Recently, the view-based 3D model retrieval methods have received great research attentions. However, these methods are difficult to preserve the spatial structure of 3D models. In this paper, we propose a novel view-based 3D model retrieval method to solve this problem. Our method is based on the two-level (3D model-level and 2D image-level) spatial structure. Firstly, we extract the spatial structure circular descriptor (SSCD) images from 3D models. The SSCD images can preserve the spatial structure on the 3D model-level. Then, we modify the bag-of-features (BOF) method to extract view-based features from these SSCD images. The modified BOF method can preserve the spatial structure on the 2D image-level. Finally, we calculate the similarity between the query model and the models in the databases by adapting the earth mover distance method. Experimental results show that our method can achieve satisfactory retrieval performance for both the articulated models and the rigid models."}, "conf/icmcs/FuMM11": {"title": "EGMM: An enhanced Gaussian mixture model for detecting moving objects with intermittent stops.", "url": "https://doi.org/10.1109/ICME.2011.6012011", "year": "2011", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": " Abstract:Moving object detection is one of the most important tasks in intelligent visual surveillance systems. Gaussian Mixture Model (GMM) has been most widely used for moving object detection, because of its robustness to variable scenes. However, to the best of our knowledge, existing GMM based methods can not detect moving objects which gradually stop and keep still state for a while. In this paper, we present an Enhanced Gaussian MixtureModel, called EGMM, to handle this problem. We integrate an Initial Gaussian Background Model (IGBM) and an extended Kalman filter based tracker with GMM, to enhance its performance. Experimental results show that our EGMM based method has a lower miss rate at the same false positives per image comparing to GMM based method for moving pedestrian detection, and it also has a higher detection rate for abandoned object detection comparing to GMM based method."}, "conf/icmcs/LiMM11": {"title": "View-based 3D model retrieval with topological structure.", "url": "https://doi.org/10.1109/ICME.2011.6011946", "year": "2011", "author": {"Pengjie Li": "88/5445", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": " Abstract:With the rapidly increasing of 3D models, the view-based 3D model retrieval methods have received significant research attention. The previous view-based methods can achieve ideal retrieval result for the rigid models, but they only obtain poor retrieval result for the deformable models because they can not preserve the topological structure well. In this paper, we propose a view-based 3D model retrieval algorithm using topological structure. We extract the view-based features from the images rendered at the salient topological points. To preserve the topological structure of the 3D model, a multiresolutional reeb graph (MRG) is constructed according to the salient topological points. We take the view-based features as the attribute information of the corresponding MRG nodes. The comparison between two 3D models is transformed to compute the similarity of the corresponding MRGs. Experimental results on two standard benchmarks show that our algorithm can achieve satisfactory retrieval performance for both the deformable models and the rigid models."}, "conf/icmcs/WangMM11": {"title": "Fast accurate pedestrian detection using a MPL-Boosted cascade of weak FIK-SVM classifiers.", "url": "https://doi.org/10.1109/ICME.2011.6012024", "year": "2011", "author": {"Junqiang Wang": "91/1760", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": " Abstract:We address the problem of pedestrian detection in still images. Current pedestrian detection systems are hard to improve both speed and accuracy simultaneously. In order to achieve a balance between speed and accuracy, we propose a novel MPL-Boosted cascade of weak FIK-SVM classifiers. Our method achieves high recall while taking the speed-advantage of cascade-of-rejectors approach. Each feature in our algorithm corresponds to a 66-D HOG-LBP feature vector that describe a block. The weak classifiers we use are the separating hyper-plane computed by using a FIK-SVM. We use MPL-Boost to select features from a large set of possible blocks. The integral image and convoluted trilinear interpolation are used for rapid calculation of block feature. For a 320×240 image, the system can process 16 frames per second with sparse scan, while defeat the accuracy level of existing methods."}, "conf/mm/LiMM11": {"title": "Non-rigid 3D model retrieval using multi-scale local features.", "url": "https://doi.org/10.1145/2072298.2072031", "year": "2011", "author": {"Pengjie Li": "88/5445", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": "\n\t\tThe number of available non-rigid 3D models in various areas increases steadily. The local features are more effective than global features for the search of these non-rigid 3D models. Global descriptors fail to consistently compensate for the intra-class variability of non-rigid 3D models. To solve this problem, we propose a non-rigid 3D model retrieval method based on multi-scale local features. Firstly, we extract keypoints at multiple scales automaticlly. Then, the Heat Kernel Signature (HKS) local descriptors are computed for each keypoint. However, the HKS descriptors are sensitive to scale. In order to solve this problem, the HKS descriptors are put into the Bag-of-Features (BOF) framework. In the BOF framework, we use a kind of histogram equalization technique to make our feature descriptor robust to model scaling. Experimental results on two public benchmarks show that our algorithm can achieve satisfactory retrieval performance for the non-rigid 3D models.\n\t"}, "conf/icip/ZengMM10": {"title": "Fast human detection using mi-sVM and a cascade of HOG-LBP features.", "url": "https://doi.org/10.1109/ICIP.2010.5654100", "year": "2010", "author": {"Chengbin Zeng": "66/7627", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": " Abstract:This paper presents a human detection approach which can process images rapidly and detect the objects accurately. The features used in our system are the cascade of the HOG (Histograms of Oriented Gradients) and LBP (Local Binary Pattern). In order to achieve high recall at each stage of the cascade, we modify the mi-SVM (Support Vector Machine for multiple instance learning) to train the HOG and LBP features respectively. In this way, we implement a novel cascade-ofrejectors method to detect the human fast, while maintaining a similar accuracy reported in previous methods. Experimental results show our method can process frames at 5 to 10 frames per second, depending on the scanning density in the image."}, "conf/pcm/LiMM10": {"title": "3D Model Retrieval Using 2D View and Transform-Based Features.", "url": "https://doi.org/10.1007/978-3-642-15702-8_41", "year": "2010", "author": {"Pengjie Li": "88/5445", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": "In this paper, we propose a novel hybrid shape descriptor that combines the 2D view and transform-based features. The 2D view features are extracted from six orthogonal directions of a 3D model by using the Scale Invariant Feature Transform (SIFT) method. In order to capture the six orthogonal 2D views, Continuous Principal Component Analysis (CPCA) is used to implement pose alignment. Meanwhile, the eigenspace is computed and stored to reduce the 2D view feature vector dimension. Then, the radial integral transform and the spherical integration transform are used to extract transform-based features. The similarity between the query model and models in the database is computed by using the weighted sum of 2D view and transform-based feature similarity. Experimental results show that the proposed hybrid shape descriptor can achieve satisfactory retrieval performance for both the articulated models in the McGill Shape Benchmark and the rigid models in the Princeton Shape Benchmark.Keywords3D model retrieval2D view featuretransform-based featurehybrid shape descriptor"}, "conf/cscwd/MingM09": {"title": "A collaborative surveillance system for role identification.", "url": "https://doi.org/10.1109/CSCWD.2009.4968076", "year": "2009", "author": {"Anlong Ming": "52/3276", "Huadong Ma": "04/6217"}, "abstract": " Abstract:Many works of conventional surveillance have focused on people tracking, behavior or event detection, gait or face based recognition, etc. However, role identification is also very important in video surveillance but usually paid less attention. In this paper, we propose a collaborative multi-camera system to identify people with specific roles using a causal network to form a best identification result from the evidences known so far. Not only visual features but also spatio-temporal features are used in our method, as well as some object specific features. Collaborative multiple cameras benefit locating the position of moving objects and overcoming occlusions. Experimental results demonstrates the effectiveness of our method."}, "conf/infocom/MaZM09": {"title": "A Coverage-Enhancing Method for 3D Directional Sensor Networks.", "url": "https://doi.org/10.1109/INFCOM.2009.5062233", "year": "2009", "author": {"Huadong Ma": "04/6217", "Xi Zhang 0005": "87/1222-5", "Anlong Ming": "52/3276"}, "abstract": " Abstract:In conventional directional sensor networks, coverage control for each sensor is based on a 2D directional sensing model. However, 2D directional sensing model cannot accurately characterize the actual application scene of image/video sensor networks. To remedy this deficiency, we propose a 3D directional sensor coverage-control model with tunable orientations. In order to improve the efficiency of target-detecting, we develop a virtual potential-field based coverage-enhancing scheme to improve the coverage performance. Furthermore, we apply the simulated annealing algorithm for objective optimization. The extensive simulations show the effectiveness of our proposed 3D sensing model and coverage enhancing method."}, "conf/pcm/ZengMMZ09": {"title": "3D Human Body Tracking in Unconstrained Scenes.", "url": "https://doi.org/10.1007/978-3-642-10467-1_10", "year": "2009", "author": {"Chengbin Zeng": "66/7627", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276", "Xiaobo Zhang": "62/4819"}, "abstract": "The 3D human body tracking from videos in unconstrained scenes is a challenging problem and has widespread applications. In this paper, we introduce a novel framework that incorporates the graph-based human limbs detection into the articulated Bayesian tracking. The 3D human body model with a hierarchical tree structure can describe human’s movement by setting relevant parameters. Particle filter, which is the optimal Bayesian estimation, is used to predict the state of the 3D human pose. In order to compute the likelihood of particles, the pictorial structure model is introduced to detect the human body limbs from monocular uncalibrated images. Then the detected articulated body limbs are matched with each particle using shape contexts. Thus the 3D pose is recovered using a weighted sum of matching costs of all particles. Experimental results show our algorithm can accurately track the walking poses on very long video sequences.Keywords3D human body trackingparticle filterpictorial structureshape contexts"}, "conf/icpr/AnlongH08": {"title": "Frame-skipping tracking for single object with global motion detection.", "url": "https://doi.org/10.1109/ICPR.2008.4761155", "year": "2008", "author": {"Anlong Ming": "52/3276", "Huadong Ma": "04/6217"}, "abstract": " Abstract:Frame-skipping videos usually appear in wireless video sensor networks which have wirelessly interconnected devices that are able to ubiquitously retrieve video content from the environment. Frame-skipping videos bring to difficulties in getting the transition model (how objects move between frames). We propose a particle filter with global motion detection requiring no offline or online learning. Experimental results show the proposed approach improves the tracking accuracy in comparison with the existing conventional methods, under the condition of frame skipping data and motion of both targets and video sensors."}, "conf/bis/MingM07": {"title": "Proposal of an Architecture for a Biometrics Grid.", "url": "https://doi.org/10.1007/978-3-540-72035-5_15", "year": "2007", "author": {"Anlong Ming": "52/3276", "Huadong Ma": "04/6217"}, "abstract": "Biometric technology is critical to the rapidly growing suite of civilian applications. In this paper, we offer a broader scope of biometrics by a novel concept, the biometrics grid. The architecture of biometrics grid aims to overcome/resolve three main problems of existing biometric technology using the grid computing: 1) the need for cooperation in different fields, 2) duplicated works and 3) the consideration of application demands in different levels. The proposed architecture of biometrics grid provides a Web portal based virtual environment as well as features like biometric applications meeting QoS goals. Experimental results are given in order to show the feasibility to deploy the idea of grid computing in biometric applications.KeywordsGrid technologyBiometrics Face recognitionGrid application"}, "conf/civr/MingM07": {"title": "A blob detector in color images.", "url": "https://doi.org/10.1145/1282280.1282335", "year": "2007", "author": {"Anlong Ming": "52/3276", "Huadong Ma": "04/6217"}, "abstract": "\n\t\tIn this paper, we propose a novel and efficient framework for the extension of multi-scale blob detection for the color domain to prevent information loss due to gray scale transformation and to allow us to exploit the photometric information. The framework also works well in gray scale space, it is mostly composed of i) a weighted multi-scale blob detector using a hybrid Laplacian and determinant of the Hessian operator, ii) a blob filter, includes a color-based Forstner operator for roundness calculation and a hue-based color histogram. The experimental results on stamp images in blob detection demonstrate the effectiveness of our proposed detector.\n\t"}, "conf/gpc/MingM07": {"title": "An Algorithm Testbed for the Biometrics Grid.", "url": "https://doi.org/10.1007/978-3-540-72360-8_38", "year": "2007", "author": {"Anlong Ming": "52/3276", "Huadong Ma": "04/6217"}, "abstract": "In this paper, we propose a novel application on grid, the biometrics grid, to promote the development of both biometrics technology and grid computing. The biometrics grid aims to overcome/resolve some main problems of existing biometric technology using grid computing. The most important service provided by the biometrics grid is an algorithm testbed for biometrics researchers on single biometric or multimodal biometrics. We give a case of two respective biometrics recognition processes in voiceprint and face on grid to show that it is feasible in deploying different biometrics applications on a testbed for performance evaluation.KeywordsGrid computingBiometricsFaceVoiceprint"}, "conf/apweb/ZhangMM06": {"title": "Grid-Based Multi-scale PCA Method for Face Recognition in the Large Face Database.", "url": "https://doi.org/10.1007/11610496_144", "year": "2006", "author": {"Haiyang Zhang": "51/222", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": "In this paper, we propose an efficient grid-based multi-scale PCA method in the large face database. This method divides the large face database into some small sub-face databases by maximizing the variance in the face sub-database and minimizing the variance between the face sub-databases, then it segments the recognition process into the local coarse profile recognition process and accurate detailed geometric sub-component analysis process, and assigns the local coarse profile recognition process to the nodes of the multimedia service grid to reduce the recognition time. Our experimental results show that with the increase of the face database, this method not only reduces the recognition time, but also remarkably increases the recognition precision, compared with other PCA methods."}, "conf/icmcs/MingM06": {"title": "An Improved Approach to the Line-Based Face Recognition.", "url": "https://doi.org/10.1109/ICME.2006.262840", "year": "2006", "author": {"Anlong Ming": "52/3276", "Huadong Ma": "04/6217"}, "abstract": " Abstract:The line-based face recognition method is distinguished by its features, but its development and application is limited to some inherent drawbacks. This paper propose a method for decreasing the influence under variable illumination intensity by using the line-based singular value (LSV) feature vector instead of image gray-level value to calculate \"distance\" between two lines. We prove that our method is invariant to the illumination intensity. Finally, we suggest a distributed computing algorithm using grid computing to solve the multi-scale computation. Experimental results show our approach is effective"}, "conf/skg/MingMZX05": {"title": "A Grid-based Face Recognition Approach Using General Template Matching.", "url": "https://doi.org/10.1109/SKG.2005.12", "year": "2005", "author": {"Anlong Ming": "52/3276", "Huadong Ma": "04/6217", "Haiyang Zhang": "51/222", "Wei Xiong": "33/4054"}, "abstract": " Abstract:Computing grids offer a very effective economic and scalable way to build high performance infrastructure. In this paper, we propose a grid-based face recognition approach using general template matching method to solve time-consuming face recognition problem. This approach is computationally efficient, has high recognition rates, and is able to handle large database problem by adopting grid computing."}, "journals/tip/ZhuKYM22": {"title": "Explored Normalized Cut With Random Walk Refining Term for Image Segmentation.", "url": "https://doi.org/10.1109/TIP.2022.3162475", "year": "2022", "author": {"Lei Zhu": "99/549", "Xuejing Kang": "33/8493", "Lizhu Ye": "254/2716", "Anlong Ming": "52/3276"}, "abstract": " Abstract:The Normalized Cut (NCut) model is a popular graph-based model for image segmentation. But it suffers from the excessive normalization problem and weakens the small object and twig segmentation. In this paper, we propose an Explored Normalized Cut (ENCut) model that establishes a balance graph model by adopting a meaningful-loop and a k-step random walk, which reduces the energy of small salient region, so as to enhance the small object segmentation. To improve the twig segmentation, our ENCut model is further enhanced by a new Random Walk Refining Term (RWRT) that adds local attention to our model with the help of an un-supervising random walk. Finally, a move-making based strategy is developed to efficiently solve the ENCut model with RWRT. Experiments on three standard datasets indicate that our model can achieve state-of-the-art results among the NCut-based segmentation models."}, "journals/corr/abs-2203-04537": {"title": "Fast Road Segmentation via Uncertainty-aware Symmetric Network.", "url": "https://doi.org/10.48550/arXiv.2203.04537", "year": "2022", "author": {"Yicong Chang": "315/3985", "Feng Xue": "03/517", "Fei Sheng": "145/6237", "Wenteng Liang": "308/0557", "Anlong Ming": "52/3276"}, "abstract": "\n      Abstract:  The high performance of RGB-D based road segmentation methods contrasts with\ntheir rare application in commercial autonomous driving, which is owing to two\nreasons: 1) the prior methods cannot achieve high inference speed and high\naccuracy in both ways; 2) the different properties of RGB and depth data are\nnot well-exploited, limiting the reliability of predicted road. In this paper,\nbased on the evidence theory, an uncertainty-aware symmetric network (USNet) is\nproposed to achieve a trade-off between speed and accuracy by fully fusing RGB\nand depth data. Firstly, cross-modal feature fusion operations, which are\nindispensable in the prior RGB-D based methods, are abandoned. We instead\nseparately adopt two light-weight subnetworks to learn road representations\nfrom RGB and depth inputs. The light-weight structure guarantees the real-time\ninference of our method. Moreover, a multiscale evidence collection (MEC)\nmodule is designed to collect evidence in multiple scales for each modality,\nwhich provides sufficient evidence for pixel class determination. Finally, in\nuncertainty-aware fusion (UAF) module, the uncertainty of each modality is\nperceived to guide the fusion of the two subnetworks. Experimental results\ndemonstrate that our method achieves a state-of-the-art accuracy with real-time\ninference speed of 43+ FPS. The source code is available at\nthis https URL.\n\n    "}, "journals/corr/abs-2203-04538": {"title": "Monocular Depth Distribution Alignment with Low Computation.", "url": "https://doi.org/10.48550/arXiv.2203.04538", "year": "2022", "author": {"Fei Sheng": "145/6237", "Feng Xue": "03/517", "Yicong Chang": "315/3985", "Wenteng Liang": "308/0557", "Anlong Ming": "52/3276"}, "abstract": "\n      Abstract:  The performance of monocular depth estimation generally depends on the amount\nof parameters and computational cost. It leads to a large accuracy contrast\nbetween light-weight networks and heavy-weight networks, which limits their\napplication in the real world. In this paper, we model the majority of accuracy\ncontrast between them as the difference of depth distribution, which we call\n\"Distribution drift\". To this end, a distribution alignment network (DANet) is\nproposed. We firstly design a pyramid scene transformer (PST) module to capture\ninter-region interaction in multiple scales. By perceiving the difference of\ndepth features between every two regions, DANet tends to predict a reasonable\nscene structure, which fits the shape of distribution to ground truth. Then, we\npropose a local-global optimization (LGO) scheme to realize the supervision of\nglobal range of scene depth. Thanks to the alignment of depth distribution\nshape and scene depth range, DANet sharply alleviates the distribution drift,\nand achieves a comparable performance with prior heavy-weight methods, but uses\nonly 1% floating-point operations per second (FLOPs) of them. The experiments\non two datasets, namely the widely used NYUDv2 dataset and the more challenging\niBims-1 dataset, demonstrate the effectiveness of our method. The source code\nis available at this https URL.\n\n    "}, "journals/corr/abs-2204-06833": {"title": "MARF: Multiscale Adaptive-switch Random Forest for Leg Detection with 2D Laser Scanners.", "url": "https://doi.org/10.48550/arXiv.2204.06833", "year": "2022", "author": {"Tianxi Wang": "275/6511", "Feng Xue": "03/517", "Yu Zhou 0016": "36/2728-16", "Anlong Ming": "52/3276"}, "abstract": "\n      Abstract:  For the 2D laser-based tasks, e.g., people detection and people tracking, leg\ndetection is usually the first step. Thus, it carries great weight in\ndetermining the performance of people detection and people tracking. However,\nmany leg detectors ignore the inevitable noise and the multiscale\ncharacteristics of the laser scan, which makes them sensitive to the unreliable\nfeatures of point cloud and further degrades the performance of the leg\ndetector. In this paper, we propose a multiscale adaptive-switch Random Forest\n(MARF) to overcome these two challenges. Firstly, the adaptive-switch decision\ntree is designed to use noisesensitive features to conduct weighted\nclassification and noiseinvariant features to conduct binary classification,\nwhich makes our detector perform more robust to noise. Secondly, considering\nthe multiscale property that the sparsity of the 2D point cloud is proportional\nto the length of laser beams, we design a multiscale random forest structure to\ndetect legs at different distances. Moreover, the proposed approach allows us\nto discover a sparser human leg from point clouds than others. Consequently,\nour method shows an improved performance compared to other state-of-the-art leg\ndetectors on the challenging Moving Legs dataset and retains the whole pipeline\nat a speed of 60+ FPS on lowcomputational laptops. Moreover, we further apply\nthe proposed MARF to the people detection and tracking system, achieving a\nconsiderable gain in all metrics.\n\n    "}, "journals/ijon/LiKZYFM21": {"title": "HDNet: Hybrid Distance Network for semantic segmentation.", "url": "https://doi.org/10.1016/j.neucom.2021.03.044", "year": "2021", "author": {"Chunpeng Li": "78/5072", "Xuejing Kang": "33/8493", "Lei Zhu": "99/549", "Lizhu Ye": "254/2716", "Panhe Feng": "254/2722", "Anlong Ming": "52/3276"}, "abstract": "Semantic segmentation is currently solved as a pixel-wise labeling task, which predicts the label of each pixel based on its features. However, current methods isolate the relations of points in a feature map and cause the discontinuous segmentation results. In order to solve this problem, we propose a Hybrid Distance Network to measure the distance from two aspects. First, the Hybrid Distance Relation is proposed to model the relations between a point and its context regions to capture contexts in a feature map by an elegant combination of positional distance and high-dimension feature distance. Then, a Location Aware Attention module is proposed to efficiently sample the contexts by the positional distance and produces sparse Hybrid Distance Relations. It synthesizes the different contexts of each point and generates position-wise attention value to compact object-level representation. During the training step, High-dimension Feature Distance loss is also presented as an auxiliary loss to compact category-level representation in feature space. Experiments show that the proposed HDNet achieves state-of-the-art performance with interpretability and efficiency on three challenging semantic segmentation benchmarks: Pascal Context, ADE20K, and COCO Stuff 10 K."}, "journals/pr/XueCZSWM21": {"title": "Boundary-induced and scene-aggregated network for monocular depth prediction.", "url": "https://doi.org/10.1016/j.patcog.2021.107901", "year": "2021", "author": {"Feng Xue": "03/517", "Junfeng Cao": "06/8492", "Yu Zhou 0016": "36/2728-16", "Fei Sheng": "145/6237", "Yankai Wang": "120/1143", "Anlong Ming": "52/3276"}, "abstract": "Monocular depth prediction is an important task in scene understanding. It aims to predict the dense depth of a single RGB image. With the development of deep learning, the performance of this task has made great improvements. However, two issues remain unresolved: (1) The deep feature encodes the wrong farthest region in a scene, which leads to a distorted 3D structure of the predicted depth; (2) The low-level features are insufficient utilized, which makes it even harder to estimate the depth near the edge with sudden depth change. To tackle these two issues, we propose the Boundary-induced and Scene-aggregated network (BS-Net). In this network, the Depth Correlation Encoder (DCE) is first designed to obtain the contextual correlations between the regions in an image, and perceive the farthest region by considering the correlations. Meanwhile, the Bottom-Up Boundary Fusion (BUBF) module is designed to extract accurate boundary that indicates depth change. Finally, the Stripe Refinement module (SRM) is designed to refine the dense depth induced by the boundary cue, which improves the boundary accuracy of the predicted depth. Several experimental results on the NYUD v2 dataset and the iBims-1 dataset illustrate the state-of-the-art performance of the proposed approach. And the SUN-RGBD dataset is employed to evaluate the generalization of our method. Code is available at https://github.com/XuefengBUPT/BS-Net."}, "journals/corr/abs-2102-13258": {"title": "Boundary-induced and scene-aggregated network for monocular depth prediction.", "url": "https://arxiv.org/abs/2102.13258", "year": "2021", "author": {"Feng Xue": "03/517", "Junfeng Cao": "06/8492", "Yu Zhou 0016": "36/2728-16", "Fei Sheng": "145/6237", "Yankai Wang": "120/1143", "Anlong Ming": "52/3276"}, "abstract": "\n      Abstract:  Monocular depth prediction is an important task in scene understanding. It\naims to predict the dense depth of a single RGB image. With the development of\ndeep learning, the performance of this task has made great improvements.\nHowever, two issues remain unresolved: (1) The deep feature encodes the wrong\nfarthest region in a scene, which leads to a distorted 3D structure of the\npredicted depth; (2) The low-level features are insufficient utilized, which\nmakes it even harder to estimate the depth near the edge with sudden depth\nchange. To tackle these two issues, we propose the Boundary-induced and\nScene-aggregated network (BS-Net). In this network, the Depth Correlation\nEncoder (DCE) is first designed to obtain the contextual correlations between\nthe regions in an image, and perceive the farthest region by considering the\ncorrelations. Meanwhile, the Bottom-Up Boundary Fusion (BUBF) module is\ndesigned to extract accurate boundary that indicates depth change. Finally, the\nStripe Refinement module (SRM) is designed to refine the dense depth induced by\nthe boundary cue, which improves the boundary accuracy of the predicted depth.\nSeveral experimental results on the NYUD v2 dataset and \\xff{the iBims-1\ndataset} illustrate the state-of-the-art performance of the proposed approach.\nAnd the SUN-RGBD dataset is employed to evaluate the generalization of our\nmethod. Code is available at this https URL.\n\n    "}, "journals/corr/abs-2108-05722": {"title": "MT-ORL: Multi-Task Occlusion Relationship Learning.", "url": "https://arxiv.org/abs/2108.05722", "year": "2021", "author": {"Panhe Feng": "254/2722", "Qi She": "171/7773", "Lei Zhu": "99/549", "Jiaxin Li": "69/327", "Lin Zhang": "37/1629", "Zijian Feng": "45/10114", "Changhu Wang": "30/3393", "Chunpeng Li": "78/5072", "Xuejing Kang": "33/8493", "Anlong Ming": "52/3276"}, "abstract": "\n      Abstract:  Retrieving occlusion relation among objects in a single image is challenging\ndue to sparsity of boundaries in image. We observe two key issues in existing\nworks: firstly, lack of an architecture which can exploit the limited amount of\ncoupling in the decoder stage between the two subtasks, namely occlusion\nboundary extraction and occlusion orientation prediction, and secondly,\nimproper representation of occlusion orientation. In this paper, we propose a\nnovel architecture called Occlusion-shared and Path-separated Network (OPNet),\nwhich solves the first issue by exploiting rich occlusion cues in shared\nhigh-level features and structured spatial information in task-specific\nlow-level features. We then design a simple but effective orthogonal occlusion\nrepresentation (OOR) to tackle the second issue. Our method surpasses the\nstate-of-the-art methods by 6.1%/8.3% Boundary-AP and 6.5%/10% Orientation-AP\non standard PIOD/BSDS ownership datasets. Code is available at\nthis https URL.\n\n    "}, "journals/corr/abs-2111-09204": {"title": "Tiny Obstacle Discovery by Occlusion-Aware Multilayer Regression.", "url": "https://arxiv.org/abs/2111.09204", "year": "2021", "author": {"Feng Xue": "03/517", "Anlong Ming": "52/3276", "Yu Zhou 0016": "36/2728-16"}, "abstract": "\n      Abstract:  Edges are the fundamental visual element for discovering tiny obstacles using\na monocular camera. Nevertheless, tiny obstacles often have weak and\ninconsistent edge cues due to various properties such as small size and similar\nappearance to the free space, making it hard to capture them. ...\n\n    "}, "journals/tip/KangZM20": {"title": "Dynamic Random Walk for Superpixel Segmentation.", "url": "https://doi.org/10.1109/TIP.2020.2967583", "year": "2020", "author": {"Xuejing Kang": "33/8493", "Lei Zhu": "99/549", "Anlong Ming": "52/3276"}, "abstract": " Abstract:In this paper, we propose a novel random walk model, called Dynamic Random Walk (DRW), which adds a new type of dynamic node to the original RW model and reduces redundant calculation by limiting the walk range. To solve the seed-lacking problem of the proposed DRW, we redefine the energy function of the original RW and use the first arrival probability among each node pair to avoid the interference for each partition. Relaxation of our DRW is performed with the help of a greedy strategy and the Weighted Random Walk Entropy(WRWE) that uses the gradient feature to approximate the stationary distribution. The proposed DRW not only can enhance the boundary adherence but also can run with linear time complexity. To extend our DRW for superpixel segmentation, a seed initialization strategy is proposed. It can evenly distribute seeds in both 2D and 3D space and generate superpixels in only one iteration. The experimental results demonstrate that our DRW is faster than existing RW models and better than the state-of-the-art superpixel segmentation algorithms with respect to both efficiency and segmentation effects."}, "journals/tip/XueMZ20": {"title": "Tiny Obstacle Discovery by Occlusion-Aware Multilayer Regression.", "url": "https://doi.org/10.1109/TIP.2020.3026636", "year": "2020", "author": {"Feng Xue": "03/517", "Anlong Ming": "52/3276", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:Edges are the fundamental visual element for discovering tiny obstacles using a monocular camera. Nevertheless, tiny obstacles often have weak and inconsistent edge cues due to various properties such as small size and similar appearance to the free space, making it hard to capture them. To this end, we propose an occlusion-based multilayer approach, which specifies the scene prior as multilayer regions and utilizes these regions in each obstacle discovery module, i.e., edge detection and proposal extraction. Firstly, an obstacle-aware occlusion edge is generated to accurately capture the obstacle contour by fusing the edge cues inside all the multilayer regions, which intensifies the object characteristics of these obstacles. Then, a multistride sliding window strategy is proposed for capturing proposals that enclose the tiny obstacles as completely as possible. Moreover, a novel obstacle-aware regression model is proposed for effectively discovering obstacles. It is formed by a primary-secondary regressor, which can learn two dissimilarities between obstacles and other categories separately, and eventually generate an obstacle-occupied probability map. The experiments are conducted on two datasets to demonstrate the effectiveness of our approach under different scenarios. And the results show that the proposed method can approximately improve accuracy by 19% over FPHT and PHT, and achieves comparable performance to MergeNet. Furthermore, multiple experiments with different variants validate the contribution of our method. The source code is available at https://github.com/XuefengBUPT/TOD_OMR."}, "journals/tbc/YaoXZM19": {"title": "Video Streaming Adaptation Strategy for Multiview Navigation Over DASH.", "url": "https://doi.org/10.1109/TBC.2018.2871370", "year": "2019", "author": {"Chao Yao": "99/6747", "Jimin Xiao": "10/11067", "Yao Zhao 0001": "45/2091-1", "Anlong Ming": "52/3276"}, "abstract": " Abstract:Video content delivery over Internet is receiving increasing attention from both industry and academia, especially for the multiview video contents, as it is the basis to support various applications, such as 3-D video, virtual reality, free view video, and so on. To cope with the dynamic nature of Internet throughput, dynamic adaptive streaming over HTTP (DASH) has been introduced to control the video streaming based on the network conditions. In this paper, we design a streaming framework to improve the user experience of the multiview video streaming over DASH, considering the user behavior of the viewpoint navigation during the streaming process. To eliminate the view switching delay, a multiple view navigation rule is introduced to pre-fetch the possible switching viewpoints. An optimal bitrate allocation scheme is proposed for the introduced rule, allowing the clients to maximize the video quality. Moreover, we found the video quality and the playback starvation probability are conflicting factors, while both are essential for the user's quality of experience (QoE). To tackle this issue, a QoE optimization solution is designed to maximize the overall performance in the proposed framework. Several experiments verify the effectiveness of the proposed framework, and the results demonstrate that the proposed framework outperforms two typical DASH methods."}, "journals/tcsv/KangMT19": {"title": "Reality-Preserving Multiple Parameter Discrete Fractional Angular Transform and Its Application to Color Image Encryption.", "url": "https://doi.org/10.1109/TCSVT.2018.2851983", "year": "2019", "author": {"Xuejing Kang": "33/8493", "Anlong Ming": "52/3276", "Ran Tao 0003": "99/955-3"}, "abstract": " Abstract:In this paper, we first define a reality-preserving multiple parameter fractional angular transform (RPMPDFrAT), which is a useful tool for image encryption. Then, we propose a new color image encryption algorithm based on the defined RPMPDFrAT. The encryption process consists of two phases: encryption in the spatial domain and RPMPDFrAT domain. In the spatial domain, three color components of the plain image are mapped by dual cylindrical transform, which can nonlinearly hide the original color information. Then, the intermediate output is scrambled by a coupled logistic map to reduce the correlation of adjacent pixels and uniformly distribute the image energy of different color components. Thereafter, the scrambled image is transformed by the proposed RPMPDFrAT, which can ensure that we obtain the real-value output. Finally, a process similar to the spatial domain is performed in the RPMPDFrAT domain to further improve the security of the cryptosystem. Numerical simulations are performed and demonstrate that the proposed image encryption algorithm is effective and sensitive to keys. Moreover, some potential attacks are tested to verify the robustness of the proposed method, and the performance of our method outperforms previously published ones."}, "journals/corr/abs-1903-08890": {"title": "Context-Constrained Accurate Contour Extraction for Occlusion Edge Detection.", "url": "http://arxiv.org/abs/1903.08890", "year": "2019", "author": {"Rui Lu": "33/2451", "Menghan Zhou": "226/2572", "Anlong Ming": "52/3276", "Yu Zhou 0016": "36/2728-16"}, "abstract": "\n      Abstract:  Occlusion edge detection requires both accurate locations and context\nconstraints of the contour. Existing CNN-based pipeline does not utilize\nadaptive methods to filter the noise introduced by low-level features. To\naddress this dilemma, we propose a novel Context-constrained accurate Contour\nExtraction Network (CCENet). Spatial details are retained and contour-sensitive\ncontext is augmented through two extraction blocks, respectively. Then, an\nelaborately designed fusion module is available to integrate features, which\nplays a complementary role to restore details and remove clutter. Weight\nresponse of attention mechanism is eventually utilized to enhance occluded\ncontours and suppress noise. The proposed CCENet significantly surpasses\nstate-of-the-art methods on PIOD and BSDS ownership dataset of object edge\ndetection and occlusion orientation detection.\n\n    "}, "journals/corr/abs-1904-10161": {"title": "A Novel Multi-layer Framework for Tiny Obstacle Discovery.", "url": "http://arxiv.org/abs/1904.10161", "year": "2019", "author": {"Feng Xue": "03/517", "Anlong Ming": "52/3276", "Menghan Zhou": "226/2572", "Yu Zhou 0016": "36/2728-16"}, "abstract": "\n      Abstract:  For tiny obstacle discovery in a monocular image, edge is a fundamental\nvisual element. Nevertheless, because of various reasons, e.g., noise and\nsimilar color distribution with background, it is still difficult to detect the\nedges of tiny obstacles at long distance. In this paper, we propose an\nobstacle-aware discovery method to recover the missing contours of these\nobstacles, which helps to obtain obstacle proposals as much as possible. First,\nby using visual cues in monocular images, several multi-layer regions are\nelaborately inferred to reveal the distances from the camera. Second, several\nnovel obstacle-aware occlusion edge maps are constructed to well capture the\ncontours of tiny obstacles, which combines cues from each layer. Third, to\nensure the existence of the tiny obstacle proposals, the maps from all layers\nare used for proposals extraction. Finally, based on these proposals containing\ntiny obstacles, a novel obstacle-aware regressor is proposed to generate an\nobstacle occupied probability map with high confidence. The convincing\nexperimental results with comparisons on the Lost and Found dataset demonstrate\nthe effectiveness of our approach, achieving around 9.5% improvement on the\naccuracy than FPHT and PHT, it even gets comparable performance to MergeNet.\nMoreover, our method outperforms the state-of-the-art algorithms and\nsignificantly improves the discovery ability for tiny obstacles at long\ndistance.\n\n    "}, "journals/corr/abs-1908-05898": {"title": "Occlusion-shared and Feature-separated Network for Occlusion Relationship Reasoning.", "url": "http://arxiv.org/abs/1908.05898", "year": "2019", "author": {"Rui Lu": "33/2451", "Feng Xue": "03/517", "Menghan Zhou": "226/2572", "Anlong Ming": "52/3276", "Yu Zhou 0016": "36/2728-16"}, "abstract": "\n      Abstract:  Occlusion relationship reasoning demands closed contour to express the\nobject, and orientation of each contour pixel to describe the order\nrelationship between objects. Current CNN-based methods neglect two critical\nissues of the task: (1) simultaneous existence of the relevance and distinction\nfor the two elements, i.e, occlusion edge and occlusion orientation; and (2)\ninadequate exploration to the orientation features. For the reasons above, we\npropose the Occlusion-shared and Feature-separated Network (OFNet). On one\nhand, considering the relevance between edge and orientation, two sub-networks\nare designed to share the occlusion cue. On the other hand, the whole network\nis split into two paths to learn the high-level semantic features separately.\nMoreover, a contextual feature for orientation prediction is extracted, which\nrepresents the bilateral cue of the foreground and background areas. The\nbilateral cue is then fused with the occlusion cue to precisely locate the\nobject regions. Finally, a stripe convolution is designed to further aggregate\nfeatures from surrounding scenes of the occlusion edge. The proposed OFNet\nremarkably advances the state-of-the-art approaches on PIOD and BSDS ownership\ndataset. The source code is available at this https URL.\n\n    "}, "journals/mta/LiMM17": {"title": "A non-rigid 3D model retrieval method based on scale-invariant heat kernel signature features.", "url": "https://doi.org/10.1007/s11042-016-3606-9", "year": "2017", "author": {"Pengjie Li": "88/5445", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": "The number of non-rigid 3D models increases steadily in various areas. It is imperative to develop efficient retrieval system for 3D non-rigid models. As we know, global features fail to consistently describe the intra-class variability of non-rigid 3D models, the local features are more effective than global features for the retrieval of non-rigid 3D models. In this paper, we use Heat Kernel Signature (HKS) as the local features to represent non-rigid 3D models and further propose the retrieval method based on scale-invariant local features. Firstly, we extract key-points at multiple scales automatically. Then, the HKS local features are computed for each key-point. However, the HKS features are sensitive to scale. In order to solve this problem, we convert the scale problem into the translation problem using the diffusion Wavelets transform. To solve the translation problem, we use a kind of histogram equalization technique. Finally, we use the bipartite graph matching algorithm to compute similarity between the 3D models. Experimental results on two public benchmarks show that our method outperforms state-of-the-art methods for non-rigid 3D models retrieval."}, "journals/expert/MingWMSZ16": {"title": "Monocular Depth-Ordering Reasoning with Occlusion Edge Detection and Couple Layers Inference.", "url": "https://doi.org/10.1109/MIS.2015.94", "year": "2016", "author": {"Anlong Ming": "52/3276", "Tianfu Wu 0001": "08/4148-1", "Jianxiang Ma": "177/6660", "Fang Sun": "24/1938", "Yu Zhou 0016": "36/2728-16"}, "abstract": " Abstract:A depth-ordering reasoning approach first provides novel occlusion edge detection, generating precise same-layer relationship judgment and producing reliable region proposals for the depth-ordering inference. Specifically, a novel sparsity-induced regression model learns a discriminative feature subspace. In addition, kernel ridge regression assigns the occlusion label for each edge. The kernel trick guarantees linearly separable edges in a rich, high-dimensional feature space. Secondly, a couple layers inference approach infers the final depth order. In the semilocal layer, a novel triple descriptor judges the foreground relationship. In the global layer, the inference is executed by finding a valid path on a directed graph model. The proposed approach is validated on the Cornell depth-order and NYU 2 datasets."}, "journals/prl/ZhouM16": {"title": "Human action recognition with skeleton induced discriminative approximate rigid part model.", "url": "https://doi.org/10.1016/j.patrec.2016.07.025", "year": "2016", "author": {"Yu Zhou 0016": "36/2728-16", "Anlong Ming": "52/3276"}, "abstract": "Human action recognition has a long research history. Despite various approaches have been designed in the last decades, it still remains challenging in computer vision and pattern recognition. In this paper, we present a skeleton induced discriminative approximate rigid part model for human action recognition, which not only captures the geometrical structure of human body, but also takes rich human body surface cues into consideration. In conventional approaches, the joint feature and the surface feature are discussed separately. While, in our proposed approach, the structural information is embedded into the surface model. In addition, to separate different approximate rigid parts generated from different human activities, a novel sparsity induced feature selection scheme is introduced. This scheme produces a discriminative feature subspace that can best separate different action classes. The presented approach is validated on two widely adopted benchmark datasets, i.e. the MSR Daily Activity 3D Dataset and the MSR Action 3D dataset. Experimental results demonstrate the effectiveness of our approach."}, "journals/mta/LiMM13": {"title": "Combining topological and view-based features for 3D model retrieval.", "url": "https://doi.org/10.1007/s11042-012-1000-9", "year": "2013", "author": {"Pengjie Li": "88/5445", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": "With the rapidly increasing of 3D models, the 3D model retrieval methods have been paid significant research attention. Most of the existing methods focus on taking advantage of one kind of feature. These methods can not achieve ideal retrieval results for different classes of 3D models. In this paper, we propose a novel 3D model retrieval algorithm by combining topological and view-based features. To preserve the topological structure of the 3D model, a multiresolutional reeb graph (MRG) is constructed according to the salient topological points. The view-based features are extracted from the images, which are rendered at each of the topological points. To preserve the spatial structure information of the images, we modify the bag-of-features (BOF) method by using the combined shell-sector model. We take the view-based features as the attribute information of the corresponding MRG nodes. The comparison between two 3D models is transformed to the problem of computing the similarity of the corresponding MRGs. Finally, we calculate the similarity between the query model and the models in the databases by adapting the earth mover distance method. Experimental results on two standard benchmarks show that our algorithm can achieve satisfactory retrieval performance."}, "journals/dsonline/MingM07": {"title": "The Biometrics Grid: A Solution to Biometric Technologies.", "url": "http://doi.ieeecomputersociety.org/10.1109/MDSO.2007.55", "year": "2007", "author": {"Anlong Ming": "52/3276", "Huadong Ma": "04/6217"}, "abstract": ""}}}