{"id": "39/10696", "name": "Huiyuan Fu", "Article": {"conf/mipr/WangWZFM21": {"title": "Transformer based Neural Network for Fine-Grained Classification of Vehicle Color.", "url": "https://doi.org/10.1109/MIPR51284.2021.00025", "year": "2021", "author": {"Yingjin Wang": "304/6724", "Chuanming Wang": "266/3767", "Yuchao Zheng": "279/5881", "Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217"}, "abstract": " Abstract:The development of vehicle color recognition technology is of great significance for vehicle identification and the development of the intelligent transportation system. However, the small variety of colors and the influence of the illumination in the environment make fine-grained vehicle color recognition a challenge task. Insufficient training data and small color categories in previous datasets causes the low recognition accuracy and the inflexibility of practical using. Meanwhile, the inefficient feature learning also leads to poor recognition performance of the previous methods. Therefore, we collect a rear shooting dataset from vehicle bayonet monitoring for fine-grained vehicle color recognition. Its images can be divided into 11 main-categories and 75 color subcategories according to the proposed labeling algorithm which can eliminate the influence of illumination and assign the color annotation for each image. We propose a novel recognition model which can effectively identify the vehicle colors. We skillfully interpolate the Transformer into recognition model to enhance the feature learning capacity of conventional neural networks, and specially design a hierarchical loss function through in-depth analysis of the proposed dataset. We evaluate the designed recognition model on the dataset and it can achieve accuracy of 97.77%, which is superior to the traditional approaches."}, "conf/mm/FuTWM21": {"title": "Stacked Semantically-Guided Learning for Image De-distortion.", "url": "https://doi.org/10.1145/3474085.3475608", "year": "2021", "author": {"Huiyuan Fu": "39/10696", "Changhao Tian": "304/1056", "Xin Wang": "10/5630", "Huadong Ma": "04/6217"}, "abstract": "\n\t\tImage de-distortion is very important because distortions will degrade the image quality significantly. It can benefit many computational visual media applications that are primarily designed for high-quality images. In order to address this challenging issue, we propose a stacked semantically-guided network, which is the first try on this task. It can capture and restore the distortions around the humans and the adjacent background effectively with the stacked network architecture and the semantically-guided scheme. In addition, a discriminative restoration loss function is proposed to recover different distorted regions in the images discriminatively. As another important effort, we construct a large-scale dataset for image de-distortion. Extensive qualitative and quantitative experiments show that our proposed method achieves a superior performance compared with the state-of-the-art approaches.\n\t"}, "conf/aaai/WangFLDM20": {"title": "Region-Based Global Reasoning Networks.", "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6893", "year": "2020", "author": {"Chuanming Wang": "266/3767", "Huiyuan Fu": "39/10696", "Charles X. Ling": "99/4062", "Peilun Du": "250/0521", "Huadong Ma": "04/6217"}, "abstract": "Global reasoning plays a significant role in many computer vision tasks which need to capture long-distance relationships. However, most current studies on global reasoning focus on exploring the relationship between pixels and ignore the critical role of the regions. In this paper, we propose an novel approach that explores the relationship between regions which have richer semantics than pixels. Specifically, we design a region aggregation method that can gather regional features automatically into a uniform shape, and adjust theirs positions adaptively for better alignment. To achieve the best performance of global reasoning, we propose various relationship exploration methods and apply them on the regional features. Our region-based global reasoning module, named ReGr, is end-to-end and can be inserted into existing visual understanding models without extra supervision. To evaluate our approach, we apply ReGr to fine-grained classification and action recognition benchmark tasks, and the experimental results demonstrate the effectiveness of our approach.\n\t\t\t\t"}, "conf/icassp/WangFM20": {"title": "Global Structure Graph Guided Fine-Grained Vehicle Recognition.", "url": "https://doi.org/10.1109/ICASSP40776.2020.9052902", "year": "2020", "author": {"Chuanming Wang": "266/3767", "Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217"}, "abstract": " Abstract:Fine-grained vehicle recognition is a challenging problem due to the subtle intra-category appearance variation, which requires the recognition model can capture discriminative features from distinguishing regions. The structure is an important characteristic of vehicles which can help to find substantial parts and learn distinguishing representations. In this paper, we propose an approach that introduces the structure graph into consideration to learn distinguishing representations for vehicle recognition. Our proposed method first constructs a global structure graph from the features generated by the convolutional network and then it applies the graph as the guidance to produce effective representations of vehicles. The results of extensive experiments demonstrate that our proposed method can produce more promising results than other state-of-the-art methods. The results of the visualization illustrate that our approach can construct a suitable structure graph and the global structure information facilitates learning discriminative representations at crucial parts of vehicles."}, "conf/mm/FuYWM20": {"title": "Cross-Granularity Learning for Multi-Domain Image-to-Image Translation.", "url": "https://doi.org/10.1145/3394171.3413656", "year": "2020", "author": {"Huiyuan Fu": "39/10696", "Ting Yu": "181/2866", "Xin Wang": "10/5630", "Huadong Ma": "04/6217"}, "abstract": "\n\t\tImage translation across diverse domains has attracted more and more attention. Existing multi-domain image-to-image translation algorithms only learn the features of the complete image without considering specific features of local instances. To ensure the important instance to be more realistically translated, we propose a cross-granularity learning model for multi-domain image-to-image translation. We provide detailed procedures to capture the features of instances during the learning process, and specifically learn the relationship between style of the global image and the style of an instance on the image through the enforcing of the cross-granularity consistency. In our design, we only need one generator to perform the instance-aware multi-domain image translation. Our extensive experiments on several multi-domain image-to-image translation datasets show that our proposed method can achieve superior performance compared with the state-of-the-art approaches.\n\t"}, "conf/mm/LiuZFMC20": {"title": "Enhancing Anomaly Detection in Surveillance Videos with Transfer Learning from Action Recognition.", "url": "https://doi.org/10.1145/3394171.3416298", "year": "2020", "author": {"Kun Liu 0016": "06/2592-16", "Minzhi Zhu": "276/3222", "Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Tat-Seng Chua": "24/6606"}, "abstract": "\n\t\tAnomaly detection in surveillance videos, as a special case of video-based action recognition, has been of increasing interest in multimedia community and public security. Action recognition in videos faces some challenges, such as cluttered background, illumination conditions. Besides these above difficulties, detecting anomaly in surveillance videos has several unique problems to be solved. For example, the lack of sufficient training samples is one of the main challenges for detecting anomalies in surveillance videos. In this paper, we propose to utilize transfer learning to leverage the good results from action recognition for anomaly detection in surveillance videos. More specially, we explore some techniques based on action recognition models from the following aspects: training samples, temporal modules for action recognition, network backbones. We draw some conclusions. First, more training samples from surveillance videos lead to higher classification accuracy. Second, stronger temporal modules designed for recognizing action and deeper networks do not achieve better results. This conclusion is reasonable since deeper networks tend to over-fitting, especially for the small-scale training set. Besides, to distinguish the hard examples from normal activities, we separately train a neural network to classify the hard category and normal events. Then we fuse the binary network and previous network to generate the final prediction for general anomaly detection. On the benchmarks of CitySCENE, our framework achieves promising performance and obtains the first prize for general anomaly detection and the second prize for specific anomaly detection.\n\t"}, "conf/hucc/TanZFMG19": {"title": "Multi-scale Attentive Residual Network for Single Image Deraining.", "url": "https://doi.org/10.1007/978-3-030-37429-7_35", "year": "2019", "author": {"Jing Tan": "96/8637", "Yu Zhang": "50/671", "Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Ning Gao": "47/544"}, "abstract": "Removing rain streaks from a single image is extremely challenging since the appearance of rain streaks in shapes, scales and densities is ever changing. Therefore, we propose a novel end-to-end two- stage multi-scale attentive residual network that is both location-aware and density-aware, in order to preferably remove various rain streaks. Specifically, in the first stage, a multi-scale progressive attention sub- network is designed to automatically locate the distribution of diverse rain streaks and further to guide the following deraining. Then the second stage with the guidance of the attention map generated in the former stage aims to efficiently remove various rain streaks. To aggregate the characteristics of rain streaks with different scales and densities, we construct a multi-scale residual sub-network in which dilated convolution and residual learning are used to combine these features. As a result, these two sub-networks make up the whole network, and accomplish the process of joint detection and removal of diverse rain streaks fairly well. Extensive experiments on both synthetic and real-world rainy images demonstrate that our proposed method significantly outperforms several recent state-of-the-art approaches.\nKeywordsSingle image derainingMulti-scaleVisual attention mechanismResidual learning"}, "conf/hucc/XueMFY19": {"title": "Person Search with Joint Detection, Segmentation and Re-identification.", "url": "https://doi.org/10.1007/978-3-030-37429-7_52", "year": "2019", "author": {"Rui Xue": "30/4367", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696", "Wenbin Yao": "80/1996"}, "abstract": "Person search is a new and challenging task proposed in recent years. It aims to jointly handle person detection and person re-identification in an end-to-end deep learning neural network. In this paper, we propose a new multi-task framework, which jointly learn person detection, person instance segmentation and person re-identification. In this framework, a segmentation branch is added into the person search pipeline to generate a high-quality segmentation mask for each person instance. Then, the segmentation feature maps are concatenated with corresponding convolution feature maps in the re-identification branch, which results as a self-attention mechanism, provides more discriminative feature for person re-identification. The experimental results on the public dataset PRW demonstrate the effectiveness of the framework.KeywordsPerson searchPerson detectionInstance segmentationPerson re-identificationDeep learning"}, "conf/icip/LyuFH019": {"title": "Esnet: Edge-Based Segmentation Network for Real-Time Semantic Segmentation in Traffic Scenes.", "url": "https://doi.org/10.1109/ICIP.2019.8803132", "year": "2019", "author": {"Haoran Lyu": "254/8127", "Huiyuan Fu": "39/10696", "Xiaojun Hu": "76/1778", "Liang Liu 0001": "10/6178-1"}, "abstract": " Abstract:Semantic segmentation is widely used in the industry recently, especially in the field of scene understanding, surveillance and autonomous driving. However, majority of current state-of-the-art algorithms run accompany with high consumption of computation resources. Thus, our work focuses on real-time semantic segmentation which could reduce a large proportion of computation. Traditional methods to speed up segmentation process tend to down sample image. However, down sampling would cause the loss of information. Hence, we propose a real-time edge-based segmentation network (ESNet) that incorporate high-resolution global edge information with low-resolution classification-level semantic information. Our network performs real-time inference on single GPU card on high-resolution Cityscapes dataset."}, "conf/iconip/YaoWF19": {"title": "Hippocampus Segmentation in MRI Using Side U-Net Model.", "url": "https://doi.org/10.1007/978-3-030-36718-3_12", "year": "2019", "author": {"Wenbin Yao": "80/1996", "Shan Wang": "55/1254", "Huiyuan Fu": "39/10696"}, "abstract": "Convolutional neural networks (CNN) have been applied in medical image analysis over the past few years. U-Net architecture is one of the most well-known CNN architectures in many different medical image segmentation tasks. However, it is hard to capture subtle local features because of its limitations in standard convolution layers and one output prediction. In addition, some objects like hippocampus in the biomedical image occupies an only small area which increases the difficulty of segmentation. In this manuscript, we present an architecture, called Side U-Net, which addresses these challenging problems. In the condition of giving unbalanced class images, Side U-Net outperforms the U-Net by upgrading loss function and capturing more important local features using multiple side outputs. And the experimental results verified our method and demonstrated that our method outperformed the U-Net model over 0.75% in terms of dice score and in the same threshold of classification, our model has a higher TPR (True Positive Rate) when evaluated in ADNI dataset.KeywordsHippocampus segmentationCross entropySide U-Net architecture"}, "conf/seke/WangYF19": {"title": "A Convolutional Neural Network Pruning Method Based On Attention Mechanism.", "url": "https://doi.org/10.18293/SEKE2019-147", "year": "2019", "author": {"Xiao-Jie Wang": "99/7033", "Wenbin Yao": "80/1996", "Huiyuan Fu": "39/10696"}, "abstract": ""}, "conf/bigmm/GengGPF18": {"title": "Multiple Vehicle Detection with Different Scales in Urban Surveillance Video.", "url": "https://doi.org/10.1109/BigMM.2018.8499095", "year": "2018", "author": {"Huan Geng": "229/4045", "Jun Guan": "43/7801", "Hui Pan": "62/1272", "Huiyuan Fu": "39/10696"}, "abstract": " Abstract:With the development of intelligent transportation, mining valuable information such as vehicles in urban surveillance video has been more and more important. In the road monitoring scenes, there is a notable issue that the multiple moving vehicles have a variance of scales which brings difficulty for vehicle detection. In order to solve the problem, we propose a multiple vehicle detection neural network in this paper. Our proposed network has multiple detectors to tackle with the feature maps of different scales. The detectors predict and generate outputs respectively which are finally integrated by non-maximum suppression to eliminate duplicated outputs. The proposed network, named MV-RCNN (Multiscale Vehicle RCNN), owns a structure of full-convolutional network and can be trained end to end. Our proposed method can achieve 78.6% average precision and has better results in the practical urban surveillance video compared with the baseline methods."}, "conf/bigmm/ZhangYCF18": {"title": "Occlusion Region Searching and Segmentation for Multi-Human Detection Based on RGB-D Information.", "url": "https://doi.org/10.1109/BigMM.2018.8499069", "year": "2018", "author": {"Xiaomou Zhang": "229/4029", "Guanghua Yu": "66/7702", "Teng Chen": "03/7721", "Huiyuan Fu": "39/10696"}, "abstract": " Abstract:Accurate multi-human detection is very important for intelligent video surveillance, especially for lots of human-centered applications, e.g. people counting and crowd detection. Currently, most existing algorithms can not solve this problem well due to the heavy occlusions between the humans. To overcome this difficult problem, we propose a new method based on RGB-D information. In the proposed algorithm, we view “Occlusion Region” which consists of multiple humans as a key issue for the whole multi-human detection task. First, we search the candidate occlusion regions with hash searching scheme on the original color image. Then, we segment each searched occlusion region with head-shoulder template matching on the corresponding depth image. Extensive experimental results show that our method can achieve significant improvement comparing to the state-of-the-art methods."}, "conf/icpr/CaoFM18": {"title": "An End-to-End Neural Network for Multi-line License Plate Recognition.", "url": "https://doi.org/10.1109/ICPR.2018.8546200", "year": "2018", "author": {"Yu Cao": "68/6563", "Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217"}, "abstract": " Abstract:Currently, license plate recognition plays an important role in numerous applications and a number of technologies have been proposed. However, most of them can only work with single-line license plates. In the practical application scenarios, there are also existing many multi-line license plates. The traditional approaches need to segment the original input images for double-line license plates. This is a very difficult problem in the complex scenes. In order to solve this problem, we propose an end-to-end neural network for both single-line and double-line license plate recognition. It is segmentation-free for the original input license plate images. We view each of these whole images as a unit on feature maps after deep convolution neural network directly. A large number of experiments show that our method is effective. It is better than the state-of-the-art algorithms in SYSU-ITS license plate library data."}, "conf/mmm/LiLMF17": {"title": "Multi-attribute Based Fire Detection in Diverse Surveillance Videos.", "url": "https://doi.org/10.1007/978-3-319-51811-4_20", "year": "2017", "author": {"Shuangqun Li": "03/8611", "Wu Liu": "31/4112", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696"}, "abstract": "Fire detection, as an immediate response of fire accident to avoid great disaster, has attracted many researchers’ focuses. However, the existing methods cannot effectively exploit the comprehensive attribute of fire to give satisfying accuracy. In this paper, we design a multi-attribute based fire detection system which combines the fire’s color, geometric, and motion attributes to accurately detect the fire in complicated surveillance videos. For geometric attribute, we propose a descriptor of shape variation by combining contour moment and line detection. Furthermore, to utilize fire’s instantaneous motion character, we design a dense optical flow based descriptor as fire’s motion attribute. Finally, we build a fire detection video dataset as the benchmark, which contains 305 fire and non-fire videos, with 135 very challenging negative samples for fire detection. Experimental results on this benchmark demonstrate that the proposed approach greatly outperforms the state-of-the-art method with 92.30% accuracy and only 8.33% false positives.KeywordsFire detectionGeometric attributeMotion estimationContour moment"}, "conf/ccis/PengFC16": {"title": "Distinguishing small moving objects and noises in the barn scene.", "url": "https://doi.org/10.1109/CCIS.2016.7790304", "year": "2016", "author": {"Shufeng Peng": "192/0973", "Huiyuan Fu": "39/10696", "Guangfu Che": "192/1028"}, "abstract": " Abstract:Foreground extraction is a popular research topic in computer vision field. However, it is difficult to distinguish small objects from noises in specific scenes like the barn. This is an important step if we want to count the numbers of objects. Recently, many methods have been proposed for the task. But existing methods cannot satisfy the requirements of real time and accuracy. In this paper, we propose an Advanced Visual Background Extractor (AViBe) algorithm to solve the problem of distinguishing small objects from noises under noisy environment. First we use the traditional background modeling method ViBe to extract the rough foreground. According to the spatial-temporal correlation of moving objects, we filter every pixel which is considered as foreground pixel, it can remove part of the isolated noise pixels. We also restore some missing foreground pixels after the above steps. From the experimental results we can see that our algorithm is effective for distinguishing small objects from noises in the barn scene."}, "conf/ccis/WangFL16": {"title": "Real time abnormal crowd behavior detection based on adjacent flow location estimation.", "url": "https://doi.org/10.1109/CCIS.2016.7790305", "year": "2016", "author": {"Gaoya Wang": "192/1082", "Huiyuan Fu": "39/10696", "Yingxin Liu": "192/1013"}, "abstract": " Abstract:The detection of abnormal behavior of people plays an important role for public security. Recently, some approaches have been proposed towards this task. However, these methods can-not satisfy the requirements of real time abnormal crowd behavior detection in practical application. To address this difficult problem, we propose a novel method based on adjacent flow location estimation. First, we use directional filter to remove image noise and histogram equalization to enhance image contrast. Second, we estimate the motion state of the humans by using the adjacent flow position. Then, we use the location of the crowd to judge whether the crowd is gathering or spreading. Extensive experiments in practical scenes show that our method is effective for real time abnormal crowd behavior detection."}, "conf/icassp/ZhangLMF16": {"title": "Siamese neural network based gait recognition for human identification.", "url": "https://doi.org/10.1109/ICASSP.2016.7472194", "year": "2016", "author": {"Cheng Zhang 0014": "82/6384-14", "Wu Liu": "31/4112", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696"}, "abstract": " Abstract:As the remarkable characteristics of remote accessed, robust and security, gait recognition has gained significant attention in the biometrics based human identification task. However, the existed methods mainly employ the handcrafted gait features, which cannot well handle the indistinctive inter-class differences and large intra-class variations of human gait in real-world situation. In this paper, we have developed a Siamese neural network based gait recognition framework to automatically extract robust and discriminative gait features for human identification. Different from conventional deep neural network, the Siamese network can employ distance metric learning to drive the similarity metric to be small for pairs of gait from the same person, and large for pairs from different persons. In particular, to further learn effective model with limited training data, we composite the gait energy images instead of raw sequence of gaits. Consequently, the experiments on the world's largest gait database show our framework impressively outperforms state-of-the-arts."}, "conf/icmcs/LiuLMF16": {"title": "Large-scale vehicle re-identification in urban surveillance videos.", "url": "https://doi.org/10.1109/ICME.2016.7553002", "year": "2016", "author": {"Xinchen Liu": "36/2028", "Wu Liu": "31/4112", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696"}, "abstract": " Abstract:Vehicle, as a significant object class in urban surveillance, attracts massive focuses in computer vision field, such as detection, tracking, and classification. Among them, vehicle re-identification (Re-Id) is an important yet frontier topic, which not only faces the challenges of enormous intra-class and subtle inter-class differences of vehicles in multicameras, but also suffers from the complicated environments in urban surveillance scenarios. Besides, the existing vehicle related datasets all neglect the requirements of vehicle Re-Id: 1) massive vehicles captured in real-world traffic environment; and 2) applicable recurrence rate to give cross-camera vehicle search for vehicle Re-Id. To facilitate vehicle Re-Id research, we propose a large-scale benchmark dataset for vehicle Re-Id in the real-world urban surveillance scenario, named “VeRi”. It contains over 40,000 bounding boxes of 619 vehicles captured by 20 cameras in unconstrained traffic scene. Moreover, each vehicle is captured by 2~18 cameras in different viewpoints, illuminations, and resolutions to provide high recurrence rate for vehicle Re-Id. Finally, we evaluate six competitive vehicle Re-Id methods on VeRi and propose a baseline which combines the color, texture, and highlevel semantic information extracted by deep neural network."}, "conf/mm/LuMWLF15": {"title": "Automatically Stereoscopic Camera Control for 3D Animation Production.", "url": "https://doi.org/10.1145/2733373.2806369", "year": "2015", "author": {"Dawei Lu": "46/4693", "Huadong Ma": "04/6217", "Zeyu Wang": "132/7882", "Liang Liu 0001": "10/6178-1", "Huiyuan Fu": "39/10696"}, "abstract": "\n\t\tThis paper proposes a novel approach for automatically controlling stereoscopic camera parameters that specifically addresses challenges in stereo 3D animation production process.Our proposed camera control method produces stereo contents with preferable depth perception and guarantees visual comfort by optimization of camera parameters. We introduce an attention tracking method to calculate convergence plane, avoiding window violation and minimizing visual conflict. Moreover, we derive an smoothing function on convergence plane that reduces depth jump over time. Then, we calculate the inter-axial separation using a perceived depth mapping. We describe how to implement our method on the Maya plug-in and test the stereo effect using professional stereo 3D animation scenes. The experimental results, including a user study, show that our method enhances the stereo effect. Our controller provides automatic camera control that can be helpful in creating comfortable and faster stereo 3D animations.\n\t"}, "conf/mmm/ZhangMFW15": {"title": "Outdoor Air Quality Inference from Single Image.", "url": "https://doi.org/10.1007/978-3-319-14442-9_2", "year": "2015", "author": {"Zheng Zhang": "181/2621", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696", "Xinpeng Wang": "156/1668"}, "abstract": "Along with rapid urbanization and industrialization processes, many developing countries are suffering from air pollution. Air quality varies non-linearly, the effective range of an air quality monitoring station is limited. While there are seldom air quality monitoring stations in cities, it is difficult to know the exact air quality of everywhere. How to obtain the air quality fast and conveniently will attract much attention. In this paper, we present an air quality inference approach based on air quality index(AQI) decision tree from a single image. We first extract several corresponding features such as medium transmission, power spectrum slope, contrast, and saturation from the single image. Then we construct a decision tree of AQI values, in accordance with the distance between the features we extract previously. For each none-leaf node of the decision tree, we use five classifiers to choose the next node respectively. We collect a dataset of high quality registered and calibrated images named Outdoor Air Quality Image Set(OAQIS). The dataset covers a wide range of daylight illumination and air pollution conditions. We evaluate our approach on the dataset, the results show the effective of our method.KeywordsOutdoor air auality inferenceoutdoor air quality image setAQI decision tree"}, "conf/mmm/LuMLF15": {"title": "Patch-Based Disparity Remapping for Stereoscopic Images.", "url": "https://doi.org/10.1007/978-3-319-14445-0_43", "year": "2015", "author": {"Dawei Lu": "46/4693", "Huadong Ma": "04/6217", "Liang Liu 0001": "10/6178-1", "Huiyuan Fu": "39/10696"}, "abstract": "Post-production and processing for stereoscopic 3D are attracting a lot attention in recent years. In particular, the acquired disparity in most situations requires further manipulation to adjust different view conditions. This paper proposes a novel method to address the issue of disparity remapping of stereoscopic images. We present a nonlinear disparity mapping model to adjust the depth range of the whole image as well as the special visual important regions. To implement this model, our method compute saliency maps for the stereoscopic images. Then we extend the PatchMatch algorithm to search for the proper patches in both the left and the right images by visual combined constraints, and use them to iteratively refine the images to meet the target depth range. Our method is capable of minimizing the distortion of the images and ensuring the correct stereo consistency after disparity remapping. The experimental results demonstrate that the proposed approach can adjust the depth range to improve the stereoscopic effects while preserving the naturalness of the scene.KeywordsStereoscopic 3Ddisparity remappingvisual importanceimage patch"}, "conf/icdsc/ZhangMFH14": {"title": "Distributed Human Action Recognition via 2D Conditional Random Field.", "url": "https://doi.org/10.1145/2659021.2659030", "year": "2014", "author": {"Zheng Zhang": "181/2621", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696", "Hao Huang": "04/5616"}, "abstract": "\n\t\tThere are two kinds of correlations in multi-view human actions. One is the temporal correlation between adjacent frames, the other is the spatial correlation between different cameras. In this paper, we introduce a discriminative model, two-dimensional Conditional Random Field (2D CRF) which can model the spatial-temporal correlations in actions, and present algorithms for multi-view single person action and two persons interaction recognition based on this model. For the action representation part, we model each action as a bag of visual words based on the spatial-temporal features; for the action recognition part, we use 2D CRF for multi-view single person action and two persons interaction recognition. We use IXMAS and our Multi-view Human Interaction (MHI) datasets for the experiments. The results show the superior performance of the proposed approach over most of the state-of-the-art methods.\n\t"}, "conf/icdsc/LiuMFZ14": {"title": "Vehicle Retrieval and Trajectory Inference in Urban Traffic Surveillance Scene.", "url": "https://doi.org/10.1145/2659021.2659055", "year": "2014", "author": {"Xinchen Liu": "36/2028", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696", "Mo Zhou": "24/4846"}, "abstract": "\n\t\tIn current cities, the number of vehicles grows rapidly especially in developing countries, and the traffic surveillance system usually has tens of thousands of cameras connected into a huge network. Hence the volume of data generated by traffic cameras becomes astronomical. So it is a great challenge to process and utilize these big data resources effectively and efficiently. Towards this end, this paper proposes a system which provides a novel service of vehicle trajectory search for urban traffic surveillance. In this system, smart cameras extract vehicle IDs with time information when vehicles appear in their views and send these information to a data center with very little bandwidth cost. After that, the center server stores and organizes traffic data using two types of tables, camera tables and inverted tables. We fuse vehicle IDs, spatial-temporal data, and topology of urban roads to build a global graph and propose a PathRank algorithm to support the vehicle trajectory search. Experiment results on data from a real city traffic surveillance network validate and evaluate our system.\n\t"}, "conf/mm/FuMX14": {"title": "Crowd Counting via Head Detection and Motion Flow Estimation.", "url": "https://doi.org/10.1145/2647868.2655040", "year": "2014", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Hongtian Xiao": "126/4528"}, "abstract": "\n\t\tCrowd counting with heavy occlusions is highly desired for public security. However, few works have been studied towards this goal. Most previous systems only count passing people robustly without heavy occlusions, else they have to estimate the crowds to a certain extent. To solve this difficult problem, this paper proposes an effective algorithm by combining head detection and motion flow estimation together. First, we detect each head in the crowd by using our proposed scene-adaptive scheme on depth data. Then, we estimate the motion flow based on the interest points in each head region on color data. We can ultimately achieve multi-direction crowd counting results with the loop of above steps. Based on this approach, we have built a practical system for reliable crowd counting. Extensive experimental results show that our system is effective.\n\t"}, "conf/mm/FuM14": {"title": "Real-time crowd detection based on gradient magnitude entropy model.", "url": "https://doi.org/10.1145/2647868.2655043", "year": "2014", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217"}, "abstract": "\n\t\tReliable and real-time crowd detection is one of the most important tasks in intelligent video surveillance system. Previous works focus on counting the number of pedestrians in the crowd directly or use holistic features of crowd scenes for crowd detection. However, the former methods will be invalid in complex crowded scenes, and the latter methods will be confused for feature selection. In this paper, we propose a simple but effective model - Gradient Magnitude Entropy (GME) model for crowd detection. Our model is based on a key observation - the value of GME in a region which will increase as the number of pedestrians grows. Thus, we can estimate the degree of crowd when the value of GME is larger than some threshold, without counting the number of pedestrians. Extensive experiments show that our GME model outperforms state-of-the-art techniques on several challenging datasets. Furthermore, our method can process in real time for practical surveillance applications.\n\t"}, "conf/pcm/LuMF13": {"title": "Efficient Sketch-Based 3D Shape Retrieval via View Selection.", "url": "https://doi.org/10.1007/978-3-319-03731-8_37", "year": "2013", "author": {"Dawei Lu": "46/4693", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696"}, "abstract": "Since the available 3D models are becoming more and more popular, there is an increasing need for effective methods of retrieving 3D models. For this purpose, we propose a sketch-based 3D model retrieval approach using a hand-drawn sketch as a query input. In our approach, we use a combined line rendering technique to represent project views of each 3D model from several predefined viewpoints. Then we compute a descriptor based on the orientation of the feature lines for both the project views and sketches. Our method consists of two processing stages: offline and online. In the offline stage, we use a sketch dataset that corresponding to the query 3D model dataset for each class to measure similarity between the sketches and the view images. With the measurement the preference viewpoints of the 3D model in each class can be selected. In the online stage, we match the query sketch to the selected view images and retrieve the query results. Evaluative experiments demonstrate that our approach is robust against variations of shape, pose and rotation. We compare our method with an approach (DTF-SC) by the first-tier precision, and the results show higher precision for most classes of 3D models.KeywordsSketch-based 3D shape retrievalcombined line renderingpreference viewpoint"}, "conf/icassp/FuM12": {"title": "From video to text: Semantic driving scene understanding using a coarse-to-fine method.", "url": "https://doi.org/10.1109/ICASSP.2012.6288151", "year": "2012", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217"}, "abstract": " Abstract:Semantic understanding from video is one of the most challenging tasks in video analysis. However, it has not been taken enough attention. In this paper, we focus on understanding the semantics of video in the driving scene. We present a coarse-to-fine method to parse the driving scene, and obtain the high-level semantic information of the scene. In the coarse phase, we divide the captured frame into four separate parts based on edge density entropy and scene context. In the fine phase, we join multi-class object segmentation and detection algorithms together in a unified Conditional Random Filed (CRF) model for each part understanding. Moreover, the object probabilistic location prior knowledge based on training and previous edge density entropy result is also integrated into our approach for better object localization. Experimental results show that our proposed method is effective comparing to current state-of-the-art approaches."}, "conf/icip/FuMX12": {"title": "Real-time accurate crowd counting based on RGB-D information.", "url": "https://doi.org/10.1109/ICIP.2012.6467452", "year": "2012", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Hongtian Xiao": "126/4528"}, "abstract": " Abstract:Real-time accurate crowd counting is one of important tasks in intelligent visual surveillance systems. Most previous works can only count passing people robustly without heavy occlusions which are very common in the practical surveillance scenes. To solve this difficult problem, we propose a new method for crowd counting for RGB-D (RGB plus depth) data using a commodity depth camera. In our method, we first detect each head-shoulder of the passing or still person in the surveillance region with fast template matching based on depth information including pedestrian filling with convex hull segmentation. Then, we track and count each detected head-shoulder based on RGB information bidirectionally. By using this approach, we have built a practical system for robust and fast crowd counting. Extensive experimental results show that our method achieves significant improvement comparing to states-of-the-art approach, and the built system is not only robust to heavy occlusions, but also can be deployed in the real time crowd counting application scenes."}, "conf/icpr/FuMW12": {"title": "Night Removal by Color Estimation and sparse representation.", "url": "https://ieeexplore.ieee.org/document/6460957/", "year": "2012", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Shixin Wu": "126/0845"}, "abstract": " Abstract:Night Removal is highly desired in both computational photography and computer vision applications. However, few works have been studied towards this goal. This paper proposes an effective algorithm for removing the night from a single input image. We present a new Color Estimation Model (CEM) for transforming the image from “night” to “day” - along with a guided statistical Dark-to-Day (D2D) prior directing for performance optimization. To restore the noisy and blurred image after CEM, sparse representation based on dozens of corresponding day-time images in different illuminations as dictionary training set is used in our algorithm. Extensive experiments on natural images show our algorithm can achieve convincing results."}, "conf/icmcs/FuMM11": {"title": "EGMM: An enhanced Gaussian mixture model for detecting moving objects with intermittent stops.", "url": "https://doi.org/10.1109/ICME.2011.6012011", "year": "2011", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Anlong Ming": "52/3276"}, "abstract": " Abstract:Moving object detection is one of the most important tasks in intelligent visual surveillance systems. Gaussian Mixture Model (GMM) has been most widely used for moving object detection, because of its robustness to variable scenes. However, to the best of our knowledge, existing GMM based methods can not detect moving objects which gradually stop and keep still state for a while. In this paper, we present an Enhanced Gaussian MixtureModel, called EGMM, to handle this problem. We integrate an Initial Gaussian Background Model (IGBM) and an extended Kalman filter based tracker with GMM, to enhance its performance. Experimental results show that our EGMM based method has a lower miss rate at the same false positives per image comparing to GMM based method for moving pedestrian detection, and it also has a higher detection rate for abandoned object detection comparing to GMM based method."}, "conf/msn/FuML11": {"title": "Robust Human Detection with Low Energy Consumption in Visual Sensor Network.", "url": "https://doi.org/10.1109/MSN.2011.84", "year": "2011", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Liang Liu 0001": "10/6178-1"}, "abstract": " Abstract:In this paper, we try to address the difficult problem of detecting humans robustly with low energy consumption in the visual sensor network. The proposed method contains two parts: one is an ESOBS (Enhanced Self-Organizing Background Subtraction) based foreground segmentation module to obtain active areas in the observed area from the visual sensor; the other is a HOG (Histograms of Oriented Gradients) based detection module to detect the appearance shape from the foreground areas. Moreover, we create a large pedestrian dataset according to the specific scene in visual sensor networks. Numerous experiments are conducted. The experimental results show the effectiveness of our method."}, "journals/corr/abs-2205-00214": {"title": "Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer.", "url": "https://doi.org/10.48550/arXiv.2205.00214", "year": "2022", "author": {"Wulian Yun": "319/5424", "Mengshi Qi": "191/2586", "Chuanming Wang": "266/3767", "Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217"}, "abstract": "\n      Abstract:  Video denoising aims to recover high-quality frames from the noisy video.\nWhile most existing approaches adopt convolutional neural networks(CNNs) to\nseparate the noise from the original visual content, however, CNNs focus on\nlocal information and ignore the interactions between long-range regions.\nFurthermore, most related works directly take the output after spatio-temporal\ndenoising as the final result, neglecting the fine-grained denoising process.\nIn this paper, we propose a Dual-stage Spatial-Channel Transformer (DSCT) for\ncoarse-to-fine video denoising, which inherits the advantages of both\nTransformer and CNNs. Specifically, DSCT is proposed based on a progressive\ndual-stage architecture, namely a coarse-level and a fine-level to extract\ndynamic feature and static feature, respectively. At both stages, a\nSpatial-Channel Encoding Module(SCEM) is designed to model the long-range\ncontextual dependencies at spatial and channel levels. Meanwhile, we design a\nMulti-scale Residual Structure to preserve multiple aspects of information at\ndifferent stages, which contains a Temporal Features Aggregation Module(TFAM)\nto summarize the dynamic representation. Extensive experiments on four publicly\navailable datasets demonstrate our proposed DSCT achieves significant\nimprovements compared to the state-of-the-art methods.\n\n    "}, "journals/corr/abs-2208-14646": {"title": "An Empirical Study and Analysis of Learning Generalizable Manipulation Skill in the SAPIEN Simulator.", "url": "https://doi.org/10.48550/arXiv.2208.14646", "year": "2022", "author": {"Kun Liu 0016": "06/2592-16", "Huiyuan Fu": "39/10696", "Zheng Zhang": "181/2621", "Huanpu Yin": "163/0491"}, "abstract": "\n      Abstract:  This paper provides a brief overview of our submission to the no interaction\ntrack of SAPIEN ManiSkill Challenge 2021. Our approach follows an end-to-end\npipeline which mainly consists of two steps: we first extract the point cloud\nfeatures of multiple objects; then we adopt these features to predict the\naction score of the robot simulators through a deep and wide transformer-based\nnetwork. More specially, %to give guidance for future work, to open up avenues\nfor exploitation of learning manipulation skill, we present an empirical study\nthat includes a bag of tricks and abortive attempts. Finally, our method\nachieves a promising ranking on the leaderboard. All code of our solution is\navailable at this https URL\\_codes.\n\n    "}, "journals/cvm/LiuFM21": {"title": "An end-to-end convolutional network for joint detecting and denoising adversarial perturbations in vehicle classification.", "url": "https://doi.org/10.1007/s41095-021-0202-3", "year": "2021", "author": {"Peng Liu": "21/6121", "Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217"}, "abstract": "Deep convolutional neural networks (DCNNs) have been widely deployed in real-world scenarios. However, DCNNs are easily tricked by adversarial examples, which present challenges for critical applications, such as vehicle classification. To address this problem, we propose a novel end-to-end convolutional network for joint detection and removal of adversarial perturbations by denoising (DDAP). It gets rid of adversarial perturbations using the DDAP denoiser based on adversarial examples discovered by the DDAP detector. The proposed method can be regarded as a pre-processing step—it does not require modifying the structure of the vehicle classification model and hardly affects the classification results on clean images. We consider four kinds of adversarial attack (FGSM, BIM, DeepFool, PGD) to verify DDAP’s capabilities when trained on BIT-Vehicle and other public datasets. It provides better defense than other state-of-the-art defensive methods."}, "journals/cvm/FuZM21": {"title": "See clearly on rainy days: Hybrid multiscale loss guided multi-feature fusion network for single image rain removal.", "url": "https://doi.org/10.1007/s41095-021-0210-3", "year": "2021", "author": {"Huiyuan Fu": "39/10696", "Yu Zhang": "50/671", "Huadong Ma": "04/6217"}, "abstract": "The quality of photos is highly susceptible to severe weather such as heavy rain; it can also degrade the performance of various visual tasks like object detection. Rain removal is a challenging problem because rain streaks have different appearances even in one image. Regions where rain accumulates appear foggy or misty, while rain streaks can be clearly seen in areas where rain is less heavy. We propose removing various rain effects in pictures using a hybrid multiscale loss guided multiple feature fusion de-raining network (MSGMFFNet). Specially, to deal with rain streaks, our method generates a rain streak attention map, while preprocessing uses gamma correction and contrast enhancement to enhanced images to address the problem of rain accumulation. Using these tools, the model can restore a result with abundant details. Furthermore, a hybrid multiscale loss combining L1 loss and edge loss is used to guide the training process to pay attention to edge and content information. Comprehensive experiments conducted on both synthetic and real-world datasets demonstrate the effectiveness of our method."}, "journals/remotesensing/FuZYFLM21": {"title": "Joint Geoeffectiveness and Arrival Time Prediction of CMEs by a Unified Deep Learning Framework.", "url": "https://doi.org/10.3390/rs13091738", "year": "2021", "author": {"Huiyuan Fu": "39/10696", "Yuchao Zheng": "279/5881", "Yudong Ye": "293/3251", "Xueshang Feng": "60/9859", "Chaoxu Liu": "293/3905", "Huadong Ma": "04/6217"}, "abstract": ""}, "journals/ijon/FuMWZZ20": {"title": "MCFF-CNN: Multiscale comprehensive feature fusion convolutional neural network for vehicle color recognition based on residual learning.", "url": "https://doi.org/10.1016/j.neucom.2018.02.111", "year": "2020", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Gaoya Wang": "192/1082", "Xiaomou Zhang": "229/4029", "Yifan Zhang": "57/4707"}, "abstract": "Automatic vehicle color recognition is very important for video surveillance, especially for intelligent transportation system. Currently, some approaches have been proposed. However, it is still very difficult to recognize the vehicle color correctly in the complex traffic scenes with constantly changing illuminations. To solve this problem, we propose a new network structure - Multiscale Comprehensive Feature Fusion Convolutional Neural Network (MCFF-CNN) based on residual learning for color feature extraction. First, we use MCFF-CNN network to extract the deep color features of the vehicles. Then, we employ support vector machine (SVM) classifier to obtain the final color recognition results. Based on the proposed approach, we have built a system for robust vehicle color recognition in practical traffic scenes. Extensive experimental results show our solution is effective."}, "journals/nn/CaiYQFH20": {"title": "FOM: Fourth-order moment based causal direction identification on the heteroscedastic data.", "url": "https://doi.org/10.1016/j.neunet.2020.01.006", "year": "2020", "author": {"Ruichu Cai": "09/6889", "Jincheng Ye": "260/1725", "Jie Qiao": "00/7723", "Huiyuan Fu": "39/10696", "Zhifeng Hao": "94/6214"}, "abstract": "Identification of the causal direction is a fundamental problem in many scientific research areas. The independence between the noise and the cause variable is a widely used assumption to identify the causal direction. However, such an independence assumption is usually violated due to heteroscedasticity of the real-world data. In this paper, we propose a new criterion for the causal direction identification which is robust to the heteroscedasticity of the data. In detail, the fourth-order moment of noise is proposed to measure the asymmetry between the cause and effect. A heteroscedastic Gaussian process regression-based estimation of the fourth-order moment is proposed accordingly. Under some commonly used assumptions of the causal mechanism, we theoretically show that the noise’s fourth-order moment of the causal direction is smaller than that of the anti-causal direction. Experimental results on both simulated and real-world data illustrate the efficiency of the proposed approach."}, "journals/ijon/ZhangMFZ16": {"title": "Scene-free multi-class weather classification on single images.", "url": "https://doi.org/10.1016/j.neucom.2016.05.015", "year": "2016", "author": {"Zheng Zhang": "181/2621", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696", "Cheng Zhang 0014": "82/6384-14"}, "abstract": "Multi-class weather classification is a fundamental and significant technique which has many potential applications, such as video surveillance and intelligent transportation. However, it is a challenging task due to the diversity of weather and lack of discriminate feature. Most existing weather classification methods only consider two-class weather conditions such as sunny-rainy or sunny-cloudy weather. Moreover, they predominantly focus on a fixed scene such as popular tourism and traffic scenario. In this paper, we propose a novel method for scene-free multi-class weather classification from single images based on multiple category-specific dictionary learning and multiple kernel learning. To improve the discrimination of image representation and enhance the performance of multiple weather classification, our approach extracts multiple weather features and learns dictionaries based on these features. To select a good subset of features, we utilize multiple kernel learning algorithm to learn an optimal linear combination of feature kernels. In addition, to evaluate the proposed approach, we collect an outdoor image set that contains 20 K images, called MWI (Multi-class Weather Image) set. Experimental results show the effectiveness of the proposed method."}, "journals/ijon/FuMLL16": {"title": "A vehicle classification system based on hierarchical multi-SVMs in crowded traffic scenes.", "url": "https://doi.org/10.1016/j.neucom.2015.12.134", "year": "2016", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Yinxin Liu": "188/1385", "Dawei Lu": "46/4693"}, "abstract": "Automatic vehicle classification is very important for video surveillance, especially for intelligent transportation system. Currently, some approaches have been proposed. However, almost all of these methods cannot play well in the practical crowded traffic scenes with heavy occlusions, shadows, and different views, etc. To solve this difficult problem, we propose a new vehicle classification method based on hierarchical multi-SVMs. First, we extract the foreground objects from the surveillance videos. Then, we use the proposed hierarchical multi-SVMs method for vehicle classification. Moreover, we present a voting based correction scheme by tracking the classified vehicles for the final precision. Based on the proposed approach, we have built a practical system for robust vehicle classification in complicated traffic scenes. Extensive experimental results show that our solution can achieve convincing results."}, "journals/mis/ZhangMFLZ16": {"title": "Outdoor Air Quality Level Inference via Surveillance Cameras.", "url": "https://doi.org/10.1155/2016/9825820", "year": "2016", "author": {"Zheng Zhang": "181/2621", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696", "Liang Liu 0001": "10/6178-1", "Cheng Zhang 0014": "82/6384-14"}, "abstract": ""}, "journals/percom/YuanMF15": {"title": "Hotspot-entropy based data forwarding in opportunistic social networks.", "url": "https://doi.org/10.1016/j.pmcj.2014.06.003", "year": "2015", "author": {"Peiyan Yuan": "72/906", "Huadong Ma": "04/6217", "Huiyuan Fu": "39/10696"}, "abstract": "Performance of data forwarding in opportunistic social networks benefits considerably if one can make use of human mobility in terms of social contexts. However, it is difficult and time-consuming to calculate the centrality and similarity of nodes by using solutions of traditional social networks analysis, this is mainly because of the transient node contact and the intermittently connected link. In this paper, we are interested in the following question: Can we exploit some other stable social attributes to quantify the centrality and similarity of nodes? Aggregating GPS traces of human walks from the real world, we find that there exist two types of phenomena. One is public hotspot, the other is personal hotspot. Motivated by this observation, we propose Hotent (HOTspot-ENTropy), a novel data forwarding metric to improve the performance of opportunistic routing. First, we use the relative entropy between the public hotspots and the personal hotspots to compute node centrality. Second, we utilize the inverse symmetrized entropy of the personal hotspots between two nodes to evaluate their similarity. Third, we integrate the two social metrics by using the law of universal gravitation. Besides, we use the entropy of personal hotspots of a node to characterize its personality. Finally, we compare our routing strategy with the state-of-the-art works through extensive trace-driven simulations, the results show that Hotent largely outperforms other solutions, especially in terms of packet delivery ratio and the average number of hops per message."}, "journals/mta/FuMX14": {"title": "Scene-adaptive accurate and fast vertical crowd counting via joint using depth and color information.", "url": "https://doi.org/10.1007/s11042-013-1608-4", "year": "2014", "author": {"Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217", "Hongtian Xiao": "126/4528"}, "abstract": "Reliable and real-time crowd counting is one of the most important tasks in intelligent visual surveillance systems. Most previous works only count passing people based on color information. Owing to the restrictions of color information influences themselves for multimedia processing, they will be affected inevitably by the unpredictable complex environments (e.g. illumination, occlusion, and shadow). To overcome this bottleneck, we propose a new algorithm by multimodal joint information processing for crowd counting. In our method, we use color and depth information together with a ordinary depth camera (e.g. Microsoft Kinect). Specifically, we first detect each head of the passing or still person in the surveillance region with adaptive modulation ability to varying scenes on depth information. Then, we track and count each detected head on color information. The characteristic advantage of our algorithm is that it is scene adaptive, which means the algorithm can be applied into all kinds of different scenes directly without additional conditions. Based on the proposed approach, we have built a practical system for robust and fast crowd counting facing complicated scenes. Extensive experimental results show the effectiveness of our proposed method."}}}