{"id": "37/2749-1", "name": "Jingyu Wang 0001", "Article": {"conf/infocom/ZhaoHZZQWSL22": {"title": "QUIC-Enabled Data Aggregation for Short Packet Communication in mMTC.", "url": "https://doi.org/10.1109/INFOCOMWKSHPS54753.2022.9798322", "year": "2022", "author": {"Haoran Zhao": "141/4084", "Bo He": "04/2868", "He Zhou": "33/9985", "Jiangyin Zhou": "323/0152", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:In this paper, we focus on the Short Packet Communication (SPC) in the typical massive Machine Type Communication (mMTC) scenario of 5G/6G networks. For the conventional scheme, tremendous Machine Type Communication Devices (MTCDs) send short status update packets directly to the central Base Station (BS) using the Transmission Control Protocol (TCP), which leads to a huge burden on the BS and may cause severe communication congestion. To solve this problem, we propose a frame-level data aggregation SPC scheme based on the Quick UDP Internet Connection (QUIC) protocol. By using the stream multiplexing feature of QUIC, some MTCDs selected as aggregators receive the short status update packets from their neighboring MTCDs, pack the data into new QUIC packets, and forward these new packets to the BS. The QUIC-based SPC scheme is evaluated in the 5G network environment. It is proved that our scheme reduces the communication overhead of the BS by about 10% and the computing burden of the BS by average 40% in CPU usage."}, "conf/mir/Zhu0Z0SL22": {"title": "FedNKD: A Dependable Federated Learning Using Fine-tuned Random Noise and Knowledge Distillation.", "url": "https://doi.org/10.1145/3512527.3531372", "year": "2022", "author": {"Shaoxiong Zhu": "322/8825", "Qi Qi 0001": "80/6406-1", "Zirui Zhuang": "235/7014", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": "\n\t\tMultimedia retrieval models need the ability to extract useful information from large-scale data for clients. As an important part of multimedia retrieval, image classification model directly affects the efficiency and effect of multimedia retrieval. We need a lot of data to train a image classification model applied to multimedia retrieval task. However, with the protection of data privacy, the data used to train the model often needs to be kept on the client side. Federated learning is proposed to use data from all clients to train one model while protecting privacy. When federated learning is applied, the distribution of data across different clients varies greatly. Disregarding this problem yields a final model with unstable performance. To enable federated learning to work dependably in the real world with complex data environments, we propose FedNKD, which utilizes knowledge distillation and random noise. The superior knowledge of each client is distilled into a central server to mitigate the instablity caused by Non-IID data. Importantly, a synthetic dataset is created by some random noise through back propagation of neural networks. The synthetic dataset will contain the abstract features of the real data. Then we will use this synthetic dataset to realize the knowledge distillation while protecting users' privacy. In our experimental scenarios, FedNKD outperforms existing representative algorithms by about 1.5% in accuracy.\n\t"}, "conf/aaai/GaoWLWZL21": {"title": "Question-Driven Span Labeling Model for Aspect-Opinion Pair Extraction.", "url": "https://ojs.aaai.org/index.php/AAAI/article/view/17523", "year": "2021", "author": {"Lei Gao": "44/2139", "Yulong Wang": "97/5856", "Tongcun Liu": "219/4242", "Jingyu Wang 0001": "37/2749-1", "Lei Zhang 0094": "97/8704-94", "Jianxin Liao": "60/4951"}, "abstract": "Aspect term extraction and opinion word extraction are two fundamental subtasks of aspect-based sentiment analysis. The internal relationship between aspect terms and opinion words is typically ignored, and information for the decision-making of buyers and sellers is insufficient. In this paper, we explore an aspect–opinion pair extraction (AOPE) task and propose a Question-Driven Span Labeling (QDSL) model to extract all the aspect–opinion pairs from user-generated reviews. Specifically, we divide the AOPE task into aspect term extraction (ATE) and aspect-specified opinion extraction (ASOE) subtasks; we first extract all the candidate aspect terms and then the corresponding opinion words given the aspect term. Unlike existing approaches that use the BIO-based tagging scheme for extraction, the QDSL model adopts a span-based tagging scheme and builds a question–answer-based machine-reading comprehension task for an effective aspect–opinion pair extraction. Extensive experiments conducted on three tasks (ATE, ASOE, and AOPE) on four benchmark datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches.\n\t\t\t\t"}, "conf/acml/ZhanCRS00L21": {"title": "Spatial Temporal Enhanced Contrastive and Pretext Learning for Skeleton-based Action Representation.", "url": "https://proceedings.mlr.press/v157/zhan21a.html", "year": "2021", "author": {"Yiwen Zhan": "309/1150", "Yuchen Chen": "33/8253", "Pengfei Ren": "31/10610", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": ""}, "conf/bigdataconf/ChengS00Q21": {"title": "Parallel Decoders Guided Lexically Constrained Response Generation.", "url": "https://doi.org/10.1109/BigData52589.2021.9671855", "year": "2021", "author": {"Daixuan Cheng": "289/2865", "Haifeng Sun 0001": "00/11044-1", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Yan Qi": "23/3508"}, "abstract": " Abstract:Response generation is a fundamental function in conversational systems, where controllability of the response is a key problem. In this paper, we consider how to control the response by lexical constraints, namely lexically constrained response generation. The stochastic search-based methods have achieved promising performance in satisfying lexical constraints. The idea of these methods is modifying a sentence through the actions of insertion, deletion and replacement guided by an optimization algorithm. The core of our method is modifying the response by incorporating the lexical constraints and preserving the message-related parts. For this purpose, we propose the novel Parallel Decoders to guide response modification. The first decoder generates responses according to the given message and constraints. The second decoder calculates the relevance between the input and the response. Based on Parallel Decoders, during the modification, we could sample the positions in the response for editing according to the relevance score. Experiments show the proposed framework achieves better performance than the state-of-the-art generation models in terms of constraint relevance, sentence fluency, response diversity and human evaluation."}, "conf/icdcs/YangQWGL21": {"title": "Towards Efficient Inference: Adaptively Cooperate in Heterogeneous IoT Edge Cluster.", "url": "https://doi.org/10.1109/ICDCS51616.2021.00011", "year": "2021", "author": {"Xiang Yang": "56/5739", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Song Guo 0001": "01/267-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:New applications such as smart homes, autonomous vehicles are leading an increasing research topic of convolutional neural network (CNN) based inference on IoT edge devices. Unfortunately, this scenario meets a huge roadblock caused by the limited computing resources owned by these devices. One popular solution is to execute inference on an edge cluster with parallelization schemes instead of on a single device. However, the heterogeneous edge devices and varied neural layers bring challenges to this process. In this paper, we propose a pipelined cooperation scheme (PICO) to efficiently execute CNN inference for edge devices. Our goal is to maximize throughput by reducing redundant computing meanwhile to keep the inference latency under a certain value. PICO divides the neural layers and edge devices into several stages. The input data is fed into the first stage and the inference result is produced at the last stage. These stages compose an inference pipeline. The execution time of each stage is optimized to approach the maximum throughput as close as possible. We also implement an adaptive framework to choose the best inference scheme under different workloads. In our experiment with 8 RaspberryPi devices, the average inference latency can be reduced by \n1.7∼6.5×1.7\\sim 6.5\\times\n under different workloads, and the throughput can be improved by \n1.8∼6.2×1.8\\sim 6.2\\times\n under various network settings."}, "conf/icde/Chu00STL21": {"title": "Prefix-Graph: A Versatile Log Parsing Approach Merging Prefix Tree with Probabilistic Graph.", "url": "https://doi.org/10.1109/ICDE51399.2021.00274", "year": "2021", "author": {"Guojun Chu": "295/6967", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Shimin Tao": "188/1236", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Logs play an important part in analyzing system behavior and diagnosing system failures. As the basic step of log analysis, log parsing converts raw log messages into structured log templates. However, existing log parsing approaches are not adaptive and versatile enough to ensure their high accuracy on all types of datasets. In particular, it is required to design regular expressions or fine-tune the hyper-parameters manually for the best performance. In this paper, we propose Prefix-Graph, an online versatile log parsing approach. Prefix-Graph is a probabilistic graph structure extended from prefix tree. It iteratively merges together two branches which have high similarity in probability distribution, and represents log templates as the combination of cut-edges in root-to-leaf paths of the graph. Since no domain knowledge is used and all the parameters are fixed, Prefix-Graph can be easily applied to different log datasets without any additional manual work. We evaluate our approach on 10 real-world datasets and 117GB log messages obtained from Huawei. The experimental results demonstrate that Prefix-Graph achieves the highest average accuracy of 0.975 and the smallest standard deviation of 0.037. Our approach is superior to baseline methods in terms of adaptability and versatility."}, "conf/icict/GuoS0021": {"title": "Keyword Extraction Algorithm Based on Pre-training and Multi-task Training.", "url": "https://doi.org/10.1007/978-981-16-2377-6_67", "year": "2021", "author": {"Lingqi Guo": "302/8664", "Haifeng Sun 0001": "00/11044-1", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "The generalization ability of the supervised model is relatively weak in keyword extraction technology. For enhancing the robustness of the model, a keyword extraction method is proposed inspired by the pre-training model. After pre-training with plenty of corpus and fine-tuning with specific datasets, the proposed method performs more robust in keyword extraction tasks. In addition, multi-task training is added in the fine-tuning stage to improve the accuracy of the model. Plenty of comparative experiments show that the proposed method is very significant in improving the robustness and accuracy of the model.KeywordsKeyword extractionPre-trainingMulti-task trainingModel robustnessTransfer learning"}, "conf/ipccc/Chen00SL21": {"title": "Accelerating DNN Inference by Edge-Cloud Collaboration.", "url": "https://doi.org/10.1109/IPCCC51483.2021.9679434", "year": "2021", "author": {"Jianan Chen 0010": "311/8913", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Deep neural networks (DNN) have become indispensable tools for intelligent applications today. The demand for deploying DNN on the edge devices increases dramatically. Unfortunately, it is challenging because the DNN inference is computation-intensive, but edge devices are always resource-constraint. Prior solutions attempted to address these challenges with collaboration between cloud and edge devices, but they do not take the inference request rate into account. However, the inference delay will increase dramatically while the request rate becomes higher.In this paper, we propose a scheme to dynamic partition DNN into two or three parts and distribute them at the edge and cloud, achieving the lowest delay with the change of request rate. The scheme selects the optimal partition points of DNN with a layer evaluation model (LEM) and a total delay prediction model (DPM) under different request rates. The experiments of distributed deploying AlexNet, VGG, NiN and ResNet DNN models on image classification dataset ImageNet show that the proposed scheme significantly reduces the total end-to-end latency by fully using both the edge and cloud resources. It reduces the inference delay by 1.3 to 1.6 times and improves the throughput 1.2 to 1.7 times compared to the state of art partition approach."}, "conf/ismar/ZhengRS00L21": {"title": "SAR: Spatial-Aware Regression for 3D Hand Pose and Mesh Reconstruction from a Monocular RGB Image.", "url": "https://doi.org/10.1109/ISMAR52148.2021.00024", "year": "2021", "author": {"Xiaozheng Zheng": "305/5577", "Pengfei Ren": "31/10610", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:3D hand reconstruction is a popular research topic in recent years, which has great potential for VR/AR applications. However, due to the limited computational resource of VR/AR equipment, the reconstruction algorithm must balance accuracy and efficiency to make the users have a good experience. Nevertheless, current methods are not doing well in balancing accuracy and efficiency. Therefore, this paper proposes a novel framework that can achieve a fast and accurate 3D hand reconstruction. Our framework relies on three essential modules, including spatial-aware initial graph building (SAIGB), graph convolutional network (GCN) based belief maps regression (GBBMR), and pose-guided refinement (PGR). At first, given image feature maps extracted by convolutional neural networks, SAIGB builds a spatial-aware and compact initial feature graph. Each node in this graph represents a vertex of the mesh and has vertex-specific spatial information that is helpful for accurate and efficient regression. After that, GBBMR first utilizes adaptive-GCN to introduce interactions between vertices to capture short-range and long-range dependencies between vertices efficiently and flexibly. Then, it maps vertices’ features to belief maps that can model the uncertainty of predictions for more accurate predictions. Finally, we apply PGR to compress the redundant vertices’ belief maps to compact-joints’ belief maps with the pose guidance and use these joints’ belief maps to refine previous predictions better to obtain more accurate and robust reconstruction results. Our method achieves state-of-the-art performance on four public benchmarks, FreiHAND, HO-3D, RHD, and STB. Moreover, our method can run at a speed of two to three times that of previous state-of-the-art methods. Our code is available at https://github.com/zxz267/SAR."}, "conf/ispa/GuiYYLZQWSL21": {"title": "Grouping Synchronous to Eliminate Stragglers with Edge Computing in Distributed Deep Learning.", "url": "https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00066", "year": "2021", "author": {"Zhiyi Gui": "309/9453", "Xiang Yang": "56/5739", "Hao Yang": "54/4089", "Wei Li": "64/6025", "Lei Zhang": "97/8704", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:With the development of artificial intelligence(AI) applications, a large number of data are generated from mobile or IoT devices at the edge of the network. Deep learning tasks are executed to obtain effective information in the user data. However, the edge nodes are heterogeneous and the network bandwidth is limited in this case, which will cause general distributed deep learning to be inefficient. In this paper, we propose Group Synchronous Parallel (GSP), which uses a density-based algorithm to group edge nodes with similar training speeds together. In order to eliminate stragglers, group parameter servers are responsible for coordinating communication of nodes in the group with Stale Synchronous Parallel and aggregating the gradients of these nodes. And a global parameter server is responsible for aggregating the gradients from the group parameter servers to update the global model. To save network bandwidth, we further propose Grouping Dynamic Sparsification (GDS). It adjusts the gradient sparsification rate of nodes dynamically based on GSP so as to differentiates the communication volume and makes the training speed of all nodes tend to be the same. We evaluate GSP and GDS’s performance on LeNet-5, ResNet, VGG, and Seq2Seq with Attention. The experimental results show that GSP speedups the training by 45% ~ 120% with 16 nodes. GDS on top of GSP can make up for some test accuracy loss, up to 0.82% for LeNet-5."}, "conf/ksem/LiuLWQS21": {"title": "Context-Aware Anomaly Detection in Attributed Networks.", "url": "https://doi.org/10.1007/978-3-030-82153-1_2", "year": "2021", "author": {"Ming Liu": "20/2039", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1"}, "abstract": "Anomaly detection in attributed networks has received increasing attention due to its broad applications in various high-impact domains. Compared to traditional anomaly detection, the main challenge of this task lies in how to integrate the network structure and node attributes to spot anomalies. However, existing methods attempt to integrate two kinds of information into a fixed representation and neglect the contextual information. Specifically, a fixed feature vector is directly adopted to evaluate its abnormality without considering the node’s diverse roles when interacting with different neighbors. In this paper, we propose a novel Context-Aware Anomaly Detection (CAAD) framework in attributed networks. CAAD derives context-aware embeddings for each node pair with a mutual attention mechanism. The embeddings extracted by feature interactions can concentrate on the most relevant attributes of network structures. Numerous context information provides us with multiple perspectives to understand the structure connection and detect local anomaly structure. Moreover, we develop an anomaly gated mechanism to assign global anomalous scores to node pairs. The anomalous scores are learnable and applied to reduce the adverse effect of anomalies during the training process. By jointly optimizing network embeddings and anomaly gated mechanism, our model can spot anomalies in local and global collaborations. Experiments on various real-world network datasets indicate that the proposed model achieves state-of-the-art results.KeywordsAttributed networksAnomaly detectionContext-aware"}, "conf/ksem/MuQWSL21": {"title": "Efficient Depth Completion Network Based on Dynamic Gated Fusion.", "url": "https://doi.org/10.1007/978-3-030-82153-1_24", "year": "2021", "author": {"Zhengyang Mu": "299/7109", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": "Depth completion aims to recover dense depth maps from sparse depth and RGB images. Current methods achieve high accuracy at the cost of large model size and huge computation complexity, which prevent them from wider applications. In this paper, we focus on making two key issues on depth completion – feature extraction and fusion – more efficient to achieve superior trade-off in model size and accuracy: (1) we propose efficient dual-branch encoder by exploring data characteristics of different modalities which can greatly reduce the model size and inference time; (2) we propose a dynamic gated fusion module, which is guided by input sparse depth to fuse information of both RGB and sparse depth feature more efficiently by generating dynamic fusing weights. Experiments on KITTI Depth Completion and NYU Depth v2 show that our method achieves 3.5x - 10x speedup against the state-of-art method, 9x param compressing and comparable accuracy compared with state-of-the-art methods, which shows our method achieves good trade-off between performance and speed.KeywordsDepth completionFeature fusionLight-weighted modelAutonomous drivingRobotics"}, "conf/mm/XuS00GZL21": {"title": "DLA-Net for FG-SBIR: Dynamic Local Aligned Network for Fine-Grained Sketch-Based Image Retrieval.", "url": "https://doi.org/10.1145/3474085.3475705", "year": "2021", "author": {"Jiaqing Xu": "57/3028", "Haifeng Sun 0001": "00/11044-1", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Ce Ge": "215/0142", "Lejian Zhang": "188/8007", "Jianxin Liao": "60/4951"}, "abstract": "\n\t\tFine-grained sketch-based image retrieval is considered as an ideal alternative to keyword-based image retrieval and image search by image due to the rich and easily accessible characteristics of sketches. Previous works always follow a paradigm that first extracting image global feature with convolution neural network and then optimizing the model with triplet loss. Many efforts on narrowing the domain gap and extracting discriminating features are made by these works. However, they ignored that the global feature is not good at capturing fine-grained details. In this paper, we emphasize the local features are more discriminating than global feature in FG-SBIR and explore an effective way to utilize local features. Specifically, Local Aligned Network (LA-Net) is proposed first, which solves FG-SBIR by directly aligning the mid-level local features. Experiment manifests it can beat all previous baselines and is easy to implement. LA-Net is hoped to be a new strong baseline for FG-SBIR. Next, Dynamic Local Aligned Network (DLA-Net) is proposed to enhance LA-Net. The question of spatial misalignment caused by the abstraction of the sketch is not considered by LA-Net. To solve this question, a dynamic alignment mechanism is introduced into LA-Net. This new mechanism makes the sketch interact with the photo and dynamically decide where to align according to the different photos. The Experiment indicates DLA-Net successfully addresses the question of spatial misalignment. It gains a significant performance boost over LA-Net and outperforms the state-of-the-art in FG-SBIR. To the best of our knowledge, DLA-Net is the first model that beats humans on all datasets---QMUL FG-SBIR, QMUL Handbag, and Sketchy.\n\t"}, "conf/networking/Feng00L21": {"title": "Few-Shot Class-Adaptive Anomaly Detection with Model-Agnostic Meta-Learning.", "url": "https://doi.org/10.23919/IFIPNetworking52078.2021.9472814", "year": "2021", "author": {"Tongtong Feng": "248/6095", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Anomaly detection in encrypted traffic is a growing problem, and many approaches have been proposed to solve it. However, those approaches need to be trained in the massive of normal traffic and specific-class abnormal traffic, so to achieve good results in that specific-class. For a new anomaly class with few labeled samples, the effectiveness of existing approaches will decline sharply. How to train a model using only a few anomaly samples to detect unseen new anomaly classes in training is a huge challenge. In this paper, we propose a Few-shot Class-adaptive Anomaly Detection framework (FCAD) with model-agnostic meta-learning (MAML) to meet this challenge. Given an input network flow, FCAD first extracts statistical features by feature extractor and feature selector, and time-series features using LSTM-based AutoEncoder. Then, FCAD designs a MAML-based few-shot anomaly detection model, relying on the episodic training paradigm and learning from the collection of K-way-M-shot classification tasks, which can mimic the few-shot regime faced at test time during training. Finally, FCAD uses the pre-trained model to adapt the new class by a few iterations steps. Our goal is to detect anomaly traffic in a before unseen anomaly class with only a few samples. A reliable solution to few-shot anomaly detection will have huge potential for real-world applications since it is expensive and arduous to collect a massive amount of data onto the new anomaly class; extensive experimental results demonstrate the effectiveness of our proposed approach."}, "conf/sigir/MaL0HYCWCL21": {"title": "Distant Supervision based Machine Reading Comprehension for Extractive Summarization in Customer Service.", "url": "https://doi.org/10.1145/3404835.3463046", "year": "2021", "author": {"Bing Ma": "53/5636", "Cao Liu": "26/6730", "Jingyu Wang 0001": "37/2749-1", "Shujie Hu": "76/5268", "Fan Yang": "29/3081", "Xunliang Cai": "294/8410", "Guanglu Wan": "256/1614", "Jiansong Chen": "36/3528", "Jianxin Liao": "60/4951"}, "abstract": "\n\t\tGiven a long text, the summarization system aims to obtain a shorter highlight while keeping important information on the original text. For customer service, the summaries of most dialogues between an agent and a user focus on several fixed key points, such as user's question, user's purpose, the agent's solution, and so on. Traditional extractive methods are difficult to extract all predefined key points exactly. Furthermore, there is a lack of large-scale and high-quality extractive summarization datasets containing key points. In order to solve the above challenges, we propose a Distant Supervision based Machine Reading Comprehension model for extractive Summarization (DSMRC-S). DSMRC-S transforms the summarization task into the machine reading comprehension problem, to fetch key points from the original text exactly according to the predefined questions. In addition, a distant supervision method is proposed to alleviate the lack of eligible extractive summarization datasets. We conduct experiments on a large-scale summarization dataset collected in customer service scenarios, and the results show that the proposed DSMRC-S outperforms the strong baseline methods by 4 points on ROUGE-L.\n\t"}, "conf/aaai/HuangRW0S20": {"title": "AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation.", "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6761", "year": "2020", "author": {"Weiting Huang": "262/0438", "Pengfei Ren": "31/10610", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1"}, "abstract": "In this paper, we propose an adaptive weighting regression (AWR) method to leverage the advantages of both detection-based and regression-based method. Hand joint coordinates are estimated as discrete integration of all pixels in dense representation, guided by adaptive weight maps. This learnable aggregation process introduces both dense and joint supervision that allows end-to-end training and brings adaptability to weight maps, making network more accurate and robust. Comprehensive exploration experiments are conducted to validate the effectiveness and generality of AWR under various experimental settings, especially its usefulness for different types of dense representation and input modality. Our method outperforms other state-of-the-art methods on four publicly available datasets, including NYU, ICVL, MSRA and HANDS 2017 dataset.\n\t\t\t\t"}, "conf/acl/DuSWQL20": {"title": "Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis.", "url": "https://doi.org/10.18653/v1/2020.acl-main.370", "year": "2020", "author": {"Chunning Du": "254/8020", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": ""}, "conf/ecai/LiuL00S20": {"title": "Dual Attention-Based Adversarial Autoencoder for Attributed Network Embedding.", "url": "https://doi.org/10.3233/FAIA200134", "year": "2020", "author": {"Ming Liu": "20/2039", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1"}, "abstract": ""}, "conf/ecai/WangS0020": {"title": "SETNet: A Novel Semi-Supervised Approach for Semantic Parsing.", "url": "https://doi.org/10.3233/FAIA200350", "year": "2020", "author": {"Xiaolu Wang": "25/2654", "Haifeng Sun 0001": "00/11044-1", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1"}, "abstract": ""}, "conf/icassp/MaSW0L20": {"title": "Semi-Supervised Sentence Classification Based on User Polarity in the Social Scenarios.", "url": "https://doi.org/10.1109/ICASSP40776.2020.9053774", "year": "2020", "author": {"Bing Ma": "53/5636", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:The data sparsity is the main challenge in sentence classification in social scenarios, the recent methods incorporate user information by encoding user node in the user-relation network to alleviate this issue. However, the connection between users is not always available due to privacy protection or other commercial reasons. Thus, in this paper, a concept called user polarity is proposed to quantify the tendency of sentences published by a user which are categorized into the same class. Then a self-training framework based on user polarity is proposed, which incorporates user information without connection between users, to alleviate the data sparsity in sentence classification. A regularization term is used to strengthen the prediction of the model in some special points, and a sample selector is designed to reduce the noise in the pseudo-labeled data generated in self-training process. Besides, some hard samples are selected to improve the retraining process. The experimental results conducted on SemEval 2019 task 8 indicate that our method performs significantly better than other three semi-supervised methods and achieves state-of-the-art performance on this benchmark."}, "conf/icc/Li0WSLY20": {"title": "GGS: General Gradient Sparsification for Federated Learning in Edge Computing", "url": "https://doi.org/10.1109/ICC40277.2020.9148987", "year": "2020", "author": {"Shiqi Li": "05/3351", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Yujian Li": "83/5205", "F. Richard Yu": "16/6654"}, "abstract": " Abstract:Federated learning is an emerging concept that trains the machine learning models with the distributed datasets, without sending the raw data to the data center. But in an edge computing enviroment where the wireless network resource is constrained, the key problem of federated learning is the communication overhead for parameters synchronization, which wastes bandwidth, increases training time, and even impacts the model accuracy. Gradient sparsification has received increasing attention, which only updates significant gradients and accumulates insignificant gradients locally. However, how to preserve the accuracy after a high ratio sparsification has always been ignored. In this paper, a General Gradient Sparsification (GGS) framework is proposed for adaptive optimizers, to correct the sparse gradient update process. It consists of two important mechanisms: gradient correction and batch normalization update with local gradients (BN-LG). With gradient correction, the optimizer can properly treat the accumulated insignificant gradients, which makes the model converge better. Furthermore, updating the batch normalization layer with local gradients can relieve the impact of delayed gradients without increasing the communication overhead. We have conducted experiments on LeNet-5, CifarNet, DenseNet-121, and AlexNet with adaptive optimizers. Results show that when 99.9% gradients are sparsified, validation datasets are maintained with top-l accuracy."}, "conf/icpp/HeW0SZLL20": {"title": "DeepHop on Edge: Hop-by-hop Routing byDistributed Learning with Semantic Attention.", "url": "https://doi.org/10.1145/3404397.3404425", "year": "2020", "author": {"Bo He": "04/2868", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Zirui Zhuang": "235/7014", "Cong Liu": "95/6404", "Jianxin Liao": "60/4951"}, "abstract": "\n\t\tMulti-access Edge Computing (MEC) and ubiquitous smart devices help serve end-users efficiently and optimally through providing emerging edge-deployed services. Meanwhile, heavy and time-varying traffic loads are produced in the edge network, so that an efficient traffic forwarding mechanism is required. In this paper, we propose a parallel and distributed learning approach, DeepHop, to adapt to the volatile environments and realize hop-by-hop routing. The Multi-Agent Deep Reinforcement Learning (MADRL) is used to alleviate the edge network congestion and maximize the utilization of network resources. DeepHop determines the routing among edge network nodes for heterogeneous types of traffic according to the current workload and capability. By joining with an attention mechanism, DeepHop obtains the semantics from the elements of the network state to help the agents learn the importance of each element on routing. Experiment results show that DeepHop achieves the increase of successfully transmitted packets by 15% compared with the state-of-the-art algorithms. Besides, DeepHop with an attention mechanism reduces convergence time by nearly half compared with the common-used structures of neural networks.\n\t"}, "conf/iwqos/Zhuang00LH20": {"title": "Adaptive and Robust Network Routing Based on Deep Reinforcement Learning with Lyapunov Optimization.", "url": "https://doi.org/10.1109/IWQoS49365.2020.9213056", "year": "2020", "author": {"Zirui Zhuang": "235/7014", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Zhu Han 0001": "83/514"}, "abstract": " Abstract:The most recent development of the Internet of Things brings massive timely-sensitive and yet bursty data flows. The adaptive network control has been explored using deep reinforcement learning, but it is not sufficient for extremely bursty network traffic flows, especially when the network traffic pattern may change over time. We model the routing control in an environment with time-variant link delays as a Lyapunov optimization problem. We identify that there is a tradeoff between optimization performance and modeling accuracy when the propagation delays are included. We propose a novel deep reinforcement learning-based adaptive network routing method to tackle the issues mentioned above. A Lyapunov optimization technique is used to reduce the upper bound of the Lyapunov drift, which leads to improved queuing stability in networked systems. Experiment results show that the proposed method can learn a routing control policy and adapt to the changing environment. The proposed method outperforms the baseline backpressure method in multiple settings, and converges faster than existing methods. Moreover, the deep reinforcement learning module can effectively learn a better estimation of the longterm Lyapunov drift and penalty functions, and thus it provides superior results in terms of the backlog size, end-to-end latency, age of information, and throughput. Extensive experiments also show that the proposed model performs well under various topologies, and thus the proposed model can be used in general cases. Also the user can adjust the preference parameter at ant time without the need to retrain the neural networks."}, "conf/wcnc/ZhangZGWH20": {"title": "Multi-Agent Deep Reinforcement Learning for Secure UAV Communications.", "url": "https://doi.org/10.1109/WCNC45663.2020.9120592", "year": "2020", "author": {"Yu Zhang 0047": "50/671-47", "Zirui Zhuang": "235/7014", "Feifei Gao": "20/6898", "Jingyu Wang 0001": "37/2749-1", "Zhu Han 0001": "83/514"}, "abstract": " Abstract:In this paper, we investigate a multi-unmanned aerial vehicle (UAV) cooperation mechanism for secure communications, where the UAV transmitter moves around to serve the multiple ground users (GUs) while the UAV jammers send the 3D jamming signals to the ground eavesdroppers (GEs) to protect the UAV transmitter from being wiretapped. The 3D jamming guarantees the GEs not being interfered by the jamming signals. It is challenging to make a joint trajectory design and power control for a UAV team without central control. To this end, we propose a multi-agent deep reinforcement learning approach to achieve the maximum sum secure rate by designing the dynamic trajectory of each UAV. The proposed multi-agent deep deterministic policy gradient (MADDPG) technique is centralized training at high altitude platforms (HAPs) and distributed execution at each UAV, which enables the fully distributed cooperation among UAVs. Finally, the simulation results show the proposed method can efficiently solve the multi-UAV cooperation trajectory design problem in secure communication scenarios."}, "conf/bigdataconf/MaSW019": {"title": "Attention-based Multi-layer Chinese Word Embedding.", "url": "https://doi.org/10.1109/BigData47090.2019.9006279", "year": "2019", "author": {"Bing Ma": "53/5636", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Word embedding is a basic task in natural language processing area. Unlike English, Chinese subword units, such as characters, radicals, and components, contain rich semantic information which can be used to enhance word embeddings. However, existing methods neglect the semantic contribution of corresponding subword units to the word. In this work, we employ attention mechanism to capture the semantic structure of Chinese words and propose a novel framework, named Attention-based multi-Layer Word Embedding model(ALWE). We also design an asynchronous strategy for updating embedding and attention efficiently. Our model learns to share subword information between distinct words selectively and adaptively. Experimental results on the word similarity, word analogy, and text classification show that the proposed model outperforms all baselines, especially when words don't appear frequently. Qualitative analysis further demonstrates the superiority of ALWE."}, "conf/bigdataconf/Zhang0WSL19": {"title": "Multi-task Deep Reinforcement Learning for Scalable Parallel Task Scheduling.", "url": "https://doi.org/10.1109/BigData47090.2019.9006027", "year": "2019", "author": {"Lingxin Zhang": "242/3729", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:The rapid development of artificial intelligence in multiple scenarios, including machine learning, image recognition, and autonomous driving, has led to an explosion of computation jobs. These jobs are often divided into parallel child tasks and executed in distributed clusters with limited computing resources, making parallel task scheduling one of the most important research topics nowadays. Most studies about parallel task scheduling focused on formulating special scenarios and service requirements as optimization problems. However, the complicated and dynamic parallel computing environments are hard to model, predict and control, making those previous methods unscalable and unable to reflect the real scenarios. In this paper, a Multi-task Deep reinforcement learning approach for scalable parallel Task Scheduling (MDTS) is firstly devised. Generally, Deep Reinforcement Learning (DRL) is a model-free optimization algorithm for long-term control by leveraging experience, but it suffers the curse of dimensionality for decision when coping with complex parallel computing environments and jobs with diverse properties. We extend the action selection in DRL to a multi-task decision, where the output branches of multitask learning are fine-matched to parallel scheduling tasks. Child tasks of a job are accordingly assigned to distributed nodes without any human knowledge while the resource competition among parallel tasks is leveraged through shared neural network layers. Extensive experiments show that the MDTS significantly reduces the job execution time compared with least-connection scheduling and particle swarm optimization algorithm by 15.3% and 39.8% respectively. Moreover, MDTS outperforms the raw DRL algorithm on job execution time, load imbalance value, and total cost by 42.8%, 47.5%, and 59.0%."}, "conf/bmvc/RenSQWH19": {"title": "SRN: Stacked Regression Network for Real-time 3D Hand Pose Estimation.", "url": "https://bmvc2019.org/wp-content/uploads/papers/0918-paper.pdf", "year": "2019", "author": {"Pengfei Ren": "31/10610", "Haifeng Sun 0001": "00/11044-1", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Weiting Huang": "262/0438"}, "abstract": ""}, "conf/cvpr/Li0WGLYS19": {"title": "OICSR: Out-In-Channel Sparsity Regularization for Compact Deep Neural Networks.", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_OICSR_Out-In-Channel_Sparsity_Regularization_for_Compact_Deep_Neural_Networks_CVPR_2019_paper.html", "year": "2019", "author": {"Jiashi Li": "241/9364", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Ce Ge": "215/0142", "Yujian Li": "83/5205", "Zhangzhang Yue": "241/9492", "Haifeng Sun 0001": "00/11044-1"}, "abstract": "\nChannel pruning can significantly accelerate and compress deep neural networks. Many channel pruning works utilize structured sparsity regularization to zero out all the weights in some channels and automatically obtain structure-sparse network in training stage. However, these methods apply structured sparsity regularization on each layer separately where the correlations between consecutive layers are omitted. In this paper, we first combine one out-channel in current layer and the corresponding in-channel in next layer as a regularization group, namely out-in-channel. Our proposed Out-In-Channel Sparsity Regularization (OICSR) considers correlations between successive layers to further retain predictive power of the compact network. Training with OICSR thoroughly transfers discriminative features into a fraction of out-in-channels. Correspondingly, OICSR measures channel importance based on statistics computed from two consecutive layers, not individual layer. Finally, a global greedy pruning algorithm is designed to remove redundant out-in-channels in an iterative way. Our method is comprehensively evaluated with various CNN architectures including CifarNet, AlexNet, ResNet, DenseNet and PreActSeNet on CIFAR-10, CIFAR-100 and ImageNet-1K datasets. Notably, on ImageNet-1K, we reduce 37.2% FLOPs on ResNet-50 while outperforming the original model by 0.22% top-1 accuracy."}, "conf/emnlp/DuSWQLWM19": {"title": "Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification.", "url": "https://doi.org/10.18653/v1/D19-1043", "year": "2019", "author": {"Chunning Du": "254/8020", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Chun Wang": "42/675", "Bing Ma": "53/5636"}, "abstract": ""}, "conf/emnlp/DuSWQLXL19": {"title": "Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification.", "url": "https://doi.org/10.18653/v1/D19-1551", "year": "2019", "author": {"Chunning Du": "254/8020", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Tong Xu 0002": "70/6770-2", "Ming Liu": "20/2039"}, "abstract": ""}, "conf/icc/LiuLW019": {"title": "AGRM: Attention-Based Graph Representation Model for Telecom Fraud Detection.", "url": "https://doi.org/10.1109/ICC.2019.8761665", "year": "2019", "author": {"Ming Liu": "20/2039", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Fraud detection is an increasingly important and difficult task in the modern technological environment. Existing classification methods for fraud detection are based on the call behavior attributes, such as duration and frequency of callings. With the evolution of the fraud mode, the methods based on self-attributes cannot detect the fraudsters who camouflage themselves by behaving like normal accounts. So we propose an Attention-based Graph Representation learning Model(AGRM), which takes into account the node's self-attributes and its context information. In our model, an attention architecture is introduced to learn the discriminative representation by focusing on the most informative context. On this basis, we design a self-attention mechanism to adjust the contribution of the node's self-attributes and its neighbors'. Extensive experiments show that the proposed method achieved significant accuracy improvement compared with existing fraud detection methods."}, "conf/iscc/Jing0WFL19": {"title": "ALSR: An Adaptive Label Screening and Relearning Approach for Anomaly Detection.", "url": "https://doi.org/10.1109/ISCC47284.2019.8969661", "year": "2019", "author": {"Yuhan Jing": "248/6262", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Tongtong Feng": "248/6095", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Anomaly detection using KPIs (Key Performance Indicators) is key to AIOps (Artificial Intelligence for IT Operations). Recent anomaly detection approaches have adopted machine learning to detect anomalies on the perspective of individual time points more than events. These approaches do not make effective use of the labels of continuous anomaly intervals, nor do they pay attention to the differences among anomaly points. The detection performance is therefore not high enough. In this paper, we propose an anomaly detection approach named ALSR, which uses a label screening model and a relearning model to analyze and utilize the continuous anomaly intervals of KPIs in finer granularity. The label screening algorithm takes advantage of the continuity of anomaly intervals to remove unnecessary data from the training set, so as to better suit to interval-oriented anomaly detection. The relearning algorithm reclassifies the true/false positive points within range of detected anomalies, thus effectively reduces the number of false positive points. ALSR uses statistical characteristics and time series models for feature extraction, and the feature set is proved to better describe the characteristics of KPIs. We conduct comprehensive experiments on 25 KPIs, and the total F-score of ALSR is 0.965, which outperforms state-of-the-art anomaly detection approaches."}, "conf/mir/LiuLWWW19": {"title": "A Geographical-Temporal Awareness Hierarchical Attention Network for Next Point-of-Interest Recommendation.", "url": "https://doi.org/10.1145/3323873.3325024", "year": "2019", "author": {"Tongcun Liu": "219/4242", "Jianxin Liao": "60/4951", "Zhigen Wu": "242/4545", "Yulong Wang": "97/5856", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "\n\t\tObtaining insight into user mobility for next point-of-interest (POI) recommendations is a vital yet challenging task in location-based social networking. Information is needed not only to estimate user preferences but to leverage sequence relationships from user check-ins. Existing approaches to understanding user mobility gloss over the check-in sequence, making it difficult to capture the subtle POI-POI connections and distinguish relevant check-ins from the irrelevant. We created a geographically-temporally awareness hierarchical attention network (GT-HAN) to resolve those issues. GT-HAN contains an extended attention network that uses a theory of geographical influence to simultaneously uncover the overall sequence dependence and the subtle POI-POI relationships. We show that the mining of subtle POI-POI relationships significantly improves the quality of next POI recommendations. A context-specific co-attention network was designed to learn changing user preferences by adaptively selecting relevant check-in activities from check-in histories, which enabled GT-HAN to distinguish degrees of user preference for different check-ins. Tests using two large-scale datasets (obtained from Foursquare and Gowalla) demonstrated the superiority of GT-HAN over existing approaches and achieved excellent results.\n\t"}, "conf/bmvc/GeW0SL18": {"title": "Fewer is More: Image Segmentation Based Weakly Supervised Object Detection with Partial Aggregation.", "url": "http://bmvc2018.org/contents/papers/0557.pdf", "year": "2018", "author": {"Ce Ge": "215/0142", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": ""}, "conf/colcom/HeW0S18": {"title": "A Single-Hop Selection Strategy of VNFs Based on Traffic Classification in NFV.", "url": "https://doi.org/10.1007/978-3-030-12981-1_19", "year": "2018", "author": {"Bo He": "04/2868", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1"}, "abstract": "Network Function Virtualization (NFV) has become a hot technology since it provides the flexible management of network functions and efficient sharing of network resources. Network resources in NVF require an appropriate management strategy which often manifests as a difficult online decision making task. Resource management in NFV can be thought of as a process of virtualized network functions (VNFs) selection or deployment. This paper proposes a single-hop VNFs selection strategy to realize network resource management. For satisfying quality requirements of different network services, this strategy is based on the results of traffic classification which utilizes Multi-Grained Cascade Forest (gcForest) to distinguish user behaviors on the internet. In the order of VNFs, a network is divided into several layers where each arrived packet needs to queue. The scheduler of each layer selects a layer which hosts the next VNF for the packets in the queue. Experiments prove that the proposed traffic classification method increases the precision by 7.7% and improves the real-time performance. The model of VNFs selection reduces network congestion compared to traditional single-hop scheduling models. Moreover, the number of packets which fail to reach target node in time drops 30% to 50% using the proposed strategy compared to the strategy without the section of traffic classification.KeywordsNFVTraffic classificationResource managementVNFs selection"}, "conf/edge2/YuWQLX18": {"title": "Boundless Application and Resource Based on Container Technology.", "url": "https://doi.org/10.1007/978-3-319-94340-4_3", "year": "2018", "author": {"Zhenguang Yu": "221/2911", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Jie Xu 0003": "37/5126-3"}, "abstract": "\nLimitations like network latency and expensive cost are pushing cloud computing to the edge. Edge computing, also known as fog computing is an extension of cloud computing, providing features to solve problems of cloud computing. However, edge computing is unstable and not enough reliable at present. Orchestration may occur occasionally in order to meet the need of the users. This paper provides an architecture named Boundless Resource Orchestrator (BRO) combing cloud computing and edge computing based on containers. Containers provide more lightweight virtualization compared to VMs. The proposed architecture leverages container technology to accelerate and optimize the orchestration process taking container as the basic component of orchestrating and minimum resource unit for service distribution. A master-slave paradigm is implemented in the architecture to provide region autonomy abilities rather than the centralized architecture. Considering the ever-changing circumstance of edge cloud, an orchestration strategy named Best Performance at Least Cost (BPLC) is proposed to maximize the performance of computing at minimum cost dynamically and automatically considering real-time conditions of the cloud. Experiments are carried out on measuring couples of infrastructures and orchestration strategies that prove the BRO and BPLC as prior choices dealing with massive jobs in edge computing.KeywordsContainersEdge computingCloud computingOrchestration"}, "conf/edge2/PangWWQXY18": {"title": "Efficient Bare Metal Auto-scaling for NFV in Edge Computing.", "url": "https://doi.org/10.1007/978-3-319-94340-4_5", "year": "2018", "author": {"Xudong Pang": "207/4104", "Jing Wang 0039": "02/736-39", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jie Xu 0003": "37/5126-3", "Zhenguang Yu": "221/2911"}, "abstract": "Elasticity is an essential attribute of cloud data center, which is critical for operating resources in face of peaks and valleys of business. At present, the automatic scaling technique of virtual machines is widely studied, but barely for physical machines. Despite lack of flexibility, we all know that physical server can perform faster and more efficiently than virtualized instances, especially in Network Function Virtualization (NFV) systems. Some virtual network functions (VNFs) actually require high performance computing, which is a hard task for virtual machines. Besides, good management of bare metal resources can be significant for the data center power cost and human maintenance cost. Accordingly, we think that auto-scaling of physical machine is worth studying. This paper proposes a bare metal automatic scaling scheme based on workload prediction, and finally make tests on an open source NFV platform. The new scheme obtains good result on computation intensive VNFs scenario, including complete the scale in minutes, guarantee for the continuity of VNF processing business, and can cope with the load fluctuation better.KeywordsBare metalNFVAuto-scalingSchedulingEdge computing"}, "conf/icccn/YuW0SZ18": {"title": "A Boundless Resource Orchestrator Based on Container Technology in Edge Computing.", "url": "https://doi.org/10.1109/ICCCN.2018.8487377", "year": "2018", "author": {"Zhenguang Yu": "221/2911", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jian Zou": "60/2647"}, "abstract": " Abstract:Edge computing reorganizes edge resources but is unstable and unreliable at present, thus, orchestration may occur occasionally. This paper provides an architecture named Boundless Resource Orchestrator (BRO) combing cloud computing and edge computing based on containers. The proposed architecture leverages container technology to accelerate and optimize the orchestration process. A master-slave paradigm is implemented in the architecture to provide region autonomy abilities rather than the centralized architecture. Considering the ever-changing circumstance of edge cloud, an orchestration strategy named Best Performance at Least Cost (BPLC) is proposed to maximize the performance of computing at minimum cost dynamically and automatically. Experiments are carried out on measuring couples of infrastructures and orchestration strategies that prove the BRO and BPLC as prior choices dealing with massive jobs in edge computing."}, "conf/icccn/ZouWQSYX18": {"title": "EVN: An Elastic Virtual Network Supporting NFV Customized and Rapid Migration.", "url": "https://doi.org/10.1109/ICCCN.2018.8487380", "year": "2018", "author": {"Jian Zou": "60/2647", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Zhenguang Yu": "221/2911", "Jun Xu": "90/514"}, "abstract": " Abstract:The virtual network functions (VNF) enables to reduce reliance on expensive proprietary networking gear and increase network elasticity. In this paper, we propose an Elastic Virtual Networks (EVN) system to realize a complete virtual network layout, which supports resource customization, automated deployment, and flexible migration. Through the port and MAC address mapping, a new physical node can access the Virtual Networks (VNs), and the related VNFs are deployed through Docker. Moreover, we set up a white list function to reserve the capacity of the hosts that will be accessed in the future to facilitate a fast access. The experiment results verify that the VNF migration time in EVN by Docker improves three to four times compared with the traditional VM solution."}, "conf/icws2/XuW0SL18": {"title": "Accelerating Training for Distributed Deep Neural Networks in MapReduce.", "url": "https://doi.org/10.1007/978-3-319-94289-6_12", "year": "2018", "author": {"Jie Xu 0003": "37/5126-3", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": "Parallel training is prevailing in Deep Neural Networks (DNN) to reduce training time. The training data sets and layered training processes of DNN are assigned to multiple Graphics Processing Units (GPUs) in parallel training. But there are some obstacles to deploy parallel training in GPU cloud services. DNN has a tight-dependent layering structure where the next layer feeds on the output of its former layer. It is unavoidable to transmit big output data between separated layered training processes. Since cloud computing offers separated storage services and computing services, data transmission through network harms the performance in training time. Thus parallel training leads to an inefficient training process in GPU cloud environment. In this paper, we construct a distributed DNN training architecture to implement parallel training for DNN in MapReduce. The architecture assigns GPU cloud resources as a web service. We also address the concern of data transmission by proposing a distributed DNN scheduler to accelerate the training time. The scheduler makes use of minimum cost flows algorithm to assign GPU resources, which considers data locality and synchronization into minimizing training time. Compared with original schedulers, experimental results reveal that distributed DNN scheduler decreases the training time by 50% with least data transmission and synchronizing parallel training.KeywordsDeep Neural NetworksParallel trainingMapReduceData transmissionSynchronization"}, "conf/ipccc/ZhuangWQSL18": {"title": "A Case-Based Decision System for Routing in Packet-Switched Networks.", "url": "https://doi.org/10.1109/PCCC.2018.8710995", "year": "2018", "author": {"Zirui Zhuang": "235/7014", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Route planning with global optimization objectives in graphs is a challenging task with enormous computational complexity and finding the best solution is NP-complete. In addition, the network's operational performance varies whenever the environment changes. Traditional routing schemes fail to deal with these situations. We propose a case-based decision system for routing in packet switched networks to track the networking status. We also design a graph-aware neural network to suggest and revise the solutions from the past cases. The low-level structure of the neural network is learned by fitting with the features not only from each standalone vertex but also from the neighbors of each vertex. Experiments show that the proposed system outperforms state-of-art traffic-split and traffic-engineered routing schemes."}, "conf/iscc/BaoWQL18": {"title": "ECTCP: An Explicit Centralized Congestion Avoidance for TCP in SDN-based Data Center.", "url": "https://doi.org/10.1109/ISCC.2018.8538608", "year": "2018", "author": {"Jiannan Bao": "195/6481", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Data centers provide high aggregate bandwidth for large-scale network services. Traditional TCP protocol does not work well to meet the demand in data center. Before congestion occurs, TCP increases sending window to preempt bandwidth, resulting in buildup queues and increase latency, moreover TCP can only adjust sending window individually for each flow, which lacks global information. This mechanism is ill-suited for the diverse mix of short and long flows, and cannot provide high burst tolerance. Fortunately, Software Defined Networking (SDN) has a global view on data center network, which provides an opportunity to centralized control flows. Leveraging the feature of SDN, we propose an explicit centralized congestion avoidance mechanism (ECTCP). By gaining network topology and the information of flows, SDN controller calculates each flow's fair bandwidth, and adjusts the receive window size of each TCP connection. ECTCP does not need to modify TCP stack, which is easy to implement in the SDN-based data center. In our evaluation, ECTCP achieves fair throughput and improves fairness among different congestion control algorithms. ECTCP also reduces Flow Completion Time (FCT) for burst short flows, and keeps Round-Trip Time (RTT) in small-scale."}, "conf/ispa/XuW0L18": {"title": "Proportion Scheduler to Improve the Mismatched Locality in YARN.", "url": "https://doi.org/10.1109/BDCloud.2018.00068", "year": "2018", "author": {"Jie Xu 0003": "37/5126-3", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:YARN is a prevailing central resource management architecture, which allocates a group of resources to each application. The resource group is consistent with locality requests of tasks in applications. But each application allocates the resources in the resource group to each task according to locality capacities of resources. Different rules in two levels of resource allocations lead to mismatched localities for tasks, which hurts performances of applications. There is a lack of researches about mismatched localities for tasks in YARN. This paper designs a Proportion scheduler to improve mismatched localities without dragging applications. The locality capacities of resources becomes unified allocation rule. Resources are classified by the locality requests of tasks, so that resources in the same category are versatile for tasks with the same level of locality request. This classification decreases the mismatched probability. In addition, the improvement of mismatched localities makes compromises between different applications. Every application is assigned with proportional resources in different locality scales for improved performances. Compared to baseline schedulers, there are 2 times data-local tasks and more than 30% rack-local tasks. The Proportion decreases makespan of applications by a maximum 66.7% and network traffic by an utmost 80%."}, "conf/ivmsp/ZhaoWQ18": {"title": "MindCamera: Interactive Image Retrieval and Synthesis.", "url": "https://doi.org/10.1109/IVMSPW.2018.8448722", "year": "2018", "author": {"Yu Zhao 0006": "57/2056-6", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Composing a realistic picture according to the mind is tough work for most people. It is not only a complex operation but also a creation process from nonexistence to existence. Therefore, the core of this problem is to provide rich existing materials for stitching. We present an interactive sketch-based image retrieval and synthesis system, MindCamera. Compared with existing methods, it can use images of daily scenes as the dataset and proposes a sketch-based scene image retrieval model. Furthermore, MindCamera can blend the target object in the gradient domain to avoid the visible seam, and it introduces alpha matting to realize real-time foreground object extraction and composition. Experiments verify that our retrieval model has higher precision and provides more reasonable and richer materials for users. The practical usage demonstrates that MindCamera allows the interactive creation of complex images, and its final compositing results are natural and realistic."}, "conf/ksem/HuoWQSG018": {"title": "Users Personalized Sketch-Based Image Retrieval Using Deep Transfer Learning.", "url": "https://doi.org/10.1007/978-3-319-99365-2_14", "year": "2018", "author": {"Qiming Huo": "215/0072", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Ce Ge": "215/0142", "Yu Zhao 0006": "57/2056-6"}, "abstract": "Traditionally, sketch-based image retrieval is mostly based on human-defined features for similarity calculation and matching. The retrieval results are generally similar in contour and lack complete semantic information of the image. Simultaneously, due to the inherent ambiguity of hand-drawn images, there is “one-to-many” category mapping relationship between hand-drawn and natural images. To accurately improve the fine-grained retrieval results, we first train a SBIR general model. Based on the two-branch full-shared parameters architecture, we innovatively propose a deep full convolutional neural network structure model, which obtains mean average precision (MAP) 0.64 on the Flickr15K dataset. On the basis of the general model, we combine the user history feedback image with the input hand-drawn image as input, and use the transfer learning idea to finetune the distribution of features in vector space so that the neural network can achieve fine-grained image feature learning. This is the first time that we propose to solve the problem of personalization in the field of sketch retrieval by the idea of transfer learning. After the model migration, we can achieve fine-grained image feature learning to meet the personalized needs of the user’s sketches.KeywordsPersonalized sketch-based image retrievalDeep full convolutional neural networkTransfer learningFeature extraction"}, "conf/lcn/XiaoSZW018": {"title": "Common Knowledge Based Transfer Learning for Traffic Classification.", "url": "https://doi.org/10.1109/LCN.2018.8638070", "year": "2018", "author": {"Yunming Xiao": "235/7025", "Haifeng Sun 0001": "00/11044-1", "Zirui Zhuang": "235/7014", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Deep neural networks have been used for traffic classification, and promising results are obtained. However, most previous work confined to one specific classification task and restricted the classifiers potential performance and applications. As the traffic flows can be labeled from different perspectives, the performance of the classifier might be improved by exploring more meaningful latent features. For this purpose, we adopted a multi-output DNN model that simultaneously learns different traffic classification tasks. The common knowledge of traffic is exploited by the synergy among the tasks and boosts the individual performances of the tasks. Experiments show that this structure has the potential to meet new future demands and achieve the classification with advanced speeds and fair accuracies. Yet, due to the heavy training cost, the neural networks, though achieving good performance, are hard to implement in the real environment. We further show that few-shot learning could be a viable approach."}, "conf/lcn/ZhuangW0SL18": {"title": "Graph-Aware Deep Learning Based Intelligent Routing Strategy.", "url": "https://doi.org/10.1109/LCN.2018.8638099", "year": "2018", "author": {"Zirui Zhuang": "235/7014", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Software defined networking decouples the control plane and data plane, which grants more computing power for routing computations. Traditional routing methods suffer from the complex dynamics in networking, and they are facing issues such as slow convergence and performance decline. Deep learning techniques have shown preliminary results on solving the routing problem, bring more accuracy and precision compared with traditional modeling techniques. However, the deep learning architecture needs to be specially customized to learn the topological relations between switches in an efficient way. Thus, we propose a deep learning based intelligent routing strategy with revised graph-aware neural networks and we design a set of features suitable for network routing. Then we demonstrate the performance of our works by using a real-world topology and the production level software switch. The simulation result shows our work is more accurate and efficient compared to state-of-art routing strategy."}, "conf/mlsp/XuWQSH18": {"title": "Deep Neural Networks for Application Awareness in SDN-based Network.", "url": "https://doi.org/10.1109/MLSP.2018.8517088", "year": "2018", "author": {"Jun Xu": "90/514", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Bo He": "04/2868"}, "abstract": " Abstract:Accurate traffic classification is essential for traffic engineering and Quality of Service (QoS) guarantee, especially in Internet of Things (IoT). Different applications have different network resource requirements, so an excellent classification algorithm can realize application awareness in traffic engineering and significantly improve QoS. Software Defined Network (SDN) with centralized controlling of network resources provides opportunities for fine-grained resource allocation. However, there are many issues when deep learning is employed in SDN, for example, sampling and classifying traffic data consume a lot of IO and computing resources of the SDN controller. In this paper, we deploy the Deep Neural Network (DNN) on Virtualized Network Function (VNF) to solve the problems of applying deep learning in SDN. The experiments show that the proposed DNN model outperforms existing traffic classification algorithm and the SDN controller can assign more appropriate route paths for different types of traffic and highly improve the network QoS."}, "conf/secon/XuWQSH18": {"title": "IARA: An Intelligent Application-Aware VNF for Network Resource Allocation with Deep Learning.", "url": "https://doi.org/10.1109/SAHCN.2018.8397153", "year": "2018", "author": {"Jun Xu": "90/514", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Bo He": "04/2868"}, "abstract": " Abstract:Application awareness is essential for traffic engineering and Quality of Service (QoS) guarantee, especially in Internet of Things (IoT). Software Defined Network (SDN) with centralized controlling of network resources provides opportunities for fine- grained resource allocation. However, the controller cannot autonomously identify applications effectively, because sampling and recognizing traffic data consumes a lot of IO and computing resources. In this demonstration, we provide an intelligent application-aware Virtualized Network Function (VNF) with deep learning technology to identify the network traffic. The traffic type information will be mapped to specific network requirements and utilized to search appropriate route paths for different applications. The intelligent VNF is deployed on a GPU-equipped standalone server and works on the data plane of SDN. It identifies the traffic and sends the type information to the controller through OpenFlow protocol. The experiments show that by introducing the type information, SDN controller can assign more appropriate route paths for different types of traffic and highly improve the network QoS."}, "conf/smartgift/ChenWSQW18": {"title": "A Dialog Robot Based on WeChat.", "url": "https://doi.org/10.1007/978-3-319-94965-9_13", "year": "2018", "author": {"Xiaoyi Chen": "55/2712", "Jing Wang 0039": "02/736-39", "Qiwei Shen": "188/7986", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "WeChat is one of the most popular instant messaging applications in the world. It has now become an important access to variety business systems for billions of users. The vast majority of companies want to provide their business services onto WeChat in order to gain advantage in fierce market competitions. However, as far as we know, today it is not easy to access WeChat with business service. In this paper, we propose a framework to integrate business services and WeChat. On the basis of this framework, companies or entrepreneurs can provide their business services on WeChat easily. Finally, we use a case study to demonstrate how our service can be used in helping tickets sells and statistical analysis.KeywordsWeChat on WebDialog robotSNSs"}, "conf/dicta/ChenWQLS17": {"title": "Bilinear CNN Models for Food Recognition.", "url": "https://doi.org/10.1109/DICTA.2017.8227411", "year": "2017", "author": {"Hesen Chen": "211/4075", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Yujian Li": "83/5205", "Haifeng Sun 0001": "00/11044-1"}, "abstract": " Abstract:Due to the diversity of food types and the slight differences between different dishes, the genre of food images becomes a new challenge in the field of computer vision. To tackle this problem, recent efforts are focusing on designing hand-crafted features or extracting features automatically by using deep convolutional neural network. Although these methods have reported a series of success, their general architectures fail to capture the fine-grained features of similar dishes adequately. Inspired by the bilinear CNN models in the field of fine-grained classification, we have exploited such a similar structure in which two deep convolution networks are used as feature extractors and the outputs of them fused to obtain fine-grained features. These features are used to train the food classifier. We have conducted experiments on three publicly benchmark food datasets to evaluate the proposed architecture. The experiments exhibit that our method is comparable to the existing approaches."}, "conf/iconip/ZhengWZWQ17": {"title": "A Tag-Based Integrated Diffusion Model for Personalized Location Recommendation.", "url": "https://doi.org/10.1007/978-3-319-70139-4_33", "year": "2017", "author": {"Yaolin Zheng": "208/8074", "Yulong Wang": "97/5856", "Lei Zhang 0094": "97/8704-94", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": "The location based services have attracted millions of users to share their locations via check-ins. It is highly important to recommend personalized POIs (Points-Of-Interest) to users in terms of their preference learned from historical data. In current research work, users’ check-in behavior is wildly used to model user’s preference. However, the sparsity of the check-in data makes it difficult to capture users’ preferences accurately. This paper proposes a tag-based integrated diffusion recommender system for location recommendation, considering not only social influence but also venue features. Firstly, we model user location preference by combining the preference extracted from check-ins data and short text tips, where sentiment analysis techniques are used. Furthermore, we collect venue information by merging descriptions and tips and then generate tags of each venue, which are processed using keyword extraction approaches. Then we apply the recommendation algorithm with user’s initial preference and obtain the final integrate diffusion results for each user, recommending top-N venues by descending order. We conduct experiments on Foursquare datasets of two cities, the results on both datasets show that our recommender system can produce better performance, providing more personalized and higher novel recommendations.KeywordsLocation recommendationSentiment analysisDiffusionKeyword extractionTag"}, "conf/icuimc/LiBLQW17": {"title": "A PSO-based virtual SDN customization for multi-tenant cloud services.", "url": "https://doi.org/10.1145/3022227.3022317", "year": "2017", "author": {"Kai Li 0003": "l/KaiLi3", "Jiannan Bao": "195/6481", "Zhonghao Lu": "134/7438", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "\n\t\tWith the development of cloud computing, the current Software Defined Networking (SDN) data center which is based on the Open Source Cloud platform OpenStack cannot satisfy multiple users' service requirements. OpenStack cannot achieve centralized control of the physical network in cloud data center. In order to satisfy the user's flexible control of the data center network, a multi-tenant virtual network customization in cloud data center is proposed in this paper. SDN with the advantages of centralized control and programming is integrated to OpenStack cloud data center. Isolated tenant virtual SDN networks (vSDN) are created in the cloud data center via the virtual network mapping model based on particle swarm optimization (PSO) algorithm. The vSDN network is controlled by tenant's own controller. Virtual network mapping performance, link customization and data transmission performance are tested in this paper. Results show that the virtual network mapping model based on PSO algorithm compared to shortest path (SP) algorithm, greatly improve the utilization rate of the underlying network bandwidth. At the same time, the SDN mode does not reduce the performance of data transmission on OpenStack cloud platform. The virtual network link customization improves the efficiency of data transmission for tenant.\n\t"}, "conf/itnac/LuoWSWQ17": {"title": "User behavior analysis based on user interest by web log mining.", "url": "https://doi.org/10.1109/ATNAC.2017.8215435", "year": "2017", "author": {"Xipei Luo": "213/9822", "Jing Wang 0039": "02/736-39", "Qiwei Shen": "188/7986", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:With the rapid development of science and technology and the growing popularity of computer networks, the scale of network users is gradually expanding, and the behavior of network users is becoming more and more complicated. A large number of studies show that the user's actual interest is closely related to the browsing behavior on the web page. Through the user browsing behavior analysis can obtain the user interest information, and then build the user interest model, so that the search results closer to the user's expectations. This paper mainly introduces the method of web log mining, which can discover the mode of web pages by digging web log records. By analyzing and exploring the rules of web log records, we can identify the potential customers of the website and improve the quality of information services to users. In the stage of user behavior analysis, this paper explores the differences in user browsing behavior in different types of access events, and calculates the user's interest based on the M5 model tree to analyze the analytic events."}, "conf/nana/ZhaoWZW017": {"title": "Performance Evaluation and Optimization for Android-Based Web Server.", "url": "https://doi.org/10.1109/NaNA.2017.66", "year": "2017", "author": {"Yajing Zhao": "61/6097", "Jing Wang 0039": "02/736-39", "Lei Zhang 0094": "97/8704-94", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Nowadays, the WEB server is mainly deployed by Internet Information Server, Apache and Nginx. These servers are running on the computer platform and they have been quite mature in the operating system such as Windows and Linux. With the mobile Internet changing our life and mobile device hardware being improved, Android platform is required to be equipped with web server to provide web services for shops, restaurant or other customer visiting places. As the performance of Android platform is different from traditional computers, we firstly analyze the concurrent performance of lightweight web servers with Nginx, PHP and MySQL on Android. By the exploratory testing method and observing the change of server performance through the test results, we modify the server's configuration parameters and obtains the best performance of server under Android system."}, "conf/nana/GuoWZW017": {"title": "GA-based Load Balancing Algorithm for Distributed Timing Task.", "url": "https://doi.org/10.1109/NaNA.2017.67", "year": "2017", "author": {"Yiming Guo": "49/11348", "Yulong Wang": "97/5856", "Lei Zhang 0094": "97/8704-94", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Distributed architectures and various load balancing algorithms have improved resource utilization and system stability observably. Tasks driven by time series will be executed and produce actual load for systems at scheduled time depending on the pre-planned arrangements. The general load balancing algorithms, such as Random, Round Robin, Least Connection, Dynamic Adaptive, all aim at balancing load based on server features or real-time server resource utilization. But few studies have been conducted on load balancing by considering temporal characteristics of distributed timing tasks, while this kind of demand is needed. To solve this issue, we propose an improved load balancing algorithm based on genetic algorithm. The algorithm fully considered the temporal parameter and the extension of genetic algorithm, can quickly obtain an approximate optimal task allocation sequence to realize load balancing in the next period of time."}, "conf/pdcat/FuWQLL17": {"title": "Tax-Based Mechanisms for Resource Scaling-Out of Stream Big Data Analytics.", "url": "https://doi.org/10.1109/PDCAT.2017.00018", "year": "2017", "author": {"Xiaoyuan Fu": "10/575", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254"}, "abstract": " Abstract:Cloud-based big data platforms provide physical resources for a variety of applications to analyze all forms of data. For the stream big data analytics, a participated task always needs to scale out resources when its input data increases steeply. Typically, the resource scaling out can be achieved by increasing the parallelism degree of the platform based on the experience. However, the resource scaling-out of each task produces additional cost not only from itself but also from other competitive tasks, which brings about great challenges to ensure the efficient utilization of resources. To solve this problem systematically, we consider the resource scaling-out problem as a non-cooperative game and formulate a total cost model including a risk function and a task execution time function. The total cost of resource scaling-out reflects the influence of topology structure for the benefit of a participated task. Hence, two economic classic tax-based incentive policies: Pivotal Mechanism and Externality Mechanism are applied, to stimulate the participation of tasks. We make simulations in different scenarios including node degree and different characteristics of tasks. The simulations results show that our resource scaling-out mechanism can achieve a better performance close to social optimality."}, "conf/ccis/LuWQ16": {"title": "SARN: A scalable resource managing framework for YARN.", "url": "https://doi.org/10.1109/CCIS.2016.7790282", "year": "2016", "author": {"Zhonghao Lu": "134/7438", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:With the fast growth of Internet, we have entered the era of big data. In the big data era, Hadoop Yet Another Resource Negotiator (YARN) is one of the common used framework for big data processing. YARN provides explicit support for programming model diversity, so multiple frameworks such as Storm, Hbase and Hive can run as applications on YARN [1]. In order to make better use of hardware resources and improve the cluster efficiency, a scalable resource managing framework (SARN) is presented. SARN includes the dynamic container and a quick deploy component for elastically expanding or shrinking the number of YARN'S work node to meet the actual resource needs of multiple tasks. I do experiment under both idle mode (the cluster is idle) and eager mode (the cluster resource is relatively not enough to meet the application's requirements). The experimental result shows that the approach can improve the performance applications under both situations."}, "conf/colcom/ChenXLLQW16": {"title": "On Demand Resource Scheduler Based on Estimating Progress of Jobs in Hadoop.", "url": "https://doi.org/10.1007/978-3-319-59288-6_62", "year": "2016", "author": {"Liangzhang Chen": "202/4093", "Jie Xu 0003": "37/5126-3", "Kai Li 0003": "l/KaiLi3", "Zhonghao Lu": "134/7438", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "\nIn order to meet the need of setting deadline for Hadoop MapReduce job and improve resource utilization of Hadoop cluster, a resource scheduler based on collecting the running information of tasks is proposed. According to the information of resource usage, the progress of job, the deadline of job, and the handling time of job, we estimate the resource demand of jobs, and then schedule these jobs according to their resource demand. Meanwhile, a method to judge whether the resource of cluster can meet the deadline of all the jobs in cluster is proposed. When the jobs will miss the deadline under the allocated resources, scheduler applies to cloud platform for extra resources. Experimental results show the on demand resource scheduler can increase the utilization of resource in Hadoop cluster and approximately ensure the deadline of jobs.KeywordsHadoopOn demandSchedulerJob with deadline"}, "conf/icnidc/LuLWWQ16": {"title": "Semi-blind compensation method for addressing memoryless nonlinearities.", "url": "https://doi.org/10.1109/ICNIDC.2016.7974590", "year": "2016", "author": {"Zhonghao Lu": "134/7438", "Kai Li 0003": "l/KaiLi3", "Jing Wang 0039": "02/736-39", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Nowadays, in order to guarantee both system performance and power efficiency, the resistance of nonlinear distortion produced by power amplifier (PA) has been a key issue in the wireless communication research. The traditional predistortion methods require prior knowledge of the amplitude, phase or bandwidth of the input signal, which is not very practical in the real world. To overcome it, we put forward a framework for compensating a nonlinear memoryless system in a semi-blind way. In this framework, by making use of the feedback branch, a nonlinear equation can be established to describe the input-output relations for the nonlinear system, and the gain of compensator can be iteratively obtained through solving this nonlinear equation with Newton method. Simulations are provided in order to verify the performance of this proposed framework and algorithm, where Saleh model is used as benchmark. Compared with traditional frameworks using the least mean square (LMS) algorithm similarly, our framework can achieve better performance in terms of mean square error (MSE) and latency without compromising the compensation effect."}, "conf/infocom/QiLWLC16": {"title": "Dynamic resource orchestration for multi-task application in heterogeneous mobile cloud computing.", "url": "https://doi.org/10.1109/INFCOMW.2016.7562076", "year": "2016", "author": {"Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Li": "181/2688", "Yufei Cao": "92/2499"}, "abstract": " Abstract:The mobile cloud computing (MCC) that takes wireless access network as transmission medium and uses mobile devices as client becomes the newest evolution trends of cloud computing. When offloading the complicated multi-task application to the MCC environment, each task executes individually in terms of its own computation, storage and bandwidth requirement. Due to user's mobility, the provided resources contain different performance metrics that may affect the destination choice. Nevertheless, these heterogeneous MCC resources lack integrated management and can hardly cooperate with each other. Thus, how to choose the appropriate offload destination and orchestrate the resources for multi-task is a challenging problem. This paper decouples resource control of mobile cloud from user plane, where a centralized controller is responsible for resource orchestration, offload and migration. The resource orchestration is formulated as multi-objective optimal problem that contains the metrics of energy consumption, cost and availability. Finally, a particle swarm algorithm is used to obtain the approximate optimal solutions. Simulation results show that the solutions can hit Pareto optimum of resource orchestration in acceptable time."}, "conf/lcn/CuiLWQW16": {"title": "An Approach to Improve the Cooperation between Heterogeneous SDN Overlays.", "url": "https://doi.org/10.1109/LCN.2016.51", "year": "2016", "author": {"Ziteng Cui": "150/5743", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jing Wang 0039": "02/736-39"}, "abstract": " Abstract:The overlay network has been widely developed in recent years. There may be various overlays that co-exist with each other upon the same underlying network. These overlays have heterogeneous performance goals, and they will compete for the physical resources, so that a sub-optimal performance of the overlays may be achieved. Moreover, the heterogeneity of the overlays makes them difficult to coordinate with each other to improve their performance. We introduce the concept of SDN to the deployment of overlay network and propose an approach to make the overlays cooperate with each other. A cooperative solution is proposed for co-existing overlays to improve their performance while leveraging their heterogeneous performance goals. Simulations are performed to evaluate the cooperative solution."}, "conf/nana/LiWSWQ16": {"title": "MVNC: A SDN-based Multi-tenant Virtual Network Customization Mechanism in Cloud Data Center.", "url": "https://doi.org/10.1109/NaNA.2016.51", "year": "2016", "author": {"Kai Li 0003": "l/KaiLi3", "Chun Wang": "42/675", "Qiwei Shen": "188/7986", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Software Defined Network (SDN) is a programmable network structure which achieves separation of control and forwarding via a centralized controller. Existing cloud data center network introduces SDN architecture, and enhances data center scalability via advantages of programmable thinking and centralized control. OpenStack is a free and open-source software platform for cloud computing, which provides computing, storage and network management services for cloud computing resources. However, with rapid development of various types of services, the current SDN data center based on OpenStack cannot satisfy multiple users' service requirements. Cloud data center network with single control node cannot achieve tenants' customization of their own network. Thus, multi-tenant virtual network customization (MVNC) mechanism is presented in this paper. The tenant isolation is achieved in cloud data center network through network slice. At the same time, centralized control of tenant virtual network provides a global topology management. Tenant isolation, link switching and efficiency of flow table sending are tested in this paper. Results show that proposed mechanism can flexibly and efficiently help tenants for network customization operation in OpenStack cloud platform."}, "conf/nana/LiWLXQ16": {"title": "Load Prediction-Based Automatic Scaling Cloud Computing.", "url": "https://doi.org/10.1109/NaNA.2016.49", "year": "2016", "author": {"Tao Li": "75/4601", "Jingyu Wang 0001": "37/2749-1", "Wei Li": "64/6025", "Tong Xu 0002": "70/6770-2", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:It is vital to cloud computing that the elastic control technology can dynamically adjust resources on demand by changing the size or number of virtual machines. It will improve the utilization and realize the cost savings. However, there are still some shortcomings for elastically scaling technology. On the one hand, the elastic scaling of resources will take some time, so changing resource requirements can not be responded timely. On the other hand, whenever the resources do not meet the demand, it can not properly allocate resources based on the size of demand. Therefore, this paper proposes an algorithm about automatic scaling of resources based on load prediction. By using the algorithm combining linear regression and the improved Knuth-Morris-Pratt match to predict the next moment load, complete automatic extension before the changes in resource requirements and reduce resource adjustment time. In addition, according to the results, integer programming algorithm is used for solving specific resource expansion. The experiment results indicated that the proposed method can increase resource utilization and reduce cost of cloud computing resources while meeting the changing demand."}, "conf/nana/HeLZWQ16": {"title": "ECCN: An Elastic Customized Cloud Network Platform.", "url": "https://doi.org/10.1109/NaNA.2016.62", "year": "2016", "author": {"Yuze He": "188/7944", "Wei Li": "64/6025", "Lejian Zhang": "188/8007", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:A network testbed with high flexibility plays an important role in testing and evaluating new network technology. Both researchers and engineers always need a network testbed which can customize network experiment on demand. Some of the existing cloud computing platforms lack network customization while some network experimental platform can not provide convenient virtual machine management functions. In this paper, we propose a preliminary implementation of the elastic customized cloud network (ECCN) platform based on Openstack project which provides cloud computing management and supports the Openflow protocol. This testbed can meet the needs of a rapid customized deployment of network resources like virtual machines, load balancers and routers in data center. Simultaneously, it provides the capability to generate and manage the customized network topology and an experimental environment where traditional network can interwork with Software Defined Network (SDN). Experiment results show that the ECCN testbed can help users deploy their own network experiment rapidly and easily with a friendly operation, as well as a flexible network design based on the highly customized experiment. In addition, ECCN guarantees the network isolation between different users based on reuse and sharing of the physical resources."}, "conf/globecom/JiangGWLL15": {"title": "Interactions among Overlays and Traffic Engineering: Equilibrium and Cooperation without Payment.", "url": "https://doi.org/10.1109/GLOCOM.2014.7416962", "year": "2015", "author": {"Shan Jiang 0008": "04/2910-8", "Jun Gong": "62/28", "Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254"}, "abstract": " Abstract:Emerging overlay technologies have enabled to distribute content efficiently over the Internet that in some sense improves the user quality of experience. However, due to inconsistent or even conflicting objectives from the perspectives of multiple co-existing overlay networks and traffic engineering (TE), the interaction among them impacts the performance of each other and results in sub- optimum, which also has not yet been investigated in detail. In this paper, we model this interaction as an n+1-person non-cooperative game, prove the existence of Nash equilibrium point (NEP) and propose an algorithm to compute NEP. In order to overcome the inefficiency of NEP, we define a non-transferable utility (NTU) game based on Shapley NTU game theory, in which overlays and TE share cost efficiently and fairly without side payment, and we propose an algorithm to calculate the Shapley NTU value (SNTUV)."}, "conf/lcn/JiangLGWL15": {"title": "Competitive equilibrium and stable coalition in overlay environments.", "url": "https://doi.org/10.1109/LCN.2015.7366331", "year": "2015", "author": {"Shan Jiang 0008": "04/2910-8", "Jianxin Liao": "60/4951", "Jun Gong": "62/28", "Jingyu Wang 0001": "37/2749-1", "Tonghong Li": "98/4254"}, "abstract": " Abstract:Overlay networks have been widely deployed upon the Internet to provide improved network services. However, the interaction between overlay and traffic engineering (TE) as well as among co-existing overlays may occur. In this paper, we adopt game theoretic approaches to analyze this hybrid interaction. Firstly, we model a situation of the hybrid interaction as an n+1-player non-cooperative game, where overlays and TE are of equal status, and prove the existence of Nash equilibrium (NE). Secondly, we model another situation of the hybrid interaction as a 1-leadern-follower Stackelberg-Nash game, where TE is the leader and coexisting overlays are followers, and prove that the cost at Stackelberg-Nash equilibrium (SNE) is at least as good as that at NE for TE. Thirdly, we propose a cooperative coalition mechanism based on Shapley value to overcome the inherent inefficiency of NE and SNE, where players can improve their performance and form stable coalitions."}, "conf/lcn/CuiLWQW15": {"title": "Cooperative traffic management for co-existing overlays.", "url": "https://doi.org/10.1109/LCN.2015.7366354", "year": "2015", "author": {"Ziteng Cui": "150/5743", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jing Wang 0039": "02/736-39"}, "abstract": " Abstract:The overlay network has been widely deployed by Service Providers to provide services. Since there are multiple SPs built upon the same ISP, their overlays are co-existing and may interfere with each other. The selfishness of overlay may lead to sub-optimal performance and traffic arrangement dilemma for overlays. To optimize the performances of overlays and maximize the benefit of SPs, we proposed a cooperative traffic management framework. Several models are applied to analyze and solve the overlay routing problem, the revenue allocation problem, and the coalition formation problem in the framework. Simulations are performed to evaluate the framework."}, "conf/apnoms/YangLWQS14": {"title": "Multiple features for image retrieval in distributed datacenter.", "url": "https://doi.org/10.1109/APNOMS.2014.6996543", "year": "2014", "author": {"Di Yang": "29/5427", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1"}, "abstract": " Abstract:The emergence of cloud datacenters enhances the capability of online data storage. Since massive data is stored in datacenters, it is necessary to effectively locate interest data in such a distributed system. However, traditional search techniques only allow users to search images over exact-match keywords through a centralized index. These techniques cannot satisfy the requirements of content based image retrieval (CBIR). In this paper, we propose the scalable image retrieval framework which can efficiently support content similarity search in the distributed environment. Its key idea is to integrate image fusion features into distributed hash tables (DHTs) by exploiting the property of the locality sensitive hashing (LSH). Thus, the images with similar content are most likely gathered into the same node without the knowledge of any global information. To the best of our knowledge, there is less comprehensive study on large-scale CBIR with fusion features in the distributed environment."}, "conf/giis/GongLWQYF14": {"title": "Improving the stability in overlay and native networks.", "url": "https://doi.org/10.1109/GIIS.2014.6934275", "year": "2014", "author": {"Jun Gong": "62/28", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Di Yang": "29/5427", "Min Feng 0003": "04/3799-3"}, "abstract": " Abstract:The objective of overlay routing is to minimize the latency of overlay users, while the objective of traffic engineering is to minimize the maximum link utilization of overall network. Thus, there exists the interaction between overlay routing and traffic engineering in the multi-layer network topology, which may lead to endless routing oscillations and performance degradation in the network. In order to solve the oscillation problem, we propose an alternative-offers bargaining model to investigate the interaction process. We view each route of two players as an offer. With the knowledge from the previous round, two strategies are introduced in each offer to weaken the inevitable conflict between two players. The simulation results show that our proposed strategies can improve the default interaction and achieve satisfactory results at the stable state."}, "conf/icc/FengLWQQ14": {"title": "Topology-aware Virtual Network Embedding based on multiple characteristics.", "url": "https://doi.org/10.1109/ICC.2014.6883774", "year": "2014", "author": {"Min Feng 0003": "04/3799-3", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Sude Qing": "122/5072", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Network Virtualization provides a promising tool to allow multiple heterogeneous virtual networks to run on a shared substrate network simultaneously. A long-standing challenge in Network Virtualization is the Virtual Network Embedding (VNE) problem: how to embed virtual networks onto specific physical nodes and links in the substrate network effectively and efficiently. Recent research presents several heuristic algorithms that only consider single network topological attribute, which may lead to decreased utilization of resources. In this paper, we introduce seven complementary characteristics that reflect different topological attributes, and propose three topology-aware VNE algorithms by leveraging their respective advantages. Due to overall considering topological attributes of substrate and virtual networks through multiple characteristics, our study better coordinates node and link embedding. Extensive simulations demonstrate that our algorithms improve the long-term average revenue, acceptance ratio, and revenue/cost ratio compared to previous algorithms."}, "conf/icc/CuiLWQW14": {"title": "Cooperative overlay routing in a multiple overlay environment.", "url": "https://doi.org/10.1109/ICC.2014.6883797", "year": "2014", "author": {"Ziteng Cui": "150/5743", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jing Wang 0039": "02/736-39"}, "abstract": " Abstract:Overlay networks have been widely developed over the past few years. More and more overlays are deployed on the top of the same native network, and share the same physical resources. Competing for these physical resources, co-existing overlays may affect each other adversely. It has been showed that by using selfish overlay routing, co-existing overlays would be likely to converge to a Nash equilibrium which is sub-optimal. However, to achieve the global optimal may also cause the performance degradation of certain overlays, which make it hard to realize. Inspired by the Nash bargaining solution, a cooperative method is proposed for two co-existing overlays to achieve a near Pareto optimal. Simulations are performed to evaluate the proposed approach. The results show that the approach is effective and efficient in the multiple overlay networks environment."}, "conf/icpads/YangLQWSJ14": {"title": "Combination feature for image retrieval in the distributed datacenter.", "url": "https://doi.org/10.1109/PADSW.2014.7097871", "year": "2014", "author": {"Di Yang": "29/5427", "Jianxin Liao": "60/4951", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Shan Jiang 0008": "04/2910-8"}, "abstract": " Abstract:Since the emergence of cloud datacenters provides an enormous amount of resources easily accessible to people, it is challenging to provide an efficient search framework in such a distributed environment. However, traditional search techniques only allow users to search images over exact-match keywords through a centralized index. These methods are insufficient to meet requirements of content based image retrieval (CBIR) and more powerful search frameworks are needed. In this paper, we present LCFIR, an effective image retrieval framework for fast content location in the distributed situation. It adopts the peer-to-peer paradigm and combines color and edge features. The basic idea is to construct multiple replicas of an image's index through exploiting the property of Locality Sensitive Hashing (LSH). Thus, the indexes of similar images are probabilistically gathered into the same node without the knowledge of any global information. The empirical results show that the system is able to yield high accuracy with load balancing, and only contacts a few number of the participating nodes."}, "conf/icufn/0094L0014": {"title": "Cooperative wireless relay networks using partial rateless codes.", "url": "https://doi.org/10.1109/ICUFN.2014.6876739", "year": "2014", "author": {"Lei Zhang 0094": "97/8704-94", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Cooperative communications enhance throughput, link reliability, power consumption, and coverage in wireless relay networks, where many relay nodes in the network work together in order to transmit information from a source to a destination [1], [2]. Many researches have been done to design and analyze the routing algorithms and protocols [3], [4]. However, frequent feedback is required for transmitter adjusting the phases of the transmit signal to optimize the receive signal [5]. Thus, the use of rateless codes has been introduced into cooperative relay networks [6], which are attractive because the realized data transmission rate is automatically a function of the instantaneous channel state, without the transmitting source or relays knowing it. Rateless codes generate a potentially infinite number of encoded symbols, which are transmitted until an acknowledgement is received from the recipient."}, "conf/icufn/ZhangLW014": {"title": "LT codes with loss rate and decreasing ripple size for wireless relay communications.", "url": "https://doi.org/10.1109/ICUFN.2014.6876783", "year": "2014", "author": {"Lei Zhang": "97/8704", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:In this paper, new fountain codes named LT-LR-DRS (Luby Transfer with Loss Rate and Decreasing Ripple Size) codes are proposed for wireless relay communications. The transmission is divided into two phases: at first phase, all original symbols are transmitted and some original symbols are lost during transmission; encoded symbols generated by LT-LR-DRS codes are transmitted at second phase to recover the lost symbols. In LT-LR-DRS, an innovative degree distribution instead of RSD (Robust Soliton degree Distribution) is proposed. The minimum degree of encoded symbols is decided by loss rate and the ripple size is decreased during decoding process. We show both analytically and through real experiments, that LT-LR-DRS codes have better performance in term of average overhead, complexity and memory usage compared to LT, SLT-LR (Shifted LT with Loss Rate), LT-DRS (LT with Decreasing Ripple Size) codes."}, "conf/iscc/TianLWQ14": {"title": "Overlay routing network construction by introducing Super-Relay nodes.", "url": "https://doi.org/10.1109/ISCC.2014.6912517", "year": "2014", "author": {"Shengwen Tian": "42/2738", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Overlay routing has emerged as a promising approach to improve reliability and efficiency of the Internet. However, deploying a routing overlay network requires the placement and maintenance of overlay infrastructure, especially, the selection and placement of key relay nodes. Spurred by the observation that a few relay nodes with high betweenness centrality, which are called Super-Relay nodes, can provide more optimal routes for a large number of node pairs, we propose an overlay routing network constructing method by introducing Super-Relay nodes. In detail, we focus on the selection and connection of Super-Relay nodes to optimize routing quality in a resilient and scalable manner, by which we address K-Minimum Spanning Tree with Super-Relay nodes algorithm (SR-KMST). Using simulations on a real Internet topology, we conclude that our approach can provide high-quality overlay routing service, and achieve better robustness by comparing with other algorithms."}, "conf/iscc/WangLWQTL14": {"title": "Introducing collaborations for multi-path selection of multiple selfish overlays.", "url": "https://doi.org/10.1109/ISCC.2014.6912456", "year": "2014", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Jing Wang 0039": "02/736-39", "Qi Qi 0001": "80/6406-1", "Shengwen Tian": "42/2738", "Tonghong Li": "98/4254"}, "abstract": " Abstract:In complex Internet, different overlay flows are likely to share and compete the same congestible resources. We present a game-theoretic study of the selfish strategic collaboration of multiple overlays when they are allowed to use multipath transfer, which is referred as the multipath selection game. Then we consider the equilibrium in this multipath selection game model where selfish players distribute their overlay traffic. Maximization of the utility functions for each overlay is the criterion of optimality. We adopt the objective of throughput maximization to capture the most typical overlay behaviors, and use the usual TCP as the basis of our analysis. We show analytically the existence and uniqueness of Nash equilibria in these games. Furthermore, we find that the loss of efficiency of Nash equilibria can be arbitrarily large if overlays do not have resource limitations. Our simulations confirm effectiveness and TCP-friendliness of multipath transfer for a range of path number and in the presence of multiple overlay traffic."}, "conf/lcn/YangLQWL14": {"title": "An image retrieval framework for distributed datacenters.", "url": "https://doi.org/10.1109/LCN.2014.6925803", "year": "2014", "author": {"Di Yang": "29/5427", "Jianxin Liao": "60/4951", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Tonghong Li": "98/4254"}, "abstract": " Abstract:As massive data is stored in cloud datacenters, it is necessary to effectively locate interest data in such a distributed environment. However, since it is difficult to create a visual vocabulary due to the lack of global information, most existing systems of Content Based Image Retrieval (CBIR) only focus on global image features. In this paper, we propose a novel image retrieval framework, which efficiently incorporates the bag-of-visual-word model into Distributed Hash Tables (DHTs). Its key idea is to establish visual words for local image features by exploiting the merit of Locality Sensitive Hashing (LSH), so that similar image patches are most likely gathered into the same nodes without the knowledge of any global information. Extensive experimental results demonstrate that our approach yields high accuracy at very low cost, while keeping the load balanced."}, "conf/vtc/QiLCW14": {"title": "A Self-Adaption Handoff Mechanism for Multimedia Services in Mobile Cloud Computing.", "url": "https://doi.org/10.1109/VTCFall.2014.6965845", "year": "2014", "author": {"Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Yufei Cao": "92/2499", "Jingyu Wang 0001": "37/2749-1"}, "abstract": " Abstract:Developing mobile multimedia cloud services over heterogeneous wireless networks poses a challenge for service continuity. The degraded link quality and connection losses are likely to happen and these may affect service execution times and service availability in mobile cloud computing scenarios. To improve handoff quality and minimize utilized bandwidth, we propose a self-adaption handoff scheme for multimedia services in mobile cloud computing. The proposed scheme uses multipath transmission for media flows, and consists of the duplicate mode and the effective mode, which are changed according to the network condition. Analytic model and simulation are developed to investigate our new scheme. The results demonstrate that the new mechanism can realize seamless handoff for multimedia services in cloud, reduce the packet loss rate, as well as obtain a more efficient use of the scarce wireless bandwidth and the power of mobile devices."}, "conf/apcc/LiaoLZWQ13": {"title": "A multi-objective service selection algorithm for service composition.", "url": "https://doi.org/10.1109/APCC.2013.6765919", "year": "2013", "author": {"Jianxin Liao": "60/4951", "Yang Liu": "51/3710", "Xiaomin Zhu 0002": "09/4144-2", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Service composition is an efficient way to implement a service of complex business process. In service composition, service selection considering multiple QoS objectives is a challenge. Existing methods mainly utilize fitness function or constraint technique to convert multiple objectives service composition problems to single objective ones. These methods take effect with priori knowledge of problem's solution space. Besides, only one solution can be obtained in each execution of one existing method. Users can hardly acquire evenly distributed solutions with acceptable computation cost. In this paper, we propose a lightweight particle swarm optimization service selection algorithm using approximate distance and external archive mechanisms. Simulation results demonstrate that the proposed algorithm is more effective and efficient than the compared algorithm for multi-objective service composition problems."}, "conf/apcc/GongLW0Z13": {"title": "Reducing the oscillations between overlay routing and traffic engineering by repeated game theory.", "url": "https://doi.org/10.1109/APCC.2013.6766017", "year": "2013", "author": {"Jun Gong": "62/28", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Lei Zhang 0094": "97/8704-94"}, "abstract": " Abstract:Due to the conflicts existing in the route objectives of overlay routing and traffic engineering, the interaction between the two selfish players may converge to an inefficient Nash equilibrium point, even if the better choice may exist. We formulate the interaction as an infinitely repeated two-player game, where overlay routing aims to minimize the average latency of overlay users and traffic engineering aims to minimize the maximum link utilization of overall network. The whole interaction process could be divided into two stages - learning stage and practice stage. The former collects the historical information and finds the best point with a simple learning algorithm, then the latter uses this point as equilibrium point and converges to it. The simulation results show that both overlay routing and traffic engineering can converge to the win-win results, and the overall network can avoid the performance volatility from endless oscillations."}, "conf/giis/TianLWWQZ13": {"title": "Load-balanced one-hop overlay source routing over shortest path.", "url": "https://doi.org/10.1109/GIIS.2013.6684347", "year": "2013", "author": {"Shengwen Tian": "42/2738", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Jing Wang 0039": "02/736-39", "Qi Qi 0001": "80/6406-1", "Lei Zhang 0094": "97/8704-94"}, "abstract": " Abstract:In one-hop overlay source routing, when the given primary path suffers from the link failure or performance degradation, the source can reroute the traffic to the destination through one intermediate overlay node. However, the over-heavy traffic passing through the same intermediate node would cause frequent package loss and network delay, which can degrade the throughput and utilization of network. To overcome this problem, we propose a Load-Balanced One-hop Overlay Source Routing based on shortest path. To reduce the complexity of the algorithm, we first introduce a heuristic method to compute the relay nodes for constructing one-hop overlay routing paths, and then formulate the problem as a linear programming (LP) problem for load balancing, whose goal is to minimize the worse-case network congestion ratio. Simulations results show that our proposed scheme reduces the network congestion ratio dramatically, and can achieve high-quality overlay routing service."}, "conf/giis/ZhangLWQXTL13": {"title": "Diversified SLT codes based on feedback for communication over wireless networks.", "url": "https://doi.org/10.1109/GIIS.2013.6684363", "year": "2013", "author": {"Lei Zhang 0094": "97/8704-94", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Tong Xu 0002": "70/6770-2", "Shengwen Tian": "42/2738", "Minyan Liao": "130/3902"}, "abstract": " Abstract:Fountain codes, such as LT (Luby transform) codes have been practical techniques for communication over wireless networks. RSD (Robust Soliton degree Distribution) is a key component of LT codes. However, LT codes do not make use of feedback. SLT (Shifted LT) codes have shown benefit by shifting RSD based on the number of recovered input symbols. However, degree values are limited to a small set. In this paper, DSLT (Diversified SLT) codes with a new degree distribution are proposed to explore the full potential of feedback. DSLT codes modify the RSD of LT codes in a different manner. They diversify the value of degree compared with SLT codes. Furthermore, hybrid SLT/DSLT codes were proposed to address the problem that most input symbols are recovered at the ending. We show both analytically and through real experiments, that DSLT codes have better performance in term of communication complexity and memory usage compared with SLT codes."}, "conf/icmlc/LiaoZWLQX13": {"title": "Security and efficient data dissemination over wireless sensor network with raptor codes.", "url": "https://doi.org/10.1109/ICMLC.2013.6890856", "year": "2013", "author": {"Jianxin Liao": "60/4951", "Lei Zhang 0094": "97/8704-94", "Jingyu Wang 0001": "37/2749-1", "Minyan Liao": "130/3902", "Qi Qi 0001": "80/6406-1", "Tong Xu 0002": "70/6770-2"}, "abstract": " Abstract:Security and efficient data dissemination over wireless network has been the focus of research in academic, military and commercial domains for many years. Unfortunately, proposed security solutions for data dissemination over wireless network are not efficient enough. In this work, a novel security and efficient data dissemination scheme which integrates authentication into a SE-Raptor (security enhanced Raptor) codes is investigated. In SE-Raptor codes, encoding packets with degree one are encoded again, and many promiscuous packets are took part in encoding. Simulation results show that the proposed method has better performance in terms of encoding ratio, degree ratio, encoding and decoding efficiency. Additionally, our analysis shows that data confidentiality, bogus data protection and deny of service properties are provided by the proposed scheme."}, "conf/lcn/LiaoLZWQ13": {"title": "Accurate QoS-based service selection algorithm for service composition.", "url": "https://doi.org/10.1109/LCN.2013.6761265", "year": "2013", "author": {"Jianxin Liao": "60/4951", "Yang Liu": "51/3710", "Xiaomin Zhu 0002": "09/4144-2", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:The most important thing of service composition (SC) is to select optimal candidate service instances compliant with non-functional requirements (e.g. QoS and load balance constraints). Particle swarm optimization (PSO) is known as an effective and efficient algorithm, which is widely used in this process. However, the premature convergence and diversity loss of PSO may result in suboptimal solutions. In this paper, we propose an accurate sub-swarms particle swarm optimization (ASPSO) algorithm which locates optimal solutions by using sub-swarms searching grid cells in which the density of feasible solutions is high. Simulation results demonstrate that the proposed algorithm improves the accuracy of the standard PSO algorithm in service composition."}, "conf/globecom/QingQWXL12": {"title": "Topology-aware virtual network embedding through bayesian network analysis.", "url": "https://doi.org/10.1109/GLOCOM.2012.6503512", "year": "2012", "author": {"Sude Qing": "122/5072", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Tong Xu 0002": "70/6770-2", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Multiple heterogenous virtual networks are given the ability to run on a shared infrastructure simultaneously as independent slices in the network virtualization environment. However, a major challenge is how to map multiple virtual networks, with specific node and link constraints, onto the shared substrate network, known as virtual network embedding problem. By taking topology attribute into account, topology-aware virtual network embedding algorithms efficiently improve the performance by leveraging a node ranking method based on Markov chain. However, as the basis of node ranking, the resource evaluation of node which is calculated as the product of its CPU and bandwidth may be incorrect. Moreover, a greedy matching strategy is always applied in the node mapping stage, which may lead to unnecessary bandwidth consumption by ignoring the relationships between the mapped substrate nodes and the mapping one. In this paper, we re-think the topology-aware virtual network embedding from a statistical perspective by proposing a statistical method to generate a dependency matrix representing the importance of every node and the relationships between every two nodes in the substrate network. Based on this dependency matrix, bayesian network analysis is leveraged to iteratively select the substrate node, with the closest relationship to the selected ones, to achieve node mapping process. Extensive simulations were conducted and the results show that our proposed algorithm has better performance in the long-term run."}, "conf/icc/QingLWZQ12": {"title": "Hybrid virtual network embedding with K-core decomposition and time-oriented priority.", "url": "https://doi.org/10.1109/ICC.2012.6363761", "year": "2012", "author": {"Sude Qing": "122/5072", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Xiaomin Zhu 0002": "09/4144-2", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Network virtualization provides a powerful tool to allow multiple networks, each customized to a specific purpose, to run on a shared substrate. However, a big challenge is how to map multiple virtual networks onto specific nodes and links in the shared substrate network, known as virtual network embedding problem. Previous works in virtual network embedding can be decomposed to two classes: two-stage virtual network embedding and one-stage virtual network embedding. In this paper, by pruning the topology of virtual network using k-core decomposition, a hybrid virtual network embedding algorithm is proposed to leverage the respective advantage of the two kinds of algorithm simultaneously in the mapping process. In addition, a time-oriented scheduling policy is introduced to improve the mapping performance. We conduct extensive simulations and the results show that the proposed algorithm obtains more revenue in the long-term run."}, "conf/icws/LiaoLZXWS12": {"title": "Ontology Alignment by Combining Lexical Analysis with Consequences from Reasoners.", "url": "https://doi.org/10.1109/ICWS.2012.57", "year": "2012", "author": {"Jianxin Liao": "60/4951", "Xiulei Liu": "23/10432", "Xiaomin Zhu 0002": "09/4144-2", "Tong Xu 0002": "70/6770-2", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1"}, "abstract": " Abstract:Aligning different ontologies from similar (or same) domains is an active field of current research. There are various solutions which process and analyze lexical, structural or semantic information to align ontologies. However, there are few solutions that focus on interpreting the concepts that entities are presented with and using them in relation to the semantics implied in an ontology. In this paper, the prototype (OACLAI) is presented to tackle this by combining lexical analysis with consequences from reasoners which reflect the semantics implied in an ontology. We evaluate OACLAI over the four real ontologies and compare it against the seven solutions. The experiments show that the accuracy of OACLAI is higher than those of others on average."}, "conf/globecom/LiaoLZXW11": {"title": "Niching Particle Swarm Optimization Algorithm for Service Composition.", "url": "https://doi.org/10.1109/GLOCOM.2011.6133596", "year": "2011", "author": {"Jianxin Liao": "60/4951", "Yang Liu": "51/3710", "Xiaomin Zhu 0002": "09/4144-2", "Tong Xu 0002": "70/6770-2", "Jingyu Wang 0001": "37/2749-1"}, "abstract": " Abstract:Service composition constructs composite applications with different services to fulfill numerous service requirements. Different users have various QoS demands for composite services. In this paper, we present a service composition model considering not only multiple QoS constraints but also load balance factors. Moreover, a service selection algorithm based on niching technique and particle swarm optimization (PSO) is proposed for the service composition problem. It supports optimization problems with any kinds of constraints and objective functions. Simulation results show the proposed algorithm is effective and efficient for service composition under any circumstances."}, "conf/icc/WangLZ08": {"title": "On Preventing Unnecessary Fast Retransmission With Optimal Fragmentation Strategy.", "url": "https://doi.org/10.1109/ICC.2008.24", "year": "2008", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": " Abstract:Multi-path transferring on the different paths with diverse delays would cause reordering and so unnecessary retransmission which eventually degrades the throughput of connections considerably. To address this issue, we first presented an optimal fragmentation strategy that should be held to avoid unnecessary retransmission/recovery timeouts events in TCP or SCTP due to reordering. Then, we introduced an analytical framework for the optimal fragmentation strategy of real-time traffic. Specifically, this paper formulates optimal traffic fragmentation as a constrained optimization problem using deterministic network calculus and derives its closed-form solution by our proposed progressive water-filling algorithm. Compared with previous work, the proposed scheme is simpler to implement and enforce."}, "conf/icc/LiaoWZ08": {"title": "cmpSCTP: An Extension of SCTP to Support Concurrent Multi-Path Transfer.", "url": "https://doi.org/10.1109/ICC.2008.1078", "year": "2008", "author": {"Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": " Abstract:This paper introduced cmpSCTP, a transport layer solution for concurrent multi-path transfer that modifies the standard Stream Control Transmission Protocol (SCTP). The cmpSCTP aims at exploiting SCTP's multi-homing capability by selecting several best paths among multiple available network interfaces to improve data transfer rate to the same multi-homed device. Through the use of path monitoring and packet allotment techniques, cmpSCTP tries to transmit given amount of packets at corresponding path as its ability. At the same time, cmpSCTP updates the transmission strategy based on the real-time information of all of paths. Using cmpSCTP's flexible path management capability, we may switch the flow between multiple paths automatically to realize seamless path handover. Extensive simulations under different scenarios using OPNET verified that cmpSCTP can effectively enhance transmission efficiency and highlighted the superiority of cmpSCTP against the other SCTP's extension implementations under performance indexes such as throughput, handover latency, packet delay, and packet loss."}, "journals/ijon/HaoSRWQL22": {"title": "Query-aware video encoder for video moment retrieval.", "url": "https://doi.org/10.1016/j.neucom.2022.01.085", "year": "2022", "author": {"Jiachang Hao": "210/8416", "Haifeng Sun 0001": "00/11044-1", "Pengfei Ren": "31/10610", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": "Given an untrimmed video and a sentence query, video moment retrieval is to locate a target video moment that semantically corresponds to the query. It is a challenging task that requires a joint understanding of natural language queries and video contents. However, video contains complex contents, including query-related and query-irrelevant contents, which brings difficulty for the joint understanding. To this end, we propose a query-aware video encoder to capture the query-related visual contents. Specifically, we design a query-guided block following each encoder layer to recalibrate the encoded visual features according to the query semantics. The core of query-guided block is a channel-level attention gating mechanism, which could selectively emphasize query-related visual contents and suppress query-irrelevant ones. Besides, to fully match with different levels of contents in videos, we learn hierarchical and structural query clues to guide the visual content capturing. We disentangle sentence query into a semantics graph and capture the local contexts inside the graph via a trilinear model as query clues. Extensive experiments on Charades-STA and TACoS datasets demonstrate the effectiveness of our approach, and we achieve the state-of-the-art on the two datasets."}, "journals/jsac/NingSFYQWLH22": {"title": "Following the Correct Direction: Renovating Sparsified SGD Towards Global Optimization in Distributed Edge Learning.", "url": "https://doi.org/10.1109/JSAC.2021.3118396", "year": "2022", "author": {"Wanyi Ning": "311/3670", "Haifeng Sun 0001": "00/11044-1", "Xiaoyuan Fu": "10/575", "Xiang Yang": "56/5739", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Zhu Han 0001": "83/514"}, "abstract": " Abstract:Distributed edge learning collaborates powerful edge devices to train a shared global model. Since the frequent communication between the server and workers is very expensive, it is desired to accelerate the learning process. The gradient sparsification is an efficient method that only uploads a small subset of gradient elements. However, most existing works neglect the distributed nature of local datasets, and consequently the local gradients uploaded by edge devices cannot follow the global correct optimization direction well, which results in the loss of accuracy. In this paper, we propose a new gradient sparsification with a renovating mechanism, called Global Renovating Stochastic Gradient Descent (GRSGD). GRSGD utilizes the previous-round global gradient to estimate the current global one and renovates the current zero-sparsified local gradients. It mitigates the communication overhead while making the convergence direction of training closer to the global optimization, accelerating the distributed edge learning process. We provide a theoretical convergence guarantee for our algorithm based on the non-convex assumption, which better fits most deep learning problems. With extensive experiments in PyTorch, we show that GRSGD effectively accelerates the learning process with a smaller communication cost and a faster convergence rate on most training tasks. For example, on ImageNet MnasNet, GRSGD cuts down the gradient size from 8.47MB to 2.13MB while achieving 9.6%+ higher accuracy."}, "journals/taslp/MaSWQL22": {"title": "Extractive Dialogue Summarization Without Annotation Based on Distantly Supervised Machine Reading Comprehension in Customer Service.", "url": "https://doi.org/10.1109/TASLP.2021.3133206", "year": "2022", "author": {"Bing Ma": "53/5636", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Given a long dialogue, the dialogue summarization system aims to obtain a shorter highlight which retains the important information in the original text. For the customer service scenarios, the summaries of most dialogues between an agent and a user focus on several fixed key points, such as users’ question, users’ purpose, the agent’s solution, and so on. Traditional extractive methods are difficult to extract all predefined key points exactly. Furthermore, there is a lack of large-scale and high-quality extractive summarization datasets containing the annotation for key points. Moreover, the speaker’s role information is ignored or not fully utilized in previous work. In order to solve the above challenges, we propose a Distant Supervision based Machine Reading Comprehension model for extractive Summarization (DSMRC-S). DSMRC-S transforms the summarization task into the machine reading comprehension problem, to fetch key points from the original text exactly according to the predefined questions. In addition, a distant supervision method is proposed to alleviate the lack of eligible extractive summarization datasets. What’s more, a speaker’s role token and the solver classification task are proposed to make full use of speaker’s role information. We conduct experiments on a real-world summarization dataset collected in customer service scenarios, and the results show that the proposed method outperforms the strong baseline methods by 6 percentage points on ROUGE\nL_L\n."}, "journals/tcc/HeWQSL22": {"title": "Towards Intelligent Provisioning of Virtualized Network Functions in Cloud of Things: A Deep Reinforcement Learning Based Approach.", "url": "https://doi.org/10.1109/TCC.2020.2985651", "year": "2022", "author": {"Bo He": "04/2868", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Cloud of Things (CoT) is an integration of Internet of Things (IoT) and cloud computing, where Network Function Virtualization (NFV) can dynamically provide Virtualized Network Functions (VNFs) for IoT devices based on service-specific requirements. The provisioning of VNFs in CoT is formulated as an online decision-making problem, but widely used methods mostly focus on characterizing the environment using simple models to obtain the optimal solution. Valuable historical experience on provisioning for the best long-term benefits is ignored and Quality of Service (QoS) requirements for different types of CoT services are also not considered, which leads to inefficient and coarse-gained provisioning. In this article, an intelligent provisioning framework of VNFs is proposed for adaptive CoT resource scheduling according to traffic identification of heterogeneous network services. The framework leverages a Deep Reinforcement Learning (DRL)-based model to make decisions based on the complexity of network environments and traffic variances. In this model, a policy gradient DRL algorithm, namely, Policy Optimization using Kronecker-Factored Trust Region (POKTR) is adopted to obtain the stable performance by a novel surrogate objective function. Experimental results verify that our framework improves the QoS in CoT by real-time VNFs provisioning. The DRL-based model with POKTR algorithm reduces network congestion and achieves higher throughput than other DRL algorithms."}, "journals/tccn/DongZQWSYSZL22": {"title": "Intelligent Joint Network Slicing and Routing via GCN-Powered Multi-Task Deep Reinforcement Learning.", "url": "https://doi.org/10.1109/TCCN.2021.3136221", "year": "2022", "author": {"Tianjian Dong": "294/8527", "Zirui Zhuang": "235/7014", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "F. Richard Yu": "16/6654", "Tao Sun": "74/3590", "Cheng Zhou": "61/3491", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:In 6G mobile systems, network slicing is an emerging technology to support services with distinct requirements by dividing a common infrastructure into multiple logical networks. However, as a network management method, it is difficult for network slicing to achieve a real-time resource allocation to satisfy the stringent requirement of services in 6G networks. This paper introduces a joint network slicing and routing mechanism, which combines the network management and control framework to provide a fine-grained, real-time and dynamic resource allocation. Graph Convolutional Networks (GCN)-powered Multi-Task Deep Reinforcement Learning (DRL) is proposed to solve this complicated resource allocation problem. We first extend the DRL model into multi-task manner, where multiple output branches are matched to joint scheduling resources in every network slice. GCN with differentiable pooling mechanism is integrated into DRL model to capture the topological information from graph-structured network status. We implement our model in the SDN controller and evaluate it with representative topologies. The packet-level experiments show that 1) compared to rule-based and other learning-based methods, GCN-powered multi-task DRL can improve the performance of joint network slicing and routing; 2) our method is robust to diverse network environments; 3) in contrast with other learning-based algorithms, our method achieves a better performance."}, "journals/tetc/QiLLWSL22": {"title": "Learning Low Resource Consumption CNN Through Pruning and Quantization.", "url": "https://doi.org/10.1109/TETC.2021.3050770", "year": "2022", "author": {"Qi Qi 0001": "80/6406-1", "Yan Lu": "15/4830", "Jiashi Li": "241/9364", "Jing-Yu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Deep learning models have evolved into powerful tools that can be used for many artificial intelligence tasks. However, deploying deep neural networks into real-world applications is still challenging due to their high computational complexity and storage overhead. Fortunately, a densely connected neural network can be converted into a sparsely connected network with low resource demand by the neural network compression. Since deep neural networks are complicated, compression mechanism should find a tradeoff between compression ratio and model accuracy. In this article, by analyzing the statistics of channel connection, we propose an interactive neural network compression mechanism including out-in-channel pruning and neural network quantization. Many channel pruning works apply structured sparsity regularization on each layer separately. We consider correlations between successive layers to retain predictive power of the compact network. A global greedy pruning algorithm is designed to remove redundant out-in-channels in an iterative way. Moreover, in order to solve the shortcomings of the one-shot quantization, we propose the incremental quantization algorithm in the dimension of the output channel, which can smooth network fluctuations and recover accuracy better during retraining. Our mechanism is comprehensively evaluated with various Convolutional Neural Networks (CNN) architectures on popular datasets. Notably, on ImageNet-1K, the out-in-channel pruning reduce 54.0 percent FLOPS on AlexNet and 50.0 percent FLOPs on ResNet-50 with only 0.15 and 0.37 percent top-1 accuracy drop respectively. On classification and style transfer tasks, the superiority of incremental quantization increases with the decrease of the number of quantization bits."}, "journals/tip/RenSHQWL22": {"title": "A Dual-Branch Self-Boosting Framework for Self-Supervised 3D Hand Pose Estimation.", "url": "https://doi.org/10.1109/TIP.2022.3192708", "year": "2022", "author": {"Pengfei Ren": "31/10610", "Haifeng Sun 0001": "00/11044-1", "Jiachang Hao": "210/8416", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Although 3D hand pose estimation has made significant progress in recent years with the development of the deep neural network, most learning-based methods require a large amount of labeled data that is time-consuming to collect. In this paper, we propose a dual-branch self-boosting framework for self-supervised 3D hand pose estimation from depth images. First, we adopt a simple yet effective image-to-image translation technology to generate realistic depth images from synthetic data for network pre-training. Second, we propose a dual-branch network to perform 3D hand model estimation and pixel-wise pose estimation in a decoupled way. Through a part-aware model-fitting loss, the network can be updated according to the fine-grained differences between the hand model and the unlabeled real image. Through an inter-branch loss, the two complementary branches can boost each other continuously during self-supervised learning. Furthermore, we adopt a refinement stage to better utilize the prior structure information in the estimated hand model for a more accurate and robust estimation. Our method outperforms previous self-supervised methods by a large margin without using paired multi-view images and achieves comparable results to strongly supervised methods. Besides, by adopting our regenerated pose annotations, the performance of the skeleton-based gesture recognition is significantly improved."}, "journals/tnn/WangCJXYTYWQ22": {"title": "Robust Unsupervised Video Anomaly Detection by Multipath Frame Prediction.", "url": "https://doi.org/10.1109/TNNLS.2021.3083152", "year": "2022", "author": {"Xuanzhao Wang": "277/9759", "Zhengping Che": "160/5944", "Bo Jiang": "34/2005", "Ning Xiao": "69/2068", "Ke Yang": "80/4136", "Jian Tang 0008": "181/2667-8", "Jieping Ye": "03/5454", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": " Abstract:Video anomaly detection is commonly used in many applications, such as security surveillance, and is very challenging. A majority of recent video anomaly detection approaches utilize deep reconstruction models, but their performance is often suboptimal because of insufficient reconstruction error differences between normal and abnormal video frames in practice. Meanwhile, frame prediction-based anomaly detection methods have shown promising performance. In this article, we propose a novel and robust unsupervised video anomaly detection method by frame prediction with a proper design which is more in line with the characteristics of surveillance videos. The proposed method is equipped with a multipath ConvGRU-based frame prediction network that can better handle semantically informative objects and areas of different scales and capture spatial-temporal dependencies in normal videos. A noise tolerance loss is introduced during training to mitigate the interference caused by background noise. Extensive experiments have been conducted on the CUHK Avenue, ShanghaiTech Campus, and UCSD Pedestrian datasets, and the results show that our proposed method outperforms existing state-of-the-art approaches. Remarkably, our proposed method obtains the frame-level AUROC score of 88.3% on the CUHK Avenue dataset."}, "journals/wc/HeWQSZZLL22": {"title": "Resilient QUIC Protocol for Emerging Wireless Networks.", "url": "https://doi.org/10.1109/MWC.003.2100610", "year": "2022", "author": {"Bo He": "04/2868", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Haibo Zhou": "14/8431", "Long Zhang": "48/2807", "Kaixin Liu": "14/9742", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:With the development of communication technology, higher requirements of emerging services in emerging 5G-Advanced wireless network scenarios bring significant challenges to network resilience. It is difficult in conventional transport protocols like TCP and UDP to ensure resilient transmissions in heterogeneous scenarios. As an alternative to the conventional HTTP2/TCP stack, the Quick UDP Internet Connections (QUIC) protocol can realize resilient transmission control since it supports novel features and extensions, such as multi-path transmission and connection migration. In this article, we first analyze the challenges of network resilience in several dominant 5G-Advanced wireless network scenarios with conventional protocols. To tackle these challenges, we propose the first QUIC-enabled resilient transmission control protocol, QUIC-RE5, introducing QUIC to emerging wireless scenarios. QUIC-RE5 contains three customized QUIC-based blocks that enhance the network resilience while satisfying the communication requirements in these scenarios. The experimental results prove that QUIC-RE5 improves the resilience and performance in 5G-Advanced wireless networks, compared to networks using conventional protocols and schemes."}, "journals/corr/abs-2207-14698": {"title": "Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding.", "url": "https://doi.org/10.48550/arXiv.2207.14698", "year": "2022", "author": {"Jiachang Hao": "210/8416", "Haifeng Sun 0001": "00/11044-1", "Pengfei Ren": "31/10610", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": "\n      Abstract:  Temporal grounding aims to locate a target video moment that semantically\ncorresponds to the given sentence query in an untrimmed video. However, recent\nworks find that existing methods suffer a severe temporal bias problem. These\nmethods do not reason the target moment locations based on the visual-textual\nsemantic alignment but over-rely on the temporal biases of queries in training\nsets. To this end, this paper proposes a novel training framework for grounding\nmodels to use shuffled videos to address temporal bias problem without losing\ngrounding accuracy. Our framework introduces two auxiliary tasks, cross-modal\nmatching and temporal order discrimination, to promote the grounding model\ntraining. The cross-modal matching task leverages the content consistency\nbetween shuffled and original videos to force the grounding model to mine\nvisual contents to semantically match queries. The temporal order\ndiscrimination task leverages the difference in temporal order to strengthen\nthe understanding of long-term temporal contexts. Extensive experiments on\nCharades-STA and ActivityNet Captions demonstrate the effectiveness of our\nmethod for mitigating the reliance on temporal biases and strengthening the\nmodel's generalization ability against the different temporal distributions.\nCode is available at this https URL.\n\n    "}, "journals/access/MemonWBFRXZ21": {"title": "Temperature and Reliability-Aware Routing Protocol for Wireless Body Area Networks.", "url": "https://doi.org/10.1109/ACCESS.2021.3117928", "year": "2021", "author": {"Saifullah Memon": "304/4023", "Jingyu Wang 0001": "37/2749-1", "Ali Raza Bhangwar": "195/3125", "Suliman Mohamed Fati": "143/9421", "Amjad Rehman": "99/7828", "Tong Xu 0002": "70/6770-2", "Lei Zhang 0094": "97/8704-94"}, "abstract": " Abstract:The Wireless Body Sensor Network (WBSN) can be envisioned as a cost-effective solution to provide monitoring and reporting services in medical and non-medical applications to improve quality of life. The dissemination of patient data in a timely and reliable manner is one of the necessities of healthcare applications of WBSN. The critical data packets are highly delay-sensitive. However, these packets reaching the destination beyond timelines undermine the benefit of such networks. To provide real-time health monitoring an adequate link (in terms of reliability, stability, and QoS) has to be maintained. However, the distinguishing characteristics of WBSN pose several challenges to be countered such as limited resources, transmission range, and unreliable wireless links in terms of QoS as low-power radios are sensitive to interference and noise. Consequently, some portions of the network experience a significant level of congestion thereby strain the communication links, available bandwidth, insufficient buffer space, increased number of collisions, packet losses, and transmission disruption. Therefore, importing QoS awareness in routing decisions is important to improve the performance of WBSN. This paper proposes a QoS-aware routing protocol named TLD-RP (Temperature, Link-reliable, and Delay-aware Routing Protocol) for WBSN. Most of the temperature-aware routing protocols proposed for the WBSN incorporate either single or composite routing metrics (temperature, hop count, or energy). However, optimized route discovery has been overlooked in most of the previous studies on QoS requirements such as link reliability, stability, and link delay. Keeping in view these limitations, the proposed TLD-RP makes use of a multi-facet composite routing metric by carefully considering the critical QoS requirements for the WBAN. The design of the proposed TLD-RP scheme centers on the link’s reliability, path delay, and link’s asymmetric property. These design factors enable the p..."}, "journals/asc/DuWSQL21": {"title": "Syntax-type-aware graph convolutional networks for natural language understanding.", "url": "https://doi.org/10.1016/j.asoc.2021.107080", "year": "2021", "author": {"Chunning Du": "254/8020", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": "The structure of a sentence conveys rich linguistic knowledge and has proven useful for natural language understanding. In this paper, we aim to incorporate syntactical constraints and long-range word dependencies into the sentence encoding procedure using the widely applied Graph Convolutional Network (GCN) and word dependency trees. Existing syntax-aware GCN methods construct the adjacency matrix by referring to whether two words are connected in the dependency tree. But they fail to model the word dependency type, which reflects how the words are linked in dependency trees. They cannot distinguish the different contributions of different word dependency paths. To avoid introducing redundant word dependencies that harm language understanding, we propose a GCN version that is extended by a novel Word Dependency Gate mechanism. Word Dependency Gate can adaptively maintain the balance between the inclusion and exclusion of specific word dependency paths based on the word dependency type and its word context. Experiments show that our approach can effectively incorporate the relevant syntactical dependency in BERT and achieve a state-of-the-art performance in the End-to-End Aspect-Based Sentiment Analysis and Relation Triple Extraction tasks."}, "journals/cm/XuYWQSL21": {"title": "Capsule Network Distributed Learning with Multi-Access Edge Computing for the Internet of Vehicles.", "url": "https://doi.org/10.1109/MCOM.001.2001130", "year": "2021", "author": {"Jie Xu 0003": "37/5126-3", "F. Richard Yu": "16/6654", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:For realizing an intelligent transport system, a vast amount of raw image data is required to train various intelligent applications on the Internet of Vehicles (IoV). A capsule network performs well in the computer vision area with fewer model parameters compared to convolutional neural networks. Due to the small-scale model, multi-access edge computing (MEC) devices can support online training for the whole capsule network model. In this article, we propose a novel framework for MEC-based capsule network (CapsMEC) distributed learning for IoV applications. Capsules in CapsMEC are specially designed to train in a collaborative way, which relieves the network pressure in MEC and the time-consuming training time of the traditional capsule network. Experimental results prove the effectiveness of the proposed framework."}, "journals/grid/XuWQSLY21": {"title": "Effective Scheduler for Distributed DNN Training Based on MapReduce and GPU Cluster.", "url": "https://doi.org/10.1007/s10723-021-09550-6", "year": "2021", "author": {"Jie Xu 0003": "37/5126-3", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951", "Di Yang": "29/5427"}, "abstract": "Parallel training accelerates the Deep Neural Networks (DNN) training by parallel GPUs. While the in-memory data transmission becomes the cross-node network transmission due to distribution of GPUs on different nodes, which drags the training time. Most researches address it by reducing the data size on network links. However, the factor of network distance is ignored. In this paper, we construct a distributed DNN training architecture based on MapReduce. The customized scheduler is designed to make the computations nodes that finish the training closer to the nodes that storage data. At the same time, the parallel training models are synchronized by adjusting the data transmission time. The experimental results show that the shortened network distance benefits the reduced network traffic usage. The resulting data transmission time decreases the training time by at least 50% and guarantees the synchronization for the parallel training."}, "journals/ijon/RenSHHC00L21": {"title": "Spatial-aware stacked regression network for real-time 3D hand pose estimation.", "url": "https://doi.org/10.1016/j.neucom.2021.01.045", "year": "2021", "author": {"Pengfei Ren": "31/10610", "Haifeng Sun 0001": "00/11044-1", "Weiting Huang": "262/0438", "Jiachang Hao": "210/8416", "Daixuan Cheng": "289/2865", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951"}, "abstract": "Making full use of the spatial information of the depth data is crucial for 3D hand pose estimation from a single depth image. In this paper, we propose a Spatial-aware Stacked Regression Network (SSRN) for fast, robust and accurate 3D hand pose estimation from a single depth image. By adopting a differentiable pose re-parameterization process, our method efficiently encodes the pose-dependent 3D spatial structure of the depth data as spatial-aware representations. Taking such spatial-aware representations as inputs, the stacked regression network utilizes multi-joint spatial context and the 3D spatial relationship between the estimated pose and the depth data to predict a refined hand pose. To further improve the estimation accuracy, we adopt a spatial attention mechanism to reduce the influence of irrelevant features for pose regression. In order to improve the speed of the network, we propose a cross-stage self-distillation mechanism to distill knowledge within the network itself. Experiments on four datasets show that our proposed method achieves state-of-the-art accuracy with high running speed around 330 FPS on a single GPU and 35 FPS on a single CPU."}, "journals/ijon/TangWQSTY21": {"title": "Deep graph alignment network.", "url": "https://doi.org/10.1016/j.neucom.2021.08.135", "year": "2021", "author": {"Wei Tang": "58/1874", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Shimin Tao": "188/1236", "Hao Yang": "54/4089"}, "abstract": "Graph alignment, also known as network alignment has many applications in data mining tasks. It aims to find the node correspondence across disjoint graphs. Recently, various methods like representation learning methods, spectral methods have been proposed to solve the graph alignment problem, but they either only consider the local structure information but ignore the neighborhood similarity, or their alignment process is easy to be disturbed by nodes with similar structure or attribute. In this paper, we consider both center and neighborhood similarities, aiming to reduce the inconsistency between them and enlarge the difference among node representations. We further propose model DGAN(Deep Graph Alignment Network) containing the DNN module and GCN module to learn more unique node representations under the guidance of the attribute-supervised module. Moreover, we theoretically prove that most spectral methods can be unified into a linear GCN model. By extensive experiments on public benchmarks, we show that our model achieves a good balance between alignment accuracy and speed over multiple datasets compared with existing methods."}, "journals/iotj/ChenQZWLH21": {"title": "Mean Field Deep Reinforcement Learning for Fair and Efficient UAV Control.", "url": "https://doi.org/10.1109/JIOT.2020.3008299", "year": "2021", "author": {"Dezhi Chen": "01/2409", "Qi Qi 0001": "80/6406-1", "Zirui Zhuang": "235/7014", "Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Zhu Han 0001": "83/514"}, "abstract": " Abstract:Unmanned aerial vehicles (UAVs) can provide flexible network coverage services. UAVs can be applied in a large number of scenarios, such as emergency communication and network access in areas without terrestrial network coverage. However, UAVs are limited to relatively short communication range and restricted energy resources. In extreme conditions such as disasters, there may also be a problem that the communication bandwidth is limited and the UAV cannot communicate with the server with a large amount of information, so a decentralized solution is expected. In addition, the interaction between multiple objectives and multiple UAVs leads to a huge state space, which makes large-scale practical applications difficult. To simplify complex interactions, we modeled the UAV control problem with mean-field game (MFG). We propose a new UAV control method, the mean-field trust region policy optimization (MFTRPO), which uses the MFG method to construct the Hamilton-Jacobi-Bellman/Fokker-Planck-Kolmogorov equation that obtains the optimal solution and solves the difficulties in the practical application through the trust region policy optimization and neural network feature embedding methods. The proposed method: 1) maximizes communication efficiency while ensuring fair communication range and network connectivity; 2) fuses the mean-field theory with deep reinforcement learning techniques; and 3) is scalable and adaptive. We conduct extensive simulations for performance evaluation. The simulation results have shown that MFTRPO significantly and consistently outperforms two commonly used baseline methods in terms of coverage, fairness, and energy consumption."}, "journals/iotj/ZhuangWQLH21": {"title": "Adaptive and Robust Routing With Lyapunov-Based Deep RL in MEC Networks Enabled by Blockchains.", "url": "https://doi.org/10.1109/JIOT.2020.3034601", "year": "2021", "author": {"Zirui Zhuang": "235/7014", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Zhu Han 0001": "83/514"}, "abstract": " Abstract:The most recent development of the Internet of Things brings massive timely sensitive and bursty data flows. Also, joint optimization on storage, computation, and communication is in need for multiaccess edge computing frameworks. The adaptive network control has been explored using deep reinforcement learning (RL), but it is not sufficient for bursty network traffic flows, especially when the network traffic pattern may change over time. We formulate the routing control in an environment with time-variant link delays as a Lyapunov optimization problem. We identify that there is a tradeoff between optimization performance and modeling accuracy when the propagation delays are included. We propose a novel deep RL (DRL)-based adaptive network routing method to tackle the issues mentioned above. A Lyapunov optimization technique is used to reduce the upper bound of the Lyapunov drift, improving queuing stability in networked systems. By modeling the network traffic pattern using the Markovian arrival process, we show that network routing problems can be modeled as Markov decision processes and value-iteration-based RL methods can be used to solve them. We design a blockchain-based protocol using proof of elapsed time consensus mechanism to ensure a trustworthy network statistics information exchange for the routing framework. Experiment results show that the proposed method can learn a routing policy and adapt to the changing environment. The proposed method outperforms the baseline backpressure method in multiple settings and converges faster than existing methods. Moreover, the DRL module can effectively learn a better estimation of the long-term Lyapunov drift and penalty functions, providing superior results in terms of the backlog size, end-to-end latency, age of information, and throughput. Furthermore, the blockchain-based network statistics exchange can provide the routing framework against malicious nodes. In addition, the proposed model performs well under var..."}, "journals/ipm/SunCWQL21": {"title": "Pattern and content controlled response generation.", "url": "https://doi.org/10.1016/j.ipm.2021.102605", "year": "2021", "author": {"Haifeng Sun 0001": "00/11044-1", "Daixuan Cheng": "289/2865", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": "Controllable response generation is an attractive and valuable task to the success of conversational systems. However, controlling both pattern and content of the response has not been well studied in existing models since they are mainly based on matching mechanisms. To tackle the problem, we first design a pattern model to automatically learn and extract speech patterns from words. The pattern is then integrated into the encoder–decoder model to control the response pattern. Second, a sentence sampling algorithm is built to directly insert or delete words in the generated response, so that the content is controlled. In this two-stage framework, the response could be explicitly controlled by the pattern and content, without any human annotation of the post-response dataset. Experiments show the proposed framework achieves better performance in response controllability than the state-of-the-art."}, "journals/jnca/XuWQLSHL21": {"title": "Network-aware task selection to reduce multi-application makespan in cloud.", "url": "https://doi.org/10.1016/j.jnca.2020.102889", "year": "2021", "author": {"Jie Xu 0039": "37/5126-39", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Haifeng Sun 0001": "00/11044-1", "Zhu Han 0001": "83/514", "Tonghong Li": "98/4254"}, "abstract": "One new metric that plays a vital role in evaluating the cloud service is the multi-application makespan. There are usually multiple applications without a deadline in the cloud, while the makespan of each application should be minimized, such as parallel applications for training neural networks. However, the previous scheduling rule, called moving the computation tasks of applications closer to data, fails to reduce multi-application makespan. Because it is unknown which application the task belongs to, the number of optimized tasks (close-to-data tasks) is unbalanced among multiple applications, which causes a big gap between the makespan of multiple applications. To address this issue, we propose a cooperative scheduler that considers the application-task relationship, whose goal is reducing the multi-application makespan. Firstly, we propose a cost sharing game model to guide the balance of optimized task selection between multiple applications, in which the cost is referred to as the makespan. Specifically, we develop a network-aware method that can accurately estimate the makespan with a given task selection strategy. In the end, we evaluate the cooperative scheduler in the actual Hadoop cluster with diverse network environments. Experimental results demonstrate that the gap between multi-application makespan decreases. Moreover, compared to the baseline schedulers, the longest makespan decreases by 61.5%, and the network traffic is saved by more than 50%."}, "journals/jstsp/TsaiZLWQWH21": {"title": "Tensor-Based Reinforcement Learning for Network Routing.", "url": "https://doi.org/10.1109/JSTSP.2021.3055957", "year": "2021", "author": {"Kai-Chu Tsai": "289/7291", "Zirui Zhuang": "235/7014", "Ricardo Lent": "51/1891", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Li-Chun Wang 0001": "w/LiChunWang", "Zhu Han 0001": "83/514"}, "abstract": " Abstract:In the recent years, we have witnessed an explosion of networking applications due to the reasons such as the rapid development of cloud infrastructure, edge computing, and the Internet of Things. Furthermore, those applications become complex, the problem related to the large size of the state space and limited metric collection has emerged. This leads to an urging demand for adaptive management method in network routing. However, the complexity of traditional routing algorithms can be prohibited for practical systems. To overcome this challenge, we propose a novel tensor-based reinforcement learning method to route and schedule the packet flows, which is adaptive and model-free. Moreover, we improve the learning quality and efficiency by combining the Tucker decomposition technique within the learning process so that the machine learning direction can be obtained with low complexity. Finally, simulation results show that our proposed algorithm can achieve better performance under the same training episode and more stable results with less convergence time than conventional routing method, K-shortest path, traditional reinforcement learning approaches (i.e. Q-learning and SARSA) and comparable results to DQL."}, "journals/network/QiSWSGL21": {"title": "Spatial-Temporal Learning-Based Artificial Intelligence for IT Operations in the Edge Network.", "url": "https://doi.org/10.1109/MNET.011.2000278", "year": "2021", "author": {"Qi Qi 0001": "80/6406-1", "Runye Shen": "285/8020", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Song Guo 0001": "01/267-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:With the rapid increase of edge network scale and the complexity of service interaction, it takes more time for operation staff to analyze anomalies from complex scenarios. To maintain the normal network operation, various key performance indicators, such as link delay, throughput, and memory usage, are monitored for timely anomaly detection and troubleshooting. We introduce artificial intelligence for IT operations to assist operators in performing anomaly detection, anomaly localization, and root cause analysis, and building an intelligent operation and maintenance platform over the software-defined networking (SON)-based edge network. In this article, the graph-based gated convolutional network for anomaly detection (GAD) is first proposed to solve the anomaly detection problem of time series data with topology information. Specifically, GAD uses a gated convolutional encoder to encode spatial-temporal time series, and a graph convolutional network is developed to capture the spatial dependence. Then, based on the features, a convolutional layer is used to decode features and reconstruct input sequence. Finally, the residual between input and reconstructed sequences is further utilized to detect anomalies. Our experimental results demonstrate that GAD outperforms the state-of-the-art anomaly detection baselines in terms of F-scores on the datasets collected by an SDN simulation platform."}, "journals/tnsm/DongQWLSZL21": {"title": "Generative Adversarial Network-Based Transfer Reinforcement Learning for Routing With Prior Knowledge.", "url": "https://doi.org/10.1109/TNSM.2021.3077249", "year": "2021", "author": {"Tianjian Dong": "294/8527", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Alex X. Liu": "l/AlexXLiu", "Haifeng Sun 0001": "00/11044-1", "Zirui Zhuang": "235/7014", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:With the incremental deployment of software defined networking, the routing algorithms have gained more power on observability and controllability. Deep reinforcement learning, as an experience-driven approach, shows considerable potential in routing problem with the help of the centralized controller. It is an adaptive, lightweight, and model-free approach to coping with dynamic runtime status, large-scale traffic, and heterogeneous objective of SDN routing. However, it is still not suitable for the variable and complex emerging networks, because the huge training cost prevents fast convergence in a varying or discrepant environment. In this paper, we propose a transfer reinforcement learning algorithm to improve the training efficiency, and handle the variation in network status and topology. Specifically, we leverage the generative adversarial network to learn domain-invariant features that is suitable for deep reinforcement learning-based routing in different network environments. This mechanism utilizes the previous model and accelerates the training process. We implement our routing algorithm in the production level software switches and controller, while evaluating it comprehensively with many topologies and network status distributions. The experimental results show that our work not only outperforms the state-of-the-art deep reinforcement learning-based routing frameworks, but also has more training efficiency than the naive transfer learning algorithm both on different topologies and network status distributions."}, "journals/tnsm/HeWQSLDLH21": {"title": "DeepCC: Multi-Agent Deep Reinforcement Learning Congestion Control for Multi-Path TCP Based on Self-Attention.", "url": "https://doi.org/10.1109/TNSM.2021.3093302", "year": "2021", "author": {"Bo He": "04/2868", "Jing-Yu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951", "Chunning Du": "254/8020", "Xiang-Yang Alex Liu": "l/AlexXLiu", "Zhu Han 0001": "83/514"}, "abstract": " Abstract:With the development of the Internet of Things (IoT) and 5G, there are ubiquitous smart devices and network functions providing emerging network services efficiently and optimally through building many network connections based on WiFi, LTE/5G, Ethernet, and etc. The Multipath TCP (MPTCP) protocol that enables these devices to establish multiple paths for simultaneous data transmission, has been a widely used extension of standard TCP in smart devices and network functions. On the other hand, more heavy and time-varying traffic loads are generated in an MPTCP network, so that an efficient congestion control mechanism that schedules the traffic between multiple subflows and avoids congestion is highly required. In this paper, we propose a decentralized learning approach, DeepCC, to adapt to the volatile environments and realize the efficient congestion control. The Multi-Agent Deep Reinforcement Learning (MADRL) is used to learn a policy of congestion control for each subflow according to the real-time network states. To deal with the problem of the fixed state space and slow convergence, we adopt two self-attention mechanisms to receive the states and train the policy, respectively. Due to the asynchronous design of DeepCC, the learning process will not introduce extra delay and overhead on the decision-making process. Experiment results show that DeepCC consistently outperforms the well-known heuristic method and DRL-based MPTCP congestion control method in terms of goodput and jitter. Besides, DeepCC with the attention mechanism reduces convergence time by about 50% and increase goodput by about 80% compared with the commonly used structures of neural networks."}, "journals/www/LiaoLYC0W21": {"title": "An integrated model based on deep multimodal and rank learning for point-of-interest recommendation.", "url": "https://doi.org/10.1007/s11280-021-00865-8", "year": "2021", "author": {"Jianxin Liao": "60/4951", "Tongcun Liu": "219/4242", "Hongzhi Yin": "04/10606", "Tong Chen 0005": "22/1512-5", "Jingyu Wang 0001": "37/2749-1", "Yulong Wang": "97/5856"}, "abstract": "Modeling point-of-Interest (POI) for recommendations is vital in location-based social networks, yet it is a challenging task due to data sparsity and cold-start problems. Most existing approaches incorporate content features into a probabilistic matrix factorization model using unsupervised learning, which results in inaccuracy and weak robustness of recommendations when data is sparse, and the cold-start problems remain unsolved. In this paper, we propose a deep multimodal rank learning (DMRL) model that improves both the accuracy and robustness of POI recommendations. DMRL exploits temporal dynamics by allowing each user to have time-dependent preferences and captures geographical influences by introducing spatial regularization to the model. DMRL jointly learns ranking for personal preferences and supervised deep learning models to create a semantic representation of POIs from multimodal content. To make model optimization converge more rapidly while preserving high effectiveness, we develop a ranking-based dynamic sampling strategy to sample adverse or negative POIs for model training. We conduct experiments to compare our DMRL model with existing models that use different approaches using two large-scale datasets obtained from Foursquare and Yelp. The experimental results demonstrate the superiority of DMRL over the other models in creating cold-start POI recommendations and achieving excellent and highly robust results for different degrees of data sparsity."}, "journals/cii/GeWWQSL20": {"title": "Towards automatic visual inspection: A weakly supervised learning method for industrial applicable object detection.", "url": "https://doi.org/10.1016/j.compind.2020.103232", "year": "2020", "author": {"Ce Ge": "215/0142", "Jing Wang 0039": "02/736-39", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": "Industrial visual detection is an essential part in modern industry for equipment maintenance and inspection. With the recent progress of deep learning, advanced industrial object detectors are built for smart industrial applications. However, deep learning methods are known data-hungry: the processes of data collection and annotation are labor-intensive and time-consuming. It is especially impractical in industrial scenarios to collect publicly available datasets due to the inherent diversity and privacy. In this paper, we explore automation of industrial visual inspection and propose a segmentation-aggregation framework to learn object detectors from weakly annotated visual data. The used minimum annotation is only image-level category labels without bounding boxes. The method is implemented and evaluated on collected insulator images and public PASCAL VOC benchmarks to verify its effectiveness. The experiments show that our models achieve high detection accuracy and can be applied in industry to achieve automatic visual inspection with minimum annotation cost."}, "journals/csl/MaQLSW20": {"title": "Learning chinese word embeddings from character structural information.", "url": "https://doi.org/10.1016/j.csl.2019.101031", "year": "2020", "author": {"Bing Ma": "53/5636", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "Word embedding is a basic task in natural language processing area. Unlike English, Chinese subword units, such as characters, radicals, and components, contain rich semantic information which can be used to enhance word embeddings. However, existing methods neglect the semantic contribution of corresponding subword units to the word. In this work, we employ attention mechanism to capture the semantic structure of Chinese words and propose a novel framework, named Attention-based multi-Layer Word Embedding model(ALWE). We also design an asynchronous strategy for updating embedding and attention efficiently. Our model learns to share subword information between distinct words selectively and adaptively. Experimental results on the word similarity, word analogy, and text classification show that the proposed model outperforms all baselines, especially when words do not appear frequently. Qualitative analysis further demonstrates the superiority of ALWE."}, "journals/ejwcn/HanLQSW20": {"title": "Band Steering Technology Based on QoE-Oriented Optimization in Wireless Network.", "url": "https://doi.org/10.1186/s13638-020-1640-9", "year": "2020", "author": {"Zhifeng Han": "184/6776", "Jianxin Liao": "60/4951", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1"}, "abstract": ""}, "journals/ijon/LiuLWWW20": {"title": "Exploiting geographical-temporal awareness attention for next point-of-interest recommendation.", "url": "https://doi.org/10.1016/j.neucom.2019.12.122", "year": "2020", "author": {"Tongcun Liu": "219/4242", "Jianxin Liao": "60/4951", "Zhigen Wu": "242/4545", "Yulong Wang": "97/5856", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "With the prosperity of the location-based social networks, next point-of-interest (POI) recommendation has become an increasingly significant requirement since it can benefit both users and business. Obtaining insight into user mobility for the next POI recommendations is a vital yet challenging task. Existing approaches to understanding user mobility mainly gloss over the check-in sequence, making it fail to explicitly capture the subtle POI–POI interactions across the entire user check-in history and distinguish relevant check-ins from the irrelevant. In this paper, we proposed a novel recommendation approach, namely geographical-temporal awareness hierarchical attention network (GT-HAN) to resolve those issues. We first establish a geographical-temporal attention network to simultaneously uncover the overall sequence dependence and the subtle POI–POI relationships. Then, a context-specific co-attention network was designed to learn to change user preferences by adaptively selecting relevant check-in activities from check-in histories, which enabled GT-HAN to distinguish degrees of user preference for different check-ins. Finally, we make a POI recommendation using a conditional probability distribution function. Experimental results on real world datasets (obtained from Foursquare and Gowalla) show that the GT-HAN model significantly outperforms current state-of-the-art approaches, and demonstrating the benefits produced by new technologies incorporated into GT-HAN."}, "journals/iotj/SunLYQWL20": {"title": "Toward Communication-Efficient Federated Learning in the Internet of Things With Edge Computing.", "url": "https://doi.org/10.1109/JIOT.2020.2994596", "year": "2020", "author": {"Haifeng Sun 0001": "00/11044-1", "Shiqi Li": "05/3351", "F. Richard Yu": "16/6654", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Federated learning is an emerging concept that trains the machine learning models with the local distributed data sets, without sending the raw data to the data center. But, in the Internet of Things (IoT) where the wireless network resource is constrained, the key problem of federated learning is the communication overhead for parameter synchronization, which wastes bandwidth, increases training time, and even impacts the model accuracy. Gradient sparsification has received increasing attention, which only updates significant gradients and accumulates insignificant gradients locally. However, how to preserve the accuracy after a high ratio sparsification has been ignored in the literature. In this article, a general gradient sparsification (GGS) framework is proposed for adaptive optimizers, to correct the sparse gradient update process. It consists of two important mechanisms: 1) gradient correction and 2) batch normalization (BN) update with local gradients. With gradient correction, the optimizer can properly treat the accumulated insignificant gradients, which makes the model converge better. Furthermore, updating the BN layer with local gradients can relieve the impact of delayed gradients without increasing the communication overhead. We have conducted experiments on LeNet-5, CifarNet, DenseNet-121, and AlexNet with adaptive optimizers. Results show that when 99.9% gradients are sparsified, validation data sets are maintained with top-1 accuracy."}, "journals/sj/ZhuangWQSL20": {"title": "Toward Greater Intelligence in Route Planning: A Graph-Aware Deep Learning Approach.", "url": "https://doi.org/10.1109/JSYST.2019.2922217", "year": "2020", "author": {"Zirui Zhuang": "235/7014", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Software-defined networking decouples the control plane and data plane, which grants more computing power for routing computations. Traditional routing methods suffer from the complex dynamics in networking, and they are facing issues such as slow convergence and performance decline. Deep learning techniques have shown preliminary results on solving the routing problem, and bringing more accuracy, precision, and intelligence compared with traditional modeling techniques. However, the existing deep learning architectures are not built to learn from the crucial topological relations between forwarding nodes, which restricts the model's ability to handle different network conditions. In this paper, we propose a deep learning based intelligent routing strategy with revised graph-aware neural networks, which learns topological information efficiently. In addition, a set of features suitable for network routing are designed so that the networking state are well represented upon each routing decision. In experiments, the performance of the proposed work is demonstrated with a real-world topology and the production level software switches. The execution time is evaluated on various kinds of network topology and different network scales. Also, the simulation result shows that the proposed work is more accurate and efficient compared to the state-of-the-art routing strategy."}, "journals/taslp/QiWSWLL20": {"title": "A Novel Multi-Task Learning Framework for Semi-Supervised Semantic Parsing.", "url": "https://doi.org/10.1109/TASLP.2020.3018233", "year": "2020", "author": {"Qi Qi 0001": "80/6406-1", "Xiaolu Wang": "25/2654", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Xiao Liang": "06/4676", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:While sequence-to-sequence (seq2seq) models on semantic parsing have demonstrated significant performance, the need for large amounts of labeled data still hinders the application of this technology to resource-poor domains. In this work, we work on alleviating data scarcity in semantic parsing. We propose a semi-supervised semantic parsing methods by exploiting unlabeled natural utterances in a novel multi-task learning framework. Two strategies are proposed. The first one takes entity sequences as training targets to improve the representations of encoder and reduce entity-mistakes in prediction. The second one extends Mean Teacher to seq2seq model and generates more target-side data to improve the generalizability of decoder network. Experiments demonstrate that our proposed methods significantly outperform the supervised baseline and achieve more impressive improvement than previous methods."}, "journals/tmm/FengSQWL20": {"title": "Vabis: Video Adaptation Bitrate System for Time-Critical Live Streaming.", "url": "https://doi.org/10.1109/TMM.2019.2962313", "year": "2020", "author": {"Tongtong Feng": "248/6095", "Haifeng Sun 0001": "00/11044-1", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:With the rise of time-critical and interactive scenarios, ultra-low latency has become the most urgent requirement. Adaptive bitrate (ABR) schemes have been widely used in reducing latency for live streaming services. However, the traditional solutions suffer from a key limitation: they only utilize coarse-grained chunk to solve the I-frame misalignment problem in different bitrate switching process at the cost of increasing latency. As a result, existing schemes are difficult to guarantee the timeliness and granularity of control in essence. In this paper, we use a frame-based approach to solve the I-frame misalignment problem and propose a video adaptation bitrate system (Vabis) in units of the frame for time-critical live streaming to obtain the optimal quality of experience (QoE). On the server-side, a Few-Wait ABR algorithm based on Reinforcement Learning (RL) is designed to adaptively select the bitrate of future frames by state information that can be observed, which can subtly solve the problem of I-frame misalignment. A rule-based ABR algorithm is designed to optimize the Vabis system for the weak network. On the client-side, three delay control mechanisms are designed to achieve frame-based fine-grained control. We construct a trace-driven simulator and the real live platform to evaluate the comprehensive live streaming performance. The results show that Vabis is significantly better than the existing methods with decreases in an average delay of 32%-77% and improvements in average QoE of 28-67%."}, "journals/tvt/FuYWQL20": {"title": "Performance Optimization for Blockchain-Enabled Distributed Network Function Virtualization Management and Orchestration.", "url": "https://doi.org/10.1109/TVT.2020.2985581", "year": "2020", "author": {"Xiaoyuan Fu": "10/575", "F. Richard Yu": "16/6654", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:Distributed network function virtualization management and orchestration (NFV-MANO) offers a flexible way to manage and orchestrate diversified network services in large-scale Internet of vehicles (IoV). However, it is challenging to manage different services and resources in distributed NFV due to the difficulties of reliable message synchronization among multiple MANO systems. Recently, blockchain technology has emerged to solve the trust and security problems for the interconnections of multiple MANO systems. Moreover, multi-access edge computing (MEC) has become a prospective paradigm shift from the centralized cloud due to its advantages of completing tasks near users. In this work, we propose a blockchain-enabled distributed NFV framework to reach consensus among multiple MANO systems where the computation tasks of the blockchain are processed with MEC. The consensus procedures of MANO systems and blockchain nodes are explained in detail and the representation of the blockchain throughput is given. The blockchain throughput is the number of transactions a blockchain system can handle per second, which is an important evaluation indicator for the performance of a blockchain system. We make decisions for the primary node selection, the MANO system selection and the edge server selection for reaching consensus. Moreover, the blockchain throughput, the processing delay of computation tasks of blockchain and operational costs are jointly considered in the problem formulation. A dueling deep reinforcement learning approach is applied to solve this problem. Simulation results show the effectiveness of the proposed scheme."}, "journals/tvt/QiZWSZLY20": {"title": "Scalable Parallel Task Scheduling for Autonomous Driving Using Multi-Task Deep Reinforcement Learning.", "url": "https://doi.org/10.1109/TVT.2020.3029864", "year": "2020", "author": {"Qi Qi 0001": "80/6406-1", "Lingxin Zhang": "242/3729", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Zirui Zhuang": "235/7014", "Jianxin Liao": "60/4951", "F. Richard Yu": "16/6654"}, "abstract": " Abstract:The Internet of Vehicles (IoV) as a promising application of Internet of Things (IoT) has played a significant role in autonomous driving, by connecting intelligent vehicles. Autonomous driving needs to process the mass environmental sensing data in coordination with surrounding vehicles, and makes an accurate driving judgment accordingly. Since the vehicles always have limited computing resources, processing these data in parallel with efficient task scheduling is one of the most important topics. Most current work focuses on formulating special scenarios and service requirements as optimization problems. However, the complicated and dynamic environment of vehicular computing is hard to model, predict and control, making those previous methods unscalable and unable to reflect the real scenario. In this paper, a Multi-task Deep reinforcement learning approach for scalable parallel Task Scheduling (MDTS) is firstly devised. For avoiding the curse of dimensionality when coping with complex parallel computing environments and jobs with diverse properties, we extend the action selection in Deep Reinforcement Learning (DRL) to a multi-task decision, where the output branches of multi- task learning are fine-matched to parallel scheduling tasks. Child tasks of a job are accordingly assigned to distributed nodes without any human knowledge while the resource competition among parallel tasks is leveraged through shared neural network layers. Moreover, we design an appropriate reward function to optimize multiple metrics simultaneously, with emphasis on specific scenarios. Extensive experiments show that the MDTS significantly increases the overall reward compared with least- connection scheduling and particle swarm optimization algorithm from -16.71, -0.67 to 2.93, respectively."}, "journals/twc/FuYWQL20": {"title": "Dynamic Service Function Chain Embedding for NFV-Enabled IoT: A Deep Reinforcement Learning Approach.", "url": "https://doi.org/10.1109/TWC.2019.2946797", "year": "2020", "author": {"Xiaoyuan Fu": "10/575", "F. Richard Yu": "16/6654", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:The Internet of things (IoT) is becoming more and more flexible and economical with the advancement in information and communication technologies. However, IoT networks will be ultra-dense with the explosive growth of IoT devices. Network function virtualization (NFV) emerges to provide flexible network frameworks and efficient resource management for the performance of IoT networks. In NFV-enabled IoT infrastructure, service function chain (SFC) is an ordered combination of virtual network functions (VNFs) that are related to each other based on the logic of IoT applications. However, the embedding process of SFC to IoT networks is becoming a big challenge due to the dynamic nature of IoT networks and the abundance of IoT terminals. In this paper, we decompose the complex VNFs into smaller virtual network function components (VNFCs) to make more effective decisions since VNF nodes and IoT network devices are usually heterogeneous. In addition, a deep reinforcement learning (DRL) based scheme with experience replay and target network is proposed as a solution that can efficiently handle complex and dynamic SFC embedding scenarios in IoT. Our simulations consider different types of IoT network topologies. The simulation results present the efficiency of the proposed dynamic SFC embedding scheme."}, "journals/corr/abs-2007-09590": {"title": "AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation.", "url": "https://arxiv.org/abs/2007.09590", "year": "2020", "author": {"Weiting Huang": "262/0438", "Pengfei Ren": "31/10610", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1"}, "abstract": "\n      Abstract:  In this paper, we propose an adaptive weighting regression (AWR) method to\nleverage the advantages of both detection-based and regression-based methods.\nHand joint coordinates are estimated as discrete integration of all pixels in\ndense representation, guided by adaptive weight maps. This learnable\naggregation process introduces both dense and joint supervision that allows\nend-to-end training and brings adaptability to weight maps, making the network\nmore accurate and robust. Comprehensive exploration experiments are conducted\nto validate the effectiveness and generality of AWR under various experimental\nsettings, especially its usefulness for different types of dense representation\nand input modality. Our method outperforms other state-of-the-art methods on\nfour publicly available datasets, including NYU, ICVL, MSRA and HANDS 2017\ndataset.\n\n    "}, "journals/corr/abs-2011-02763": {"title": "Robust Unsupervised Video Anomaly Detection by Multi-Path Frame Prediction.", "url": "https://arxiv.org/abs/2011.02763", "year": "2020", "author": {"Xuanzhao Wang": "277/9759", "Zhengping Che": "160/5944", "Ke Yang": "80/4136", "Bo Jiang": "34/2005", "Jian Tang 0008": "181/2667-8", "Jieping Ye": "03/5454", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": "\n      Abstract:  Video anomaly detection is commonly used in many applications such as\nsecurity surveillance and is very challenging.A majority of recent video\nanomaly detection approaches utilize deep reconstruction models, but their\nperformance is often suboptimal because of insufficient reconstruction error\ndifferences between normal and abnormal video frames in practice. Meanwhile,\nframe prediction-based anomaly detection methods have shown promising\nperformance. In this paper, we propose a novel and robust unsupervised video\nanomaly detection method by frame prediction with proper design which is more\nin line with the characteristics of surveillance videos. The proposed method is\nequipped with a multi-path ConvGRU-based frame prediction network that can\nbetter handle semantically informative objects and areas of different scales\nand capture spatial-temporal dependencies in normal videos. A noise tolerance\nloss is introduced during training to mitigate the interference caused by\nbackground noise. Extensive experiments have been conducted on the CUHK Avenue,\nShanghaiTech Campus, and UCSD Pedestrian datasets, and the results show that\nour proposed method outperforms existing state-of-the-art approaches.\nRemarkably, our proposed method obtains the frame-level AUROC score of 88.3% on\nthe CUHK Avenue dataset.\n\n    "}, "journals/access/QiHWSCL19": {"title": "Personalized Sketch-Based Image Retrieval by Convolutional Neural Network and Deep Transfer Learning.", "url": "https://doi.org/10.1109/ACCESS.2019.2894351", "year": "2019", "author": {"Qi Qi 0001": "80/6406-1", "Qiming Huo": "215/0072", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1", "Yufei Cao": "92/2499", "Jianxin Liao": "60/4951"}, "abstract": "Topic: AI-Driven Big Data Processing: Theory, Methodology, and Applications"}, "journals/access/LuoLWY19": {"title": "Simplifying Flow Updates in Software-Defined Networks Using Atoman.", "url": "https://doi.org/10.1109/ACCESS.2019.2892557", "year": "2019", "author": {"Long Luo": "56/10016", "Zonghang Li": "218/5224", "Jingyu Wang 0001": "37/2749-1", "Hongfang Yu": "94/60"}, "abstract": " Abstract:Flow updates are common in today's networks, and software-defined networking (SDN) enables network operators to reconfigure switches for updating flows easily. However, the implementation of flow updates requires to meet many different expectations regarding consistency, resource constraints, and performance. To carry updates out as intended, network operators often need to spend significant effort in update management, developing complex network optimizations and customized heuristics on a case-bycase basis. In this paper, we strive to simplify the flow updates in SDN networks. To this end, we present Atoman, a framework that uses high-level abstractions to capture various update intents and formulates flow updates problems as segment-based update scheduling optimizations to obtain satisfied update solutions. The captured update intents are translated into constraints and objectives of update scheduling optimizations. By extracting critical updating flows and employing decomposition techniques, Atoman can efficiently reduce the scale of problems in each solving and generate near-optimal update solutions. We conduct extensive simulations to evaluate Atoman and the simulation results show that Atoman significantly saves operator efforts in managing flow updates and provides comparable or better efficiency than prior customized solutions."}, "journals/access/SunXWWQLL19": {"title": "Common Knowledge Based and One-Shot Learning Enabled Multi-Task Traffic Classification.", "url": "https://doi.org/10.1109/ACCESS.2019.2904039", "year": "2019", "author": {"Haifeng Sun 0001": "00/11044-1", "Yunming Xiao": "235/7025", "Jing Wang 0039": "02/736-39", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Xiulei Liu": "23/10432"}, "abstract": "Topic: AI-Driven Big Data Processing: Theory, Methodology, and Applications"}, "journals/ahswn/FuWQL19": {"title": "Incentive Mechanisms for Big Data Analytics in the Internet of Things (IoT).", "url": "https://www.oldcitypublishing.com/journals/ahswn-home/ahswn-issue-contents/ahswn-volume-45-number-1-2-2019/ahswn-45-1-p-93-115/", "year": "2019", "author": {"Xiaoyuan Fu": "10/575", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": ""}, "journals/asc/WangZQLL19": {"title": "Deep reinforcement learning-based cooperative interactions among heterogeneous vehicular networks.", "url": "https://doi.org/10.1016/j.asoc.2019.105557", "year": "2019", "author": {"Jingyu Wang 0001": "37/2749-1", "Zirui Zhuang": "235/7014", "Qi Qi 0001": "80/6406-1", "Tonghong Li": "98/4254", "Jianxin Liao": "60/4951"}, "abstract": "Most real-world vehicle nodes can be structured into an interconnected network of vehicles. Through structuring these services and vehicle device interactions into multiple types, such internet of vehicles becomes multidimensional heterogeneous overlay networks. The heterogeneousness of the overlays makes it difficult for the overlay networks to coordinate with each other to improve their performance. Therefore, it poses an interesting but critical challenge to the effective analysis of heterogeneous virtual vehicular networks. A variety of virtual vehicular networks can be easily deployed onto the native network by applying the concept of SDN (Software Defined Networking). These virtual networks reflect their heterogeneousness due to their different performance goals, and they compete for the same physical resources of the underlying network, so that a sub-optimal performance of the virtual networks may be achieved. Therefore, we propose a Deep Reinforcement Learning (DRL) approach to make the virtual networks cooperate with each other through the SDN controller. A cooperative solution based on the asymmetric Nash bargaining is proposed for co-existing virtual networks to improve their performance. Moreover, the Markov Chain model and DRL resolution are introduced to leverage the heterogeneous performance goals of virtual networks. The implementation of the approach is introduced, and simulation results confirm the performance improvement of the latency sensitive, loss-rate sensitive and throughput sensitive heterogeneous vehicular networks using our cooperative solution."}, "journals/cm/FuYWQL19": {"title": "Service Function Chain Embedding for NFV-Enabled IoT Based on Deep Reinforcement Learning.", "url": "https://doi.org/10.1109/MCOM.001.1900097", "year": "2019", "author": {"Xiaoyuan Fu": "10/575", "F. Richard Yu": "16/6654", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:It is challenging to efficiently manage different resources in the IoT. Recently, Network function virtualization has attracted attention because of its prospect to achieve efficient resource management for IoT. In NFV-enabled IoT infrastructure, a service function chain (SFC) is composed of an ordered set of virtual network functions (VNFs) that are connected based on the business logic of service providers. However, the inefficiency of the SFC embedding process is one major problem due to the dynamic nature of IoT networks and the abundance of IoT terminals. In this article, we decompose the complex VNFs into smaller VNF components (VNFCs) to make more effective decisions since VNF nodes and physical network devices are usually heterogeneous. In addition, a deep reinforcement learning (DRL)-based scheme with experience replay and target network is proposed as a solution that can efficiently handle complex and dynamic SFC embedding scenarios. Simulation results present the efficient performance of the proposed DRL-based dynamic SFC embedding scheme."}, "journals/eswa/WangJQFL19": {"title": "ALSR: An adaptive label screening and relearning approach for interval-oriented anomaly detection.", "url": "https://doi.org/10.1016/j.eswa.2019.06.028", "year": "2019", "author": {"Jingyu Wang 0001": "37/2749-1", "Yuhan Jing": "248/6262", "Qi Qi 0001": "80/6406-1", "Tongtong Feng": "248/6095", "Jianxin Liao": "60/4951"}, "abstract": "Anomaly detection using KPIs (Key Performance Indicators) is a key part of AIOps (Artificial Intelligence for IT Operations). Recent anomaly detection approaches have adopted Machine Learning to detect anomalies on the perspective of individual time points more than events. These approaches do not effectively utilize the labels of continuous anomaly intervals, nor do they pay attention to the differences among anomaly points. The detection performances are therefore not precise enough to be applied in practice, and the differences in length of anomaly intervals also cause loss of performance. In this paper, we propose an anomaly detection approach named ALSR, which uses a label screening model and a relearning model to analyze and utilize the continuous anomaly intervals of KPIs in finer granularity. The label screening model takes advantage of the continuity of anomaly intervals to remove some unnecessary data from the training set, making it more suitable for interval-oriented anomaly detection. The relearning model based on random forest reclassifies the true/false positive points within domain of detected anomalies, thus effectively reduces the number of false positive points. ALSR uses several features extracted by sliding windows, and the feature set is proved to better describe the characteristics of KPI time series. Finally, we conduct comprehensive experiments on 25 KPIs. The total F-score of ALSR is 0.965, which outperforms state-of-the-art anomaly detection approaches."}, "journals/iet-ipr/LiaoLYW0019": {"title": "Deep supervised hashing network with integrated regularisation.", "url": "https://doi.org/10.1049/iet-ipr.2018.6644", "year": "2019", "author": {"Jianxin Liao": "60/4951", "Baoran Li": "221/4399", "Di Yang": "29/5427", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jing Wang 0039": "02/736-39"}, "abstract": ""}, "journals/mta/LiuLWWQ19": {"title": "Collaborative tensor-topic factorization model for personalized activity recommendation.", "url": "https://doi.org/10.1007/s11042-018-7019-9", "year": "2019", "author": {"Tongcun Liu": "219/4242", "Jianxin Liao": "60/4951", "Yulong Wang": "97/5856", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": "Activity recommendation is a new aspect of location-based social networks (LBSNs) that is being increasingly researched in academia and industry. Previous studies focus mainly on the identification of behavioral regularity by users and use sporadic check-in data, so they suffer severely from data sparsity problems and provide inaccurate recommendations. Furthermore, tips that imply a user’s interests and the semantic data available for locations have not been extensively investigated. In this paper, we describe a collaborative tensor–topic factorization (CTTF) model that incorporates user interest topics and activity topics into a tensor factorization framework to create improved activity recommendations for users. We represent user activity feedback with a third-order tensor and penalize false preferences inferred from check-ins using term frequency–inverse document frequency. A biterm topic model was used to learn user interest topics and activity topics from location content information. We learned the latent relations between users, activities, and times by incorporating user interest topics and activity topics into a tensor factorization framework. Experimental results on real world datasets show that the CTTF model outperforms current state-of-the-art approaches."}, "journals/tvt/WangHWL19": {"title": "Intelligent VNFs Selection Based on Traffic Identification in Vehicular Cloud Networks.", "url": "https://doi.org/10.1109/TVT.2018.2880754", "year": "2019", "author": {"Jingyu Wang 0001": "37/2749-1", "Bo He": "04/2868", "Jing Wang 0039": "02/736-39", "Tonghong Li": "98/4254"}, "abstract": " Abstract:Internet of Vehicles (IoV) has become a significant research area due to its specific features and applications such as efficient traffic management, road safety, and entertainment. Vehicles are expected to carry relatively more communication systems, onboard computing facilities, storage, and increased sensing power. Vehicular Cloud Network (VCN) is a hybrid technology that has a remarkable impact on IoV by instantly using vehicular resources. Merging software-defined networking with VCN is valuable to vehicular networks. Network function virtualization can be deployed across the servers and vehicles within the VCN to provide customized agile services. Its deployment requires an appropriate management strategy that often manifests as the difficult online decision making tasks. This paper proposes an intelligent Virtualized Network Functions (VNFs) selection strategy. For satisfying the quality requirements of different vehicular services, this strategy is based on the results of traffic identification that utilizes deep neural network and Multi-Grained Cascade Forest to distinguish service behaviors. According to the order of VNFs, the vehicular network is divided into several layers where each arrived packet needs to be queued. The scheduler of each layer selects a layer to host the next VNF for the packets in the queue. Our experiments demonstrate that the proposed traffic identification method increases the precision by 7.7% and improves the real-time performance. Furthermore, the proposed model of VNFs selection reduces network congestion compared to traditional scheduling models."}, "journals/tvt/QiWMSCZL19": {"title": "Knowledge-Driven Service Offloading Decision for Vehicular Edge Computing: A Deep Reinforcement Learning Approach.", "url": "https://doi.org/10.1109/TVT.2019.2894437", "year": "2019", "author": {"Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Zhanyu Ma": "56/8107", "Haifeng Sun 0001": "00/11044-1", "Yufei Cao": "92/2499", "Lingxin Zhang": "242/3729", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:The smart vehicles construct Internet of Vehicle (IoV), which can execute various intelligent services. Although the computation capability of a vehicle is limited, multi-type of edge computing nodes provide heterogeneous resources for intelligent vehicular services. When offloading the complex service to the vehicular edge computing node, the decision for its destination should be considered according to numerous factors. This paper mostly formulate the offloading decision as a resource scheduling problem with single or multiple objective function and constraints, where some customized heuristics algorithms are explored. However, offloading multiple data dependence tasks in a complex service is a difficult decision, as an optimal solution must understand the resource requirement, the access network, the user mobility, and importantly the data dependence. Inspired by recent advances in machine learning, we propose a knowledge driven (KD) service offloading decision framework for IoV, which provides the optimal policy directly from the environment. We formulate the offloading decision for the multiple tasks as a long-term planning problem, and explore the recent deep reinforcement learning to obtain the optimal solution. It can scruple the future data dependence of the following tasks when making decision for a current task from the learned offloading knowledge. Moreover, the framework supports the pre-training at the powerful edge computing node and continually online learning when the vehicular service is executed, so that it can adapt the environment changes and can learn policy that are sensible in foresight. The simulation results show that KD service offloading decision converges quickly, adapts to different conditions, and outperforms a greedy offloading decision algorithm."}, "journals/wicomm/HanLQSW19": {"title": "Radio Environment Map Construction by Kriging Algorithm Based on Mobile Crowd Sensing.", "url": "https://doi.org/10.1155/2019/4064201", "year": "2019", "author": {"Zhifeng Han": "184/6776", "Jianxin Liao": "60/4951", "Qi Qi 0001": "80/6406-1", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1"}, "abstract": ""}, "journals/corr/abs-1905-11664": {"title": "OICSR: Out-In-Channel Sparsity Regularization for Compact Deep Neural Networks.", "url": "http://arxiv.org/abs/1905.11664", "year": "2019", "author": {"Jiashi Li": "241/9364", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Ce Ge": "215/0142", "Yujian Li": "83/5205", "Zhangzhang Yue": "241/9492", "Haifeng Sun 0001": "00/11044-1"}, "abstract": "\n      Abstract:  Channel pruning can significantly accelerate and compress deep neural\nnetworks. Many channel pruning works utilize structured sparsity regularization\nto zero out all the weights in some channels and automatically obtain\nstructure-sparse network in training stage. However, these methods apply\nstructured sparsity regularization on each layer separately where the\ncorrelations between consecutive layers are omitted. In this paper, we first\ncombine one out-channel in current layer and the corresponding in-channel in\nnext layer as a regularization group, namely out-in-channel. Our proposed\nOut-In-Channel Sparsity Regularization (OICSR) considers correlations between\nsuccessive layers to further retain predictive power of the compact network.\nTraining with OICSR thoroughly transfers discriminative features into a\nfraction of out-in-channels. Correspondingly, OICSR measures channel importance\nbased on statistics computed from two consecutive layers, not individual layer.\nFinally, a global greedy pruning algorithm is designed to remove redundant\nout-in-channels in an iterative way. Our method is comprehensively evaluated\nwith various CNN architectures including CifarNet, AlexNet, ResNet, DenseNet\nand PreActSeNet on CIFAR-10, CIFAR-100 and ImageNet-1K datasets. Notably, on\nImageNet-1K, we reduce 37.2% FLOPs on ResNet-50 while outperforming the\noriginal model by 0.22% top-1 accuracy.\n\n    "}, "journals/access/WangZQHZGL18": {"title": "MindCamera: Interactive Sketch-Based Image Retrieval and Synthesis.", "url": "https://doi.org/10.1109/ACCESS.2018.2796638", "year": "2018", "author": {"Jingyu Wang 0001": "37/2749-1", "Yu Zhao 0006": "57/2056-6", "Qi Qi 0001": "80/6406-1", "Qiming Huo": "215/0072", "Jian Zou": "60/2647", "Ce Ge": "215/0142", "Jianxin Liao": "60/4951"}, "abstract": "Topic: Recent Advantages of Computer Vision based on Chinese Conference on Computer Vision (CCCV) 2017"}, "journals/access/LiaoLLWWS18": {"title": "Multi-Context Integrated Deep Neural Network Model for Next Location Prediction.", "url": "https://doi.org/10.1109/ACCESS.2018.2827422", "year": "2018", "author": {"Jianxin Liao": "60/4951", "Tongcun Liu": "219/4242", "Meilian Liu": "03/6680", "Jingyu Wang 0001": "37/2749-1", "Yulong Wang": "97/5856", "Haifeng Sun 0001": "00/11044-1"}, "abstract": " Abstract:The prediction of next location for users in location-based social networks has become an increasing significant requirement since it can benefit both users and business. However, existing methods lack an integrated analysis of sequence context, input contexts, and user preferences in a unified way, and result in an unsatisfactory prediction. Moreover, the interaction between different kinds of input contexts has not been investigated. In this paper, we propose a multi-context integrated deep neural network model (MCI-DNN) to improve the accuracy of the next location prediction. In this model, we integrate sequence context, input contexts, and user preferences into a cohesive framework. First, we model sequence context and interaction of different kinds of input contexts jointly by extending the recurrent neural network to capture the semantic pattern of user behaviors from check-in records. After that, we design a feedforward neural network to capture high-level user preferences from check-in data and incorporate that into MCI-DNN. To deal with different kinds of input contexts in the form of multi-field categorical, we adopt embedding representation technology to automatically learn dense feature representations of input contexts. Experimental results on two typical real-world data sets show that the proposed model outperforms the current state-of-the-art approaches by about 57.12% for Foursquare and 76.4% for Gowalla on average regarding F1-score@5."}, "journals/grid/FuWQLL18": {"title": "Incentive Mechanisms for Resource Scaling-out Game of Stream Big Data Analytics.", "url": "https://doi.org/10.1007/s10723-018-9458-y", "year": "2018", "author": {"Xiaoyuan Fu": "10/575", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254"}, "abstract": "For stream big data analytics, a participated task always needs to scale out resources when its input data increases steeply. Typically, the resource scaling-out can be achieved by increasing the parallelism degree of the platform based on the experience. However, the resource scaling-out of each task produces additional cost not only from itself but also from other competitive tasks, which brings about great challenges to ensure the efficient utilization of resources. To solve it systematically, we consider the resource scaling-out as a non-cooperative game and formulate a total cost model including a risk function and a task execution time function. The total cost of resource scaling-out reflects the influence of topology structure for the benefit of a participated task. Then we introduce the concept of price of anarchy (POA) to this game and get its upper bounds under specific conditions to describe the efficiency loss of Nash equilibrium. Hence, two economic classic tax-based incentive policies: Pivotal Mechanism and Externality Mechanism are applied, to stimulate the participation of tasks. We make simulations in different scenarios including node degree and different characteristics of tasks. The simulations results show the influence of the topological structure and interdependent relationships of tasks for resource scaling-out game in the proposed scenarios and that the incentive mechanisms can effectively improve the performance of resource scaling-out."}, "journals/ijon/SunLWQ18": {"title": "MTDE: Multi-typed data embedding in heterogeneous networks.", "url": "https://doi.org/10.1016/j.neucom.2017.06.079", "year": "2018", "author": {"Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": "Vectorized representations as an important data representation way play an essential role in many data mining applications. Now, more and more applications are based on multi-typed information network, such as social networks, which is called heterogeneous networks. However, most data in heterogeneous networks are far from Gaussian distribution. Gaussian models are inappropriate choices to model such data. On the other hand, most traditional embedding methods are based on single typed data, and cannot be directly applied in data with network structures. In this paper, we propose an embedding method, named as Multi-typed Data Embedding (MTDE), vectorized represents the data in non-Gaussian distribution. It achieves Latent Spaces for every typed data and a multi-typed latent translational space by a probabilistic model based on Gibbs sampling method. First, it embeds the objects in network not only considering the relationships in same typed data, but also the network structure. Second, it provides a translational space to make the comparison of different typed data available. Thus, we can utilize MTDE to compare different typed data in more data mining applications. Our experiments on DBLP show that MTDE learns high-quality embedding. Moreover, other data mining tasks, e.g. Clustering, based on MTDE achieve a better performance than the state-of-the-art methods."}, "journals/ijswis/LiaolWQL18": {"title": "Rapid Relevance Feedback Strategy Based on Distributed CBIR System.", "url": "https://doi.org/10.4018/IJSWIS.2018040101", "year": "2018", "author": {"Jianxin Liao": "60/4951", "Baoran Li": "221/4399", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Tonghong Li": "98/4254"}, "abstract": ""}, "journals/jnsm/FengLQLW18": {"title": "COVE: Co-operative Virtual Network Embedding for Network Virtualization.", "url": "https://doi.org/10.1007/s10922-017-9408-1", "year": "2018", "author": {"Min Feng 0003": "04/3799-3", "Jianxin Liao": "60/4951", "Sude Qing": "122/5072", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "Network virtualization provides a promising solution for next-generation network management by allowing multiple isolated and heterogeneous virtual networks to coexist and run on a shared substrate network. A long-standing challenge in network virtualization is how to effectively and efficiently map these virtual nodes and links of heterogeneous virtual networks onto specific nodes and links of the shared substrate network, known as the Virtual Network Embedding (VNE) problem. Existing centralized VNE algorithms and distributed VNE algorithms both have advantages and disadvantages. In this paper, a novel cooperative VNE algorithm is proposed to coordinate centralized and distributed algorithms and unite their respective advantages and specialties. By leveraging the learning technology and topology decomposition, autonomous substrate nodes entrusted with detailed mapping solutions cooperate closely with the central controller with a global view and in charge of general management to achieve a successful embedding process. Besides a topology-aware resource evaluation mechanism and customized mapping management policies, Bloom filter is elaborately introduced to synchronize the mapping information within the substrate network, instead of flooding which generates massive communication overhead. Extensive simulations demonstrate that the proposed cooperative algorithm has acceptable and even better performance in terms of long-term average revenue and acceptance ratio than previous algorithms."}, "journals/sj/Wang0QL18": {"title": "Elastic Vehicular Resource Providing Based on Service Function-Group Resource Mapping of Smart Identify Network.", "url": "https://doi.org/10.1109/JSYST.2017.2771443", "year": "2018", "author": {"Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Sude Qing": "122/5072", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:In the era of smart cities, all vehicle systems will be connected to enhance the comfort of driving, relieve traffic congestion, and enjoy in-vehicle multimedia entertainment. The vision of all vehicles connected poses a crucial challenge for an individual vehicle system to efficiently support these applications. Network virtualization is a very promising enabling solution, by allowing multiple isolated and heterogeneous virtual networks (VNs), to satisfy the different quality of service (QoS) requirements. Smart identifier network (SINET) may provide VNs through effective resources allocation and control based on its model of three layers and two domains. In this paper, we provide resource allocation and mapping of the vehicular networks through elastic network virtualization based on SINET. The appropriate vehicles are selected and generated as a function group for special service, by which the difference of heterogeneous vehicular resources is hided. Vehicular nodes are autonomously organized that each of them can evaluate others’ resource availability in a topology-aware way with information by leveraging the learning technology and make its own decision to realize the whole mapping process through a phasing virtual network embedding (PVNE) algorithm. The results demonstrate that our proposed mechanism has better performance in long-term acceptance ratio and average revenue than existing state-of-the-art solutions."}, "journals/sj/WangQGL18": {"title": "Mitigating the Oscillations Between Service Routing and SDN Traffic Engineering.", "url": "https://doi.org/10.1109/JSYST.2018.2805898", "year": "2018", "author": {"Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Jun Gong": "62/28", "Jianxin Liao": "60/4951"}, "abstract": " Abstract:A wide variety of existing applications will be deployed in software-defined networking (SDN). The centralized controller of SDN has a global network view and determines the forwarding path for each flow, which provides a feasible solution to deploy traffic engineering (SDN-TE). However, the conflict between each service and SDN-TE will occur inevitably due to the divergence of their objectives, and the interaction between these two selfish players will cause the network instability in fact. In order to solve this problem, we model the interaction as an alternative-offers bargaining game. In our strategies, each player sacrifices its performance to weaken the inevitable conflict and mitigate the oscillations. An exploratory study is adopt to investigate the interaction process based on the alternative-offers bargaining approach of the game theory, including the knowledge from the previous round and their strategies in each offer. This alternative-offers bargaining can make two players reach an agreement with a few rounds. Moreover, service routing and SDNTE can achieve the optimal outcome values at a stable state so that the network stability is improved. The simulation results show that the network stability is improved, and both SDN-TE and service routing converge to a satisfactory state."}, "journals/wpc/QiWCWSL18": {"title": "Cluster-PSO Based Resource Orchestration for Multi-task Applications in Vehicular Cloud.", "url": "https://doi.org/10.1007/s11277-017-5046-x", "year": "2018", "author": {"Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Yufei Cao": "92/2499", "Jing Wang 0039": "02/736-39", "Haifeng Sun 0001": "00/11044-1", "Jianxin Liao": "60/4951"}, "abstract": "Vehicular cloud computing (VCC) provides a vehicular user attaching several resources with different types at the same time. Additionally, the vehicular applications especially for big data processing are always complicated and may be decomposed into several fine-grained tasks. When offloading the complicated multi-task application to the vehicular clouds, the task executes individually in terms of its own computation, storage and bandwidth requirement. Different from the task offloading in mobile cloud computing which aims to optimize the energy consumption, the important metric for vehicular users is the application delay. Moreover, the moving vehicles always have the similar resource properties and may form the solution clusters when finding the resource orchestration policy, which brings an opportunity of improving resource orchestration performance. In this paper, we formulate the VCC resource orchestration as an optimization problem, and propose a cluster-particle swarm optimization (PSO) algorithm to obtain the resource orchestration policy. A fast cluster algorithm is used to divide the solution space and generate sub-swarms for better exploring the orchestration solutions. The experiment results show that the cluster-PSO algorithm can achieve a higher resource orchestration accuracy in an acceptable time comparing to the other PSO algorithms. Especially, when there are more tasks in an application and the vehicle has more optional VCC resources, the performance of the cluster-PSO based resource orchestration is outstanding."}, "journals/cn/LiaoSWQLL17": {"title": "Density cluster based approach for controller placement problem in large-scale software defined networkings.", "url": "https://doi.org/10.1016/j.comnet.2016.10.014", "year": "2017", "author": {"Jianxin Liao": "60/4951", "Haifeng Sun 0001": "00/11044-1", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Kai Li 0003": "l/KaiLi3", "Tonghong Li": "98/4254"}, "abstract": "Software Defined Networking (SDN) decouples control and data planes. The separation arises a problem known as the controller placement, i.e., how many and where controllers should be deployed. Currently, most works defined this problem as the multi-objective combinatorial optimization problem and used heuristic algorithms to search the optimal solution. However, these heuristic algorithms have the drawback of being easily trapped in local optimal solutions and consuming high time. In this paper, we propose an approach named as Density Based Controller Placement (DBCP), which uses a density-based switch clustering algorithm to split the network into several sub-networks. As switches are tightly connected within the same sub-network and less connected from the switches in other sub-networks, we deploy one controller in each sub-network. In DBCP, the size of each sub-network can be decided by the capacity of the controller deployed. Moreover, the optimal number of controllers is obtained according to the density-based clustering. We evaluate DBCP’s performance on a set of 262 publicly available network topologies. The experimental results show that DBCP provides better performance than the state-of-the-art approaches in terms of time consumption, propagation latency, and fault tolerance."}, "journals/itiis/TianLLWC17": {"title": "Resilient Routing Overlay Network Construction with Super-Relay Nodes.", "url": "https://doi.org/10.3837/tiis.2017.04.005", "year": "2017", "author": {"Shengwen Tian": "42/2738", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1", "Guanghai Cui": "119/3626"}, "abstract": ""}, "journals/winet/LiaoZLWQ17": {"title": "A generalized design of distributed rateless codes with decreasing ripple size for multiple-access relay networks.", "url": "https://doi.org/10.1007/s11276-016-1279-y", "year": "2017", "author": {"Jianxin Liao": "60/4951", "Lei Zhang 0094": "97/8704-94", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": "In this correspondence, the problem of existing distributed rateless codes (DRC) with inaccurate degree distributions in the sources and the relay is addressed. Based on a well-known multiple-access relay network model, a generalized design of DRC (GDRC) is proposed by using optimization theory and heuristic Jacobian iterative algorithm, in which the interaction between the degree distributions in the sources and in the relay is considered. Our GDRC can be applied to Luby transfer codes or any other rateless codes that can be described with a degree distribution. Furthermore, a novel approach of designing GDRC with decreasing ripple size is proposed by directly analysing the interaction between the ripple size revolutions in the sources and in the relay. Our proposed scheme is evaluated and compared with existing schemes. It is shown that our proposed scheme exhibits reduced overhead, memory usage, bit error rate and energy consumption compared to the existing schemes."}, "journals/iee/LiaoLWWQ16": {"title": "Lightweight approach for multi-objective web service composition.", "url": "https://doi.org/10.1049/iet-sen.2014.0155", "year": "2016", "author": {"Jianxin Liao": "60/4951", "Yang Liu": "51/3710", "Jing Wang 0039": "02/736-39", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": ""}, "journals/iee/LiaoZLWQ16": {"title": "Efficient and fair scheduler of multiple resources for MapReduce system.", "url": "https://doi.org/10.1049/iet-sen.2015.0109", "year": "2016", "author": {"Jianxin Liao": "60/4951", "Lei Zhang 0094": "97/8704-94", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": ""}, "journals/iet-com/LiaoZLWQ16": {"title": "Design of optimised multiple partial recovery LT codes.", "url": "https://doi.org/10.1049/iet-com.2015.0539", "year": "2016", "author": {"Jianxin Liao": "60/4951", "Lei Zhang 0094": "97/8704-94", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": ""}, "journals/itiis/LiaoGJLW16": {"title": "Conflicts in Overlay Environments: Inefficient Equilibrium and Incentive Mechanism.", "url": "https://doi.org/10.3837/tiis.2016.05.018", "year": "2016", "author": {"Jianxin Liao": "60/4951", "Jun Gong": "62/28", "Shan Jiang 0008": "04/2910-8", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1"}, "abstract": ""}, "journals/jnsm/LiaoFQLW16": {"title": "LIVE: Learning and Inference for Virtual Network Embedding.", "url": "https://doi.org/10.1007/s10922-015-9349-5", "year": "2016", "author": {"Jianxin Liao": "60/4951", "Min Feng 0003": "04/3799-3", "Sude Qing": "122/5072", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "\nNetwork virtualization provides a promising tool for next-generation network management by allowing multiple heterogeneous virtual networks to run on a shared substrate network. A long-standing challenge in network virtualization is how to effectively map these virtual networks onto the shared substrate network, known as the virtual network embedding (VNE) problem. Most heuristic VNE algorithms find practical solutions by leveraging a greedy matching strategy in node mapping. However, greedy node mapping may lead to unnecessary bandwidth consumption and increased network fragmentation because it ignores the relationships between the mapped virtual network requests and the mapping ones. In this paper, we re-visit the VNE problem from a statistical perspective and explore the potential dependencies between every two substrate nodes. We define a well-designed dependency matrix that represents the importance of substrate nodes and the topological relationships between them, i.e., every substrate node’s degree of belief. Based on the dependency matrix generated from collecting and processing records of accepted virtual network requests, Bayesian inference is leveraged to iteratively select the most suitable substrate nodes and realize our novel statistical VNE algorithm consisting of a learning stage and an inference stage in node mapping. Due to the overall consideration of the relationships between the mapped nodes and the mapping ones, our statistical approach reduces unnecessary bandwidth consumption and achieves a better performance of embedding. Extensive simulations demonstrate that our algorithm significantly improves the long-term average revenue, acceptance ratio, and revenue/cost ratio compared to previous algorithms."}, "journals/mis/QiLWLC16": {"title": "Software Defined Resource Orchestration System for Multitask Application in Heterogeneous Mobile Cloud Computing.", "url": "https://doi.org/10.1155/2016/2784548", "year": "2016", "author": {"Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Qi Li": "181/2688", "Yufei Cao": "92/2499"}, "abstract": ""}, "journals/mta/LiaoQWWC16": {"title": "A dual mode self-adaption handoff for multimedia services in mobile cloud computing environment.", "url": "https://doi.org/10.1007/s11042-015-2498-4", "year": "2016", "author": {"Jianxin Liao": "60/4951", "Qi Qi 0001": "80/6406-1", "Jing Wang 0039": "02/736-39", "Jingyu Wang 0001": "37/2749-1", "Yufei Cao": "92/2499"}, "abstract": "Since mobile devices are becoming the primary platforms for many users who always roam around and access the cloud computing applications, the two concepts of mobile computing and cloud computing have emerged as a new widely accepted paradigm, mobile cloud computing. More and more users use cloud computing service and offload their local applications to the cloud. Unfortunately, developing mobile multimedia cloud services over heterogeneous wireless networks poses a challenge for service continuity. The degraded link quality and connection losses are likely to happen and these may affect service availability and service usage times in mobile cloud computing scenarios. To improve handoff quality and minimize utilized bandwidth, we propose a dual mode self-adaption handoff mechanism for multimedia services in mobile cloud computing environment. The new mechanism uses multipath transmission for media flows based on “make before break” technology, and consists of the duplicate mode and the effective mode, which are changed according to the network condition. Analytic model and simulation are developed to investigate our new mechanism. The results demonstrate that the self-adaption handoff mechanism can realize seamless handoff for multimedia services in cloud, reduce the packet loss rate, as well as obtain a more efficient use of the scarce wireless bandwidth and the power of mobile devices."}, "journals/mta/LiaoYLQWS16": {"title": "Fusion feature for LSH-based image retrieval in a cloud datacenter.", "url": "https://doi.org/10.1007/s11042-015-2892-y", "year": "2016", "author": {"Jianxin Liao": "60/4951", "Di Yang": "29/5427", "Tonghong Li": "98/4254", "Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Haifeng Sun 0001": "00/11044-1"}, "abstract": "Since the emergence of cloud datacenters provides an enormous amount of resources easily accessible to people, it is challenging to provide an efficient search framework in such a distributed environment. However, traditional search techniques only allow users to search images over exact-match keywords through a centralized index. These methods are insufficient to meet requirements of content based image retrieval (CBIR) and more powerful search frameworks are needed. In this paper, we present LFFIR, a multi-feature image retrieval framework for content similar search in the distributed situation. The key idea is to effectively incorporate image retrieval based on multi-feature into the peer-to-peer (P2P) paradigm. LFFIR fuses the multiple features in order to capture the overall image characteristics. And then it constructs the distributed indexes for the fusion feature through exploiting the property of locality sensitive hashing (LSH). We implement a prototype system to evaluate the system performance with two image datasets. Comprehensive performance evaluations demonstration that our approach brings major performance and accuracy gains compared to the advanced distributed image retrieval framework."}, "journals/percom/WangLLW16": {"title": "Game-theoretic model of asymmetrical multipath selection in pervasive computing environment.", "url": "https://doi.org/10.1016/j.pmcj.2015.10.012", "year": "2016", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254", "Jing Wang 0039": "02/736-39"}, "abstract": "In order to transfer the ever increasing multimedia data we need to make use of multiple paths to realize the bandwidth aggregating. In pervasive computing environment, the combination of ubiquitous overlay networks and the capacity of multihoming can provide the possibility of exploiting plentiful available paths. Thus, we introduce a Pervasive Multipath Architecture (PEMA), in which we use the game theory to investigate the selfish strategic collaboration of multiple heterogeneous overlays when they are allowed to use the massively-multipath transfer. Overlay networks are modeled as “players” in this multipath selection game, and we study the asymmetric case where all overlays have different Round Trip Times (RTT) and different degrees of waste. We demonstrate the existence and uniqueness of Nash equilibrium (NE). In addition, we find that overlays differing only in their RTTs still receive proportional throughput shares and utilities at the NE. However, if overlays differ only in their degrees of waste, a more wasteful overlay has a larger utility and a larger throughput (bandwidth) share than a less wasteful overlay. Since maintaining connected paths consumes resource, we further consider a more generalized cost function where an overlay’s total resource consumption includes both transferring data packets and maintaining path connections. In this general game, we find that the overlay is more conservative, which opens smaller number of paths and obtains smaller efficiency loss at the NE than in other simplified games. Our simulations confirm the effectiveness and friendliness of multipath transfer for a range of path numbers and in the presence of multi-overlay traffic."}, "journals/tce/QiWLLC16": {"title": "Resource orchestration for multi-task application in home-to-home cloud.", "url": "https://doi.org/10.1109/TCE.2016.7514719", "year": "2016", "author": {"Qi Qi 0001": "80/6406-1", "Jingyu Wang 0001": "37/2749-1", "Qi Li": "181/2688", "Tonghong Li": "98/4254", "Yufei Cao": "92/2499"}, "abstract": " Abstract:Along with the increasing capability of consumer electronic devices, it becomes the new trend that the home cloud computing takes household consumer electronic devices as a sharing resource pool, which can bring great convenience to consumers. However, how to find the most suitable resources or services in the home cloud is a challenge for the consumers. When offloading a complicated application to the home cloud, each task of the application can be executed individually with its own computation, storage and bandwidth requirements. The heterogeneous consumer electronic devices that contain different performance metrics affect the destination choice. This paper proposes a distributed home-to-home cloud infrastructure where volunteer users allow the resources of their consumer electronic devices to be shared with the neighboring houses. By extending the home gateways, a decentralized controlling is responsible for resource monitoring, orchestration, task offload and migration. With consideration of load balance and volunteers' opinion, the resource orchestration is formulated as an optimization problem with one objective and multi-constraints. Finally, a Particle Swarm Optimization (PSO)-based algorithm is used to obtain the approximate optimal solution. Simulation results show that the solution for all cases studied almost achieves 90% objective value in acceptable time\n1\n."}, "journals/tsc/LiuCLBWW16": {"title": "OMI-DL: An Ontology Matching Framework.", "url": "https://doi.org/10.1109/TSC.2015.2410794", "year": "2016", "author": {"Xiulei Liu": "23/10432", "Bo Cheng": "05/2700", "Jianxin Liao": "60/4951", "Payam M. Barnaghi": "22/4255", "Li Wan": "35/5678", "Jingyu Wang 0001": "37/2749-1"}, "abstract": " Abstract:This paper focuses on matching ontologies created for similar domains through different sources. Different solutions use lexical, structural or logical processing and analysis to match ontologies. However, an important aspect is also interpreting concepts that entities are presented with and using them in relation to semantics in an ontology. The paper demonstrates analyzing and extending the concepts used to define entities in an ontology, discusses establishing and filtering matching candidates by reasoners, and then describes constructing correspondences between entities from different ontologies using lexical and semantic analysis. The experiments show that our prototype, called OMI-DL, is among the top group over many ontologies adopted from the OAEI benchmark data set. We also provide an evaluation of the OMI-DL method for matching the DOLCE+DnS Ultralite ontology and the domain ontology developed in the m:Ciudad project."}, "journals/wpc/QiLWWLC16": {"title": "Integrated Multi-service Handoff Mechanism with QoS-Support Strategy in Mobile Cloud Computing.", "url": "https://doi.org/10.1007/s11277-016-3210-3", "year": "2016", "author": {"Qi Qi 0001": "80/6406-1", "Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Jing Wang 0039": "02/736-39", "Qi Li": "181/2688", "Yufei Cao": "92/2499"}, "abstract": "Mobile devices are becoming the primary platforms for many users who always roam around and access the cloud computing applications. Using cloud computing services extensively and offloading local applications to the cloud, facilitate mobile devices keep multiple services online at the same time. Since the mobile device may move fast and session handoff among access networks comes up frequently, multiple services initiate handoff procedure results in lots of redundant signaling. The current handoff mechanisms only make the handoff control for each service individually, which brings much unnecessary energy consumption and ignores the different Quality of Service (QoS) requirement during handoff execution. To solve these issues, we propose an improved multi-services handoff mechanism, which uses the list method of session initial protocol to make all active services execute handoff together. And considering to support the QoS for each active service during handoff procedure, the differentiate policy for multi-service media transfer is included based on the integrated handoff signaling. Analytic model and simulation are developed to investigate the new mechanism. The results demonstrate that the improved multi-service handoff mechanism can efficiently save energy consumption for mobile devices and realize seamless handoff."}, "journals/cn/LiaoCWLQW15": {"title": "A coalitional game approach on improving interactions in multiple overlay environments.", "url": "https://doi.org/10.1016/j.comnet.2015.05.006", "year": "2015", "author": {"Jianxin Liao": "60/4951", "Ziteng Cui": "150/5743", "Jingyu Wang 0001": "37/2749-1", "Tonghong Li": "98/4254", "Qi Qi 0001": "80/6406-1", "Jing Wang 0039": "02/736-39"}, "abstract": "The overlay network has been widely deployed by Service Providers (SPs) to provide services. Since there are multiple SPs built upon the same Internet Service Provider (ISP), their overlays are co-existing and may interfere with each other. The selfishness of overlay may lead to sub-optimal performance and traffic arrangement dilemma for overlays. To optimize the performance of overlays and maximize the benefit of SPs, we propose a cooperative overlay optimization approach, in which overlays can form coalitions freely for the purpose of cooperation. This approach performs a two-step process: the coalition optimization process and the coalition formation process. Several models are applied to describe these two processes. The overlay routing problem of a coalition, the revenue allocation problem, and the convergence problem of coalition formation are analyzed and solved. In the coalition formation process, the relationship between co-existing overlays is also analyzed. Simulations are performed to evaluate our approach, which is proved to be effective on improving the performance and balancing the fairness of co-existing overlays."}, "journals/ijcomsys/WangXWZS15": {"title": "Paths selection-based resequencing queue length in concurrent multipath transfer.", "url": "https://doi.org/10.1002/dac.2792", "year": "2015", "author": {"Fenghua Wang": "23/5743", "Dongliang Xie": "27/3906", "Jingyu Wang 0001": "37/2749-1", "Peng Zhang": "21/1048", "Yan Shi 0002": "67/2601-2"}, "abstract": ""}, "journals/ppna/WangLLW15": {"title": "On the collaborations of multiple selfish overlays using multi-path resources.", "url": "https://doi.org/10.1007/s12083-013-0245-z", "year": "2015", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254", "Jing Wang 0039": "02/736-39"}, "abstract": "Overlay networks provide the possibility of taking advantage of multiple available paths to realize the bandwidth aggregating through multipath transfer. We present a game-theoretic study of the selfish strategic collaboration of multiple overlays when they are allowed to use multipath transfer, which is referred as the multipath selection game. Then we consider the equilibrium in this multipath selection game model where selfish players distribute their overlay traffic. Maximization of the utility functions for each overlay is the criterion of optimality. We adopt the objective of throughput maximization to capture the most typical overlay behaviors, and use the usual TCP as the basis of our analysis. We show analytically the existence and uniqueness of Nash equilibria in these games. Furthermore, we find that the loss of efficiency of Nash equilibria can be arbitrarily large if overlays do not have resource limitations."}, "journals/wpc/WangLLW15": {"title": "Asymmetrical Multi-path Selection Game for Wireless Overlay Networks.", "url": "https://doi.org/10.1007/s11277-015-2912-2", "year": "2015", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254", "Jing Wang 0039": "02/736-39"}, "abstract": "In order to transfer the increasing big data we need to make use of multiple wireless paths. Overlay networks provide the possibility of taking advantage of multiple available routing paths to realize the bandwidth aggregating. We present a game-theoretic study of the selfish strategic collaboration of multiple heterogeneous overlays when they are allowed to use massively-multipath transfer. Overlays are modeled as players in this multipath selection game model, we discuss the asymmetric case where all overlays have the different round trip times (RTT) and different wastefulness level, and demonstrate the existence and uniqueness of Nash equilibrium (NE). Then we find overlays differing only in their RTTs still receive equal throughput shares and utilities at the NE. However, if overlays differ only in their wastefulness levels, a more wasteful overlay has a larger utility and a larger throughput (bandwidth) share than a less wasteful overlay."}, "journals/isf/LiaoYLWQZ14": {"title": "A scalable approach for content based image retrieval in cloud datacenter.", "url": "https://doi.org/10.1007/s10796-013-9467-0", "year": "2014", "author": {"Jianxin Liao": "60/4951", "Di Yang": "29/5427", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": "The emergence of cloud datacenters enhances the capability of online data storage. Since massive data is stored in datacenters, it is necessary to effectively locate and access interest data in such a distributed system. However, traditional search techniques only allow users to search images over exact-match keywords through a centralized index. These techniques cannot satisfy the requirements of content based image retrieval (CBIR). In this paper, we propose a scalable image retrieval framework which can efficiently support content similarity search and semantic search in the distributed environment. Its key idea is to integrate image feature vectors into distributed hash tables (DHTs) by exploiting the property of locality sensitive hashing (LSH). Thus, images with similar content are most likely gathered into the same node without the knowledge of any global information. For searching semantically close images, the relevance feedback is adopted in our system to overcome the gap between low-level features and high-level features. We show that our approach yields high recall rate with good load balance and only requires a few number of hops."}, "journals/itiis/LiaoFLWQ14": {"title": "Topology-aware Virtual Network Embedding Using Multiple Characteristics.", "url": "https://doi.org/10.3837/tiis.2014.01.009", "year": "2014", "author": {"Jianxin Liao": "60/4951", "Min Feng 0003": "04/3799-3", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1", "Sude Qing": "122/5072"}, "abstract": ""}, "journals/itiis/LiaoTWLQ14": {"title": "Load-Balanced One-hop Overlay Multipath Routing with Path Diversity.", "url": "https://doi.org/10.3837/tiis.2014.02.007", "year": "2014", "author": {"Jianxin Liao": "60/4951", "Shengwen Tian": "42/2738", "Jingyu Wang 0001": "37/2749-1", "Tonghong Li": "98/4254", "Qi Qi 0001": "80/6406-1"}, "abstract": ""}, "journals/itiis/WangLWLQ14": {"title": "Estimating the Effects of Multipath Selection on Concurrent Multipath Transfer.", "url": "https://doi.org/10.3837/tiis.2014.04.014", "year": "2014", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Jing Wang 0039": "02/736-39", "Tonghong Li": "98/4254", "Qi Qi 0001": "80/6406-1"}, "abstract": ""}, "journals/jnca/WangLLWWQ14": {"title": "Probe-based end-to-end overload control for networks of SIP servers.", "url": "https://doi.org/10.1016/j.jnca.2013.10.017", "year": "2014", "author": {"Jinzhu Wang": "70/9529", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254", "Jing Wang 0039": "02/736-39", "Jingyu Wang 0001": "37/2749-1", "Qi Qi 0001": "80/6406-1"}, "abstract": "The Session Initiation Protocol (SIP) has been adopted by the IETF as the control protocol for creating, modifying and terminating multimedia sessions. Overload occurs in SIP networks when SIP servers have insufficient resources to handle received messages. Under overload, SIP networks may suffer from congestion collapse due to current ineffective SIP overload control mechanisms. This paper introduces a probe-based end-to-end overload control (PEOC) mechanism, which is deployed at the edge servers of SIP networks and is easy to implement. By probing the SIP network with SIP messages, PEOC estimates the network load and controls the traffic admitted to the network according to the estimated load. Theoretic analysis and extensive simulations verify that PEOC can keep high throughput for SIP networks even when the offered load exceeds the capacity of the network. Besides, it can respond quickly to the sudden variations of the offered load and achieve good fairness."}, "journals/jss/LiaoLZW14": {"title": "Accurate sub-swarms particle swarm optimization algorithm for service composition.", "url": "https://doi.org/10.1016/j.jss.2013.11.1113", "year": "2014", "author": {"Jianxin Liao": "60/4951", "Yang Liu": "51/3710", "Xiaomin Zhu 0002": "09/4144-2", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "Service composition (SC) generates various composite applications quickly by using a novel service interaction model. Before composing services together, the most important thing is to find optimal candidate service instances compliant with non-functional requirements. Particle swarm optimization (PSO) is known as an effective and efficient algorithm, which is widely used in this process. However, the premature convergence and diversity loss of PSO always results in suboptimal solutions. In this paper, we propose an accurate sub-swarms particle swarm optimization (ASPSO) algorithm by adopting parallel and serial niching techniques. The ASPSO algorithm locates optimal solutions by using sub-swarms searching grid cells in which the density of feasible solutions is high. Simulation results demonstrate that the proposed algorithm improves the accuracy of the standard PSO algorithm in searching the optimal solution of service selection problem."}, "journals/corr/abs-1305-2387": {"title": "Loss Rate Based Fountain Codes for Data Transfer", "url": "http://arxiv.org/abs/1305.2387", "year": "2013", "author": {"Jianxin Liao": "60/4951", "Lei Zhang 0094": "97/8704-94", "Xiaomin Zhu 0002": "09/4144-2", "Jingyu Wang 0001": "37/2749-1", "Minyan Liao": "130/3902"}, "abstract": "\n      Abstract:  Fountain codes are becoming increasingly important for data transferring over\ndedicated high-speed long-distance network. However, the encoding and decoding\ncomplexity of traditional fountain codes such as LT and Raptor codes are still\nhigh. In this paper, a new fountain codes named LRF (Loss Rate Based Fountain)\ncodes for data transfer is proposed. In order to improve the performance of\nencoding and decoding efficiency and decrease the number of redundant encoding\nsymbols, an innovative degree distribution instead of robust soliton degree\ndistribution in LT (Luby Transfer) codes is proposed. In LRF codes, the degree\nof encoding symbol is decided by loss rate property, and the window size is\nextended dynamic. Simulations result using LRF codes show that the proposed\nmethod has better performance in term of encoding ratio, degree ratio, encoding\nand decoding efficiency with respect to LT and Raptor codes.\n\n    "}, "journals/cee/LiaoQXLCW12": {"title": "A linear chained approach for service invocation in IP Multimedia Subsystem.", "url": "https://doi.org/10.1016/j.compeleceng.2012.03.010", "year": "2012", "author": {"Jianxin Liao": "60/4951", "Qi Qi 0001": "80/6406-1", "Zhaoyong Xun": "11/1586", "Tonghong Li": "98/4254", "Yufei Cao": "92/2499", "Jingyu Wang 0001": "37/2749-1"}, "abstract": "IP Multimedia Subsystem (IMS) is considered to provide multimedia services to users through an IP-based control plane. The current IMS service invocation mechanism, however, requires the Serving-Call Session Control Function (S-CSCF) invokes each Application Server (AS) sequentially to perform service subscription profile, which results in the heavy load of the S-CSCF and the long session set-up delay. To solve this issue, this paper proposes a linear chained service invocation mechanism to invoke each AS consecutively. By checking all the initial Filter Criteria (iFC) one-time and adding the addresses of all involved ASs to the “Route” header, this new approach enables multiple services to be invoked as a linear chain during a session. We model the service invocation mechanisms through Jackson networks, which are validated through simulations. The analytic results verify that the linear chained service invocation mechanism can effectively reduce session set-up delay of the service layer and decrease the load level of the S-CSCF."}, "journals/cm/LiaoWWW12": {"title": "Toward a multiplane framework of NGSON: a required guideline to achieve pervasive services and efficient resource utilization.", "url": "https://doi.org/10.1109/MCOM.2012.6122537", "year": "2012", "author": {"Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Bin Wu": "98/4432", "Wei Wu": "95/6985"}, "abstract": " Abstract:The next generation service overlay network will enable the provision of pervasive services over a common IP-based transport infrastructure. However, along with the service consolidation over IP and the rapid increase of network scale, the task of service and network management has become extremely complex and disordered. Efforts are being made in various areas, such as SOA, web services, IMS, P2P/CDN overlay, and cloud computing, to achieve a scalable, secure, converged, and selfmanaged service network framework to shorten the time to market of new services, as well as lower the management costs of network/service providers. Nevertheless, these technologies are not developed in a coordinated manner for optimal scalability, resource utilization, and QoS performance. In this article, we propose a multiplane framework (MPF) for NGSON based on the integration of service composition, management, and delivery, which is a top-down reference model to achieve pervasive services and efficient resource utilization."}, "journals/cn/LiaoWLWWZ12": {"title": "A distributed end-to-end overload control mechanism for networks of SIP servers.", "url": "https://doi.org/10.1016/j.comnet.2012.04.024", "year": "2012", "author": {"Jianxin Liao": "60/4951", "Jinzhu Wang": "70/9529", "Tonghong Li": "98/4254", "Jing Wang 0039": "02/736-39", "Jingyu Wang 0001": "37/2749-1", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": "The Session Initiation Protocol (SIP) is an application-layer control protocol standardized by the IETF for creating, modifying and terminating multimedia sessions. With the increasing use of SIP in large deployments, the current SIP design cannot handle overload effectively, which may cause SIP networks to suffer from congestion collapse under heavy offered load. This paper introduces a distributed end-to-end overload control (DEOC) mechanism, which is deployed at the edge servers of SIP networks and is easy to implement. By applying overload control closest to the source of traffic, DEOC can keep high throughput for SIP networks even when the offered load exceeds the capacity of the network. Besides, it responds quickly to the sudden variations of the offered load and achieves good fairness. Theoretic analysis and extensive simulations verify that DEOC is effective in controlling overload of SIP networks."}, "journals/ijahuc/WangLLZ12": {"title": "Correlation-aware multipath selection to enhance path diversity in ubiquitous computing environment.", "url": "https://doi.org/10.1504/IJAHUC.2012.050438", "year": "2012", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254", "Ping Zhang 0003": "77/2428-3"}, "abstract": ""}, "journals/ijcomsys/LiaoQLCZW12": {"title": "An optimized QoS scheme for IMS-NEMO in heterogeneous networks.", "url": "https://doi.org/10.1002/dac.1263", "year": "2012", "author": {"Jianxin Liao": "60/4951", "Qi Qi 0001": "80/6406-1", "Tonghong Li": "98/4254", "Yufei Cao": "92/2499", "Xiaomin Zhu 0002": "09/4144-2", "Jingyu Wang 0001": "37/2749-1"}, "abstract": ""}, "journals/itiis/LiaoLWZ12": {"title": "Service Composition Based on Niching Particle Swarm Optimization in Service Overlay Networks.", "url": "https://doi.org/10.3837/tiis.2012.04.009", "year": "2012", "author": {"Jianxin Liao": "60/4951", "Yang Liu": "51/3710", "Jingyu Wang 0001": "37/2749-1", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": ""}, "journals/jnca/WangLL12": {"title": "OSIA: Out-of-order Scheduling for In-order Arriving in concurrent multi-path transfer.", "url": "https://doi.org/10.1016/j.jnca.2011.09.004", "year": "2012", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Tonghong Li": "98/4254"}, "abstract": "One major problem of concurrent multi-path transfer (CMT) scheme in multi-homed mobile networks is that the utilization of different paths with diverse delays may cause packet reordering among packets of the same flow. In the case of TCP-like, the reordering exacerbates the problem by bringing more timeouts and unnecessary retransmissions, which eventually degrades the throughput of connections considerably. To address this issue, we first propose an Out-of-order Scheduling for In-order Arriving (OSIA), which exploits the sending time discrepancy to preserve the in-order packet arrival. Then, we formulate the optimal traffic scheduling as a constrained optimization problem and derive its closed-form solution by our proposed progressive water-filling solution. We also present an implementation to enforce the optimal scheduling scheme using cascaded leaky buckets with multiple faucets, which provides simple guidelines on maximizing the utilization of aggregate bandwidth while decreasing the probability of triggering 3 dupACKs. Compared with previous work, the proposed scheme has lower computation complexity and can also provide the possibility for dynamic network adaptability and finer-grain load balancing. Simulation results show that our scheme significantly alleviates reordering and enhances transmission performance."}, "journals/cn/LiaoWLZ11": {"title": "Introducing multipath selection for concurrent multipath transfer in the future internet.", "url": "https://doi.org/10.1016/j.comnet.2010.12.010", "year": "2011", "author": {"Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Tonghong Li": "98/4254", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": "It is essential for the Future Internet to fully support multihoming and select most appropriate paths for Concurrent Multipath Transfer (CMT). In real complex networks, different paths are likely to overlap each other and even share bottlenecks which can weaken the path diversity gained through CMT. Spurred by this observation, it is necessary to select multiple independent paths insofar as possible. However, the path correlation lurks behind the IP/network layer topology, so we have to fall back to end-to-end probes to estimate this correlation by analyzing path delay characteristics. In this paper, we present the first step towards a new topic of correlation-aware multipath selection, with formal and systematic problem definition, modeling and solution. Based on a well-designed delay probing, a Grouping-based Multipath Selection (GMS) mechanism is developed to avoid underlying shared bottlenecks between topologically joint paths. In addition, we further propose a practical functionality framework and define a novel multihoming sublayer for the exchange of the multipath capabilities. Extensive simulations demonstrate that the GMS under different network conditions performs much better than other selection schemes, even with burst background traffic."}, "journals/comcom/LiaoWLWZ11": {"title": "A token-bucket based notification traffic control mechanism for IMS presence service.", "url": "https://doi.org/10.1016/j.comcom.2010.12.017", "year": "2011", "author": {"Jianxin Liao": "60/4951", "Jinzhu Wang": "70/9529", "Tonghong Li": "98/4254", "Jingyu Wang 0001": "37/2749-1", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": "Presence is a service that allows a user to be informed about the specified state of another user. Presence service has become a key enabler for next-generation applications such as instant messaging, push-to-talk and web2.0. However, recent studies show that the notification traffic of presence service causes heavy signaling load on IP multimedia subsystem (IMS) network. This paper introduced a token-bucket based notification traffic control (TNTC) mechanism, which is an application layer solution deployed at the presence server. The TNTC aims at upgrading valid access probability while controlling the notification traffic. A mathematical model of a queuing system is proposed to describe TNTC. We analyzed its main probability features and investigated the effects of different parameters on the performance of TNTC. Extensive simulations verified that TNTC can effectively control notification traffic and perform better than the existing schemes in terms of valid access probability and update arrival rate."}, "journals/ieicet/LiaoWLZ11": {"title": "Multipath Probing and Grouping in Multihomed Networks.", "url": "https://doi.org/10.1587/transinf.E94.D.710", "year": "2011", "author": {"Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Tonghong Li": "98/4254", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": ""}, "journals/tce/LiaoWZL10": {"title": "Exploiting path diversity to enhance aggregating throughput for multihomed wireless devices.", "url": "https://doi.org/10.1109/TCE.2010.5505978", "year": "2010", "author": {"Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Xiaomin Zhu 0002": "09/4144-2", "Tonghong Li": "98/4254"}, "abstract": " Abstract:Multi-homed wireless devices equipped with multiple network interfaces can transmit data over multiple access paths simultaneously for high-speed services. It is essential for data transmission to select suitable number of paths to use from all available paths. At real multihomed network, multiple paths are likely to overlap each other and share the same bottleneck, which will weaken path diversity drastically. Spurred by this observation, selection of multiple paths should consider path correlation to select multiple independent paths as much as possible. In this paper, we propose a multi-path selection scheme to avoid underlying shared bottleneck between topologically joint paths. The results demonstrate that our approach under different network conditions is more accurate than other selection schemes, even with burst background traffic."}, "journals/tce/LiaoWLZZ10": {"title": "Sender-based multipath out-of-order scheduling for high-definition videophone in multi-homed devices.", "url": "https://doi.org/10.1109/TCE.2010.5606284", "year": "2010", "author": {"Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Tonghong Li": "98/4254", "Xiaomin Zhu 0002": "09/4144-2", "Ping Zhang 0003": "77/2428-3"}, "abstract": " Abstract:Multi-homed devices equipped with multiple network interfaces can transmit data over multiple access paths simultaneously for high-speed services. One major problem is that the utilization of different paths with diverse delays may cause packet reordering among packets of the same flow. In the case of TCP-like, the reordering exacerbates the problem by bringing more timeouts and unnecessary retransmissions, which eventually degrades the throughput of connections considerably. To address this issue, we propose a Sender-based Multipath Out-of-order Scheduling (SMOS) scheme, which exploits the sending time discrepancy to preserve the in-order packet arrival. The performance of the proposed SMOS is investigated and compared against other schemes. The obtained results demonstrate the outstanding performance of SMOS as it enhances the utilization of aggregate bandwidth while decreasing the probability of triggering 3 dupACKs."}, "journals/tce/QiCLZW10": {"title": "Soft handover mechanism based on RTP parallel transmission for mobile IPTV services.", "url": "https://doi.org/10.1109/TCE.2010.5681100", "year": "2010", "author": {"Qi Qi 0001": "80/6406-1", "Yufei Cao": "92/2499", "Tonghong Li": "98/4254", "Xiaomin Zhu 0002": "09/4144-2", "Jingyu Wang 0001": "37/2749-1"}, "abstract": " Abstract:Mobile Internet Protocol Television (IPTV) provides users with multimedia content through wireless access technologies whenever they want and wherever they are. However, vertical handover in heterogeneous networks results in long interruption time and large packet loss rate, which poses a challenge for seamless mobile IPTV service. The existing solutions that make use of the heterogeneous access networks to transmit multimedia content have two drawbacks: high bandwidth consumption of the wireless networks and lots of packets loss when a user resides in the overlapped area of the boundary. To solve these issues, we propose a new soft handover mechanism, based on the IPTV server and the consumer electronic devices. The proposed Parallel Soft Handover (PSH) divides the Real-time Transport Protocol (RTP) packets according to the packet loss rates, and exploits different networks efficiently by using parallel transmission. Moreover, the selective retransmission for RTP packets is introduced to reduce the packet loss. Simulation results demonstrate that the PSH can reduce the packet loss rate, as well as obtain a more efficient use of the wireless bandwidth."}, "journals/cn/LiaoWZ08": {"title": "A multi-path mechanism for reliable VoIP transmission over wireless networks.", "url": "https://doi.org/10.1016/j.comnet.2008.04.008", "year": "2008", "author": {"Jianxin Liao": "60/4951", "Jingyu Wang 0001": "37/2749-1", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": "With the advances in audio encoding standards and wireless access networks, voice over IP (VoIP) is becoming quite popular. However, real-time voice data over lossy networks (such as WLAN and UMTS), still posses several challenging problems because of the adverse effects caused by complex network dynamics. One approach to provide QoS for VoIP applications over the wireless networks is to use multiple paths to deliver VoIP data destined for a particular receiver. This paper introduced cmpSCTP, a transport layer solution for concurrent multi-path transfer that modifies the standard stream control transmission protocol (SCTP). The cmpSCTP aims at exploiting SCTP’s multi-homing capability by selecting several best paths among multiple available network interfaces to improve data transfer rate to the same multi-homed device. Through the use of path monitoring and packet allotment techniques, cmpSCTP tries to transmit an amount of packets corresponding to the path’s ability. At the same time, cmpSCTP updates the transmission strategy based on the real-time information of all of paths. Using cmpSCTP’s flexible path management capability, we may switch the flow between multiple paths automatically to realize seamless path handover. The theoretical analysis evaluated the model of cmpSCTP and formulated optimal traffic fragmentation of VoIP data. Extensive simulations under different scenarios using OPNET verified that cmpSCTP can effectively enhance VoIP transmission efficiency and highlighted the superiority of cmpSCTP against the other SCTP’s extension implementations under performance indexes such as throughput, handover latency, packet delay, and packet loss."}, "journals/comcom/WangLZ08": {"title": "Latent Handover: A flow-oriented progressive handover mechanism.", "url": "https://doi.org/10.1016/j.comcom.2008.02.017", "year": "2008", "author": {"Jingyu Wang 0001": "37/2749-1", "Jianxin Liao": "60/4951", "Xiaomin Zhu 0002": "09/4144-2"}, "abstract": "The next generation network may be regarded as an IP (Internet Protocol) converged network, combining various access technologies. In this environment, ubiquitous computing and seamless networking are required for the mobile host. The growing availability of multiple network interfaces on mobile devices makes Concurrent Multi-path Transfer (CMT) an appealing candidate to realize the seamless handover. In this paper, we proposed a novel Latent Handover mechanism, which is flow-oriented and switches the traffic to the new path progressively to make the handover process unconscious to users or upper layer applications, especially real-time services. In addition, to support the proposed Latent Handover, the transport layer introduces our proposed cmpSCTP (concurrent multi-path Stream Control Transmission Protocol), which adds concurrent multi-path transfer and dynamic Multi-Path Management support to the standard SCTP for realizing seamless handover.The theoretical analysis verified that the proposed Latent Handover can efficiently optimize the handover process and enhance transmission efficiency during handover. Extensive simulations under different scenarios using OPNET highlighted the superiority of the proposed Latent Handover solution against other existing handover schemes under performance indexes such as throughput, handover latency, packet delay, and packet loss."}}}