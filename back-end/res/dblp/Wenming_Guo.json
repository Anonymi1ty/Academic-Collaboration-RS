{"id": "89/6072", "name": "Wenming Guo", "Article": {"conf/icaiis/GuoWH21": {"title": "Attention-Guided CutMix Data Augmentation Network for Fine-Grained Bird Recognition.", "url": "https://doi.org/10.1145/3469213.3470323", "year": "2021", "author": {"Wenming Guo": "89/6072", "Yifei Wang": "00/555", "Fang Han": "33/4395"}, "abstract": "\n\t\tLarge inter-class variations, intra-class similarities and the difficulties in collectiing training samples are three major challenges in fine-grained bird recognition. Hence, it's essential for accurate classification that achieving a discriminative feature representation from birds’ parts. In this work, we propose Attention-Guided CutMix Data Augmentation Network (AGCN) to exercise the network to pay more attention on subtle features about birds’ parts. Firstly, we generate feature maps and attention maps from an image by a backbone network. Specially, each attention map contains discriminative information of a bird's part. Then local features can be further extracted by the element-wise multiplication between feature maps and attention maps. Next, to reduce overfitting and optimize performance, we design a data augmentation strategy according to region-level replacement. For each training image, based on an attention map, a bird's discriminative region is located, copied and pasted into another image to generate an augmented image. In addition, a loss is designed to supervise both the learning of attention maps and the training of the network. AGCN only needs image-level category labels rather than bounding boxes/part annotations. The results state that our proposed data augmentation effectively improves the classification performance of the network and AGCN achieves excellent performance on the challenging dataset, CUB Birds.\n\t"}, "conf/miccai/ZhuCZXGZ19": {"title": "ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths.", "url": "https://doi.org/10.1007/978-3-030-32239-7_79", "year": "2019", "author": {"Yanhao Zhu": "55/4839", "Zhineng Chen": "96/7998", "Shuai Zhao": "116/8682", "Hongtao Xie": "25/588", "Wenming Guo": "89/6072", "Yongdong Zhang 0001": "z/YongdongZhang"}, "abstract": "Nowadays U-net-like FCNs predominate various biomedical image segmentation applications and attain promising performance, largely due to their elegant architectures, e.g., symmetric contracting and expansive paths as well as lateral skip-connections. It remains a research direction to devise novel architectures to further benefit the segmentation. In this paper, we develop an ACE-net that aims to enhance the feature representation and utilization by augmenting the contracting and expansive paths. In particular, we augment the paths by the recently proposed advanced techniques including ASPP, dense connection and deep supervision mechanisms, and novel connections such as directly connecting the raw image to the expansive side. With these augmentations, ACE-net can utilize features from multiple sources, scales and reception fields to segment while still maintains a relative simple architecture. Experiments on two typical biomedical segmentation tasks validate its effectiveness, where highly competitive results are obtained in both tasks while ACE-net still runs fast at inference."}, "conf/icnc/GuoQL18": {"title": "WDXI: The Dataset of X-Ray Image for Weld Defects.", "url": "https://doi.org/10.1109/FSKD.2018.8686975", "year": "2018", "author": {"Wenming Guo": "89/6072", "Huifan Qu": "239/2615", "Lihong Liang": "31/6973"}, "abstract": " Abstract:We establish the dataset of X-ray image for weld defects called WDXI. WDXI consists of 13,766 X-ray images, including seven significant types of weld defects. The images of metal plates, metal fittings and other welded parts are included in the data set. Qualified nondestructive testing(NDT)inspectors annotates every image. After that the image is audited by a higher level inspector. The inspector gives the final result based on the image and image test report. According to the report, we have sorted out the defect type, defect size, quality level of each weld, and other detailed information during the NDT process. We remove the letters and numbers which was placed manually when irradiated and extract the weld area from the image. The window width and window level method is used to enhance the image quality of weld area and highlights details of defects. On this data set, we trained a 5-layer Convolution neural network(CNN)to identify the type of defects. The rate of top-l accuracy reached 46.6%. Although the result is not very high, it can provide a useful example of how to use this data set. And this result also shows that there is still an extensive research space for this task in the future. The WDXI dataset will be publicly available in the future for academic research purposes which may promote the development of industrial radiographic NDT. WDXI will be the foundation for the study of weld defects classification and localization. It will provide data support for the majority of researchers to test their models."}, "conf/ccis/WangJLGW16": {"title": "An improved text similarity algorithm research for clinical decision support system.", "url": "https://doi.org/10.1109/CCIS.2016.7790244", "year": "2016", "author": {"Wen Wang": "29/4680", "Qiaowei Jiang": "192/0972", "Tao Lv 0003": "69/5034-3", "Wenming Guo": "89/6072", "Cong Wang 0003": "18/2771-3"}, "abstract": " Abstract:The paper presents an improved text similarity algorithm and applies to the Clinical Decision Support System. It can improve the efficiency of decision making. After analyzing the disadvantages of the conventional TF-IDF algorithm and Cosine Similarity calculation, we propose an improvement to the text similarity algorithm. The proposed algorithm takes into full consideration the impact of semantically-similar keywords and textually-similar keywords on text similarity, therefore achieves improved accuracy. The function of this Clinical Decision Support System is to search for relevant biomedical articles and locate solutions."}, "conf/ccis/WengWJGW16": {"title": "Application of support vector machines in medical data.", "url": "https://doi.org/10.1109/CCIS.2016.7790253", "year": "2016", "author": {"Yongqiang Weng": "192/1071", "Chunshan Wu": "192/1053", "Qiaowei Jiang": "192/0972", "Wenming Guo": "89/6072", "Cong Wang 0003": "18/2771-3"}, "abstract": " Abstract:Compared with ordinary data, medical data has its own characteristics. Such as mode of polymorphism, incomplete and longer timeliness. These characteristics brought a lot of difficulties on medical data of collection and processing, so the incremental learning method in the application of medical data is particularly critical. In this paper, Based on the support vector machines (SVM) proposed an incremental learning method that combined with fuzzy c-average and generalized KKT conditions. Through the filter of historical sample set and new sample that is invalid to reduce the training sample. So as to achieve rapid, incremental learning. Finally, the improved algorithm applied to the two standard medical database from UCI, which verify the improved algorithm advantage."}, "conf/ccis/WuWJGW16": {"title": "Applied research on visual mining technology in medical data.", "url": "https://doi.org/10.1109/CCIS.2016.7790259", "year": "2016", "author": {"Chunshan Wu": "192/1053", "Yongqiang Weng": "192/1071", "Qiaowei Jiang": "192/0972", "Wenming Guo": "89/6072", "Cong Wang 0003": "18/2771-3"}, "abstract": " Abstract:Data mining is a process of discovering information from large datasets by using automated algorithms, which makes it difficult for users to intuitively understand, explore and optimize the datasets and algorithms process. Recently with the development of the visual field, researchers have been exploring how to use the visualization methods to make users intuitively understand datasets and algorithms. In this paper we introduce the concept of visual data mining and dissect the visual data mining technology, mainly focusing on three related aspects: source data, data mining processes and data mining results; then design and implement a visual mining system based on decision tree algorithm, which achieves the encapsulation of data mining algorithms, and presents to the user a simple interactive interface, so that users can carry out visual data mining; finally apply the visual data mining technology to the survey data of elderly anxiety patients, and analyze the decision rules of anxiety disorders, which has important significance for clinical research and decision-making."}, "conf/bmei/GuoSX15": {"title": "A disease forecasting algorithm based on single factor correlation analysis and the JacUOD algorithm.", "url": "https://doi.org/10.1109/BMEI.2015.7401558", "year": "2015", "author": {"Wenming Guo": "89/6072", "Yinfeng Sun": "175/8800", "Xiaotong Xie": "175/8920"}, "abstract": " Abstract:There is a close relationship between the occurrence of a variety of diseases and meteorological factors. However, the typical disease forecasting methods are based on history data and the requirement of initial data is strict. To solve these problems, we proposed a disease forecasting algorithm to adapt to real-time data. The proposed algorithm has two contributions: (1) It uses the single factor correlation analysis methods when selecting meteorological factors that affect disease (2) It introduces a new method to calculate disease prediction to build date _number _meteorological factor matrix and use JacUOD algorithm to evaluate the similarity of meteorological factors between the target dates and past ones. To find out the top-N dates are of the maximum similarity with the target one, therefore, we could forecast the number combining the similarity value and the N date's patient number. Obviously, the number of patient is obtained by calculating the similarity of different dates' meteorological factors. Experiments show that the algorithm generates a better accuracy than the traditional algorithms in disease prediction."}, "conf/icnc/GuoD14": {"title": "Topic mining for call centers based on LDA.", "url": "https://doi.org/10.1109/ICNC.2014.6975947", "year": "2014", "author": {"Wenming Guo": "89/6072", "Tianlang Deng": "155/5714"}, "abstract": " Abstract:Latent Dirichlet Allocation, which is a non-supervised learning method, can be used for topic detection, automatic text categorization, keyword extraction and so on. It only focuses on the text itself, not considering other external correlation properties. External association property refers to some structured attributes that correspondence with the text data, for example, a paper usually has several properties like authors, publishing time etc. A telephone call usually has several properties like caller number, call time etc. To iron out flaws; we propose an improved model A-LDA based LDA. We use data sets from telephone call centers (a kind of data centers in rapid growth) to experiment on topic detection. The topic results show that A-LDA with introduce of external correlation properties, compared with the traditional LDA, is decreased in perplexity value and has better generalization performance. At the same time, we can obtain the topic that external attributes contained."}, "conf/interaction/ZhangZG09": {"title": "TCP portscan detection based on single packet flows and entropy.", "url": "https://doi.org/10.1145/1655925.1656116", "year": "2009", "author": {"Hai Zhang": "72/4420", "Xuyang Zhu": "44/7955", "Wenming Guo": "89/6072"}, "abstract": "\n\t\tPortscanning is a common activity of considerable importance. It is often used by computer attackers to characterize hosts or networks which they are considering hostile activity against. Thus it is useful for system administrators and other network defenders to detect portscans as possible preliminaries to a more serious attack. Thus it is of considerable interest to attackers to determine whether or not the defenders of a network are portscanning it regularly. A major difficulty with detecting these portscans on a high-speed monitoring point is that the traffic volume on high speed links can be tens of gigabits per second and can contain millions of flow and high volume of traffic. Our purpose is to detect portscans based on the flow records on the internet. This data set is sometimes too large for us. Fortunately, we have an approach to detect some specific portscan. First, filter out any web traffic on port 80 and other non-TCP flows. So the data sets are reduced significantly. However, the data sets still are too large for us. Then employ sampling on the data sets. There had been many alternative sampling methods. In this paper, we used simple random sampling, considering this method could select flow records uniformly. Finally, with the sampled data, we introduce a new way to identify ports scanners. As the host which scan large number of different destination IP addresses and ports is probably a ports scanners we can compute the entropy of each host, which reflect the distribution of its destination IP addresses and ports. In theory, simple random sampling has minimal impact on the result of entropy of each host. Therefore the estimation of entropy will be more precise. The experimental results show that datum from the sample also can tell which hosts are port scanners accurately. We will see that the attackers' entropy for destination IP address is bigger than others clearly. So entropy-based SYN detection can help us find out scanners effectively.\n\t"}, "conf/ncm/ZhangYG09": {"title": "Flow Byte Sizes Estimation from Simple Random Packet Sampling.", "url": "https://doi.org/10.1109/NCM.2009.12", "year": "2009", "author": {"Hai Zhang": "72/4420", "Zhuxu Yang": "87/8391", "Wenming Guo": "89/6072"}, "abstract": " Abstract:The flow byte sizes is an important metric for network measurement. Previous work on estimating the flow size distribution (in bytes) has been focus on improving sampling technique to increase measurement accuracy. In this paper, we present an algorithm of flow byte sizes estimation based on random packet sampling. Our algorithm first obtains a maximum likelihood estimator of flow length using the TCP protocol information. After that the estimation of the flow byte sizes is accomplished through a linear regression model that relies on the flow length information previously obtained. Experimental results show that our approach is effective enough to capture the network traffic characteristics."}, "conf/skg/SahotaLG06": {"title": "Resource Monitoring with Globus Toolkit 4.", "url": "https://doi.org/10.1109/SKG.2006.79", "year": "2006", "author": {"Vijay Sahota": "26/2382", "Maozhen Li 0001": "l/MaozhenLi", "Wenming Guo": "89/6072"}, "abstract": " Abstract:The past few years have seen the Grid rapidly evolving towards a service-oriented computing infrastructure. With the OGSA facilitating this evolution, it is expected that WSRF will be acting as the main an enabling technology to drive the Grid further. Resource monitoring plays a critical role in managing a large-scale Grid system. This paper presents GREMO, a lightweight resource monitor developed with Globus Toolkit 4 (GT4) for monitoring CPU and memory of computing nodes in a Windows and Linux environments."}, "conf/skg/YuGLSHQ06": {"title": "Service Matchmaking and Discovery with Rough Sets.", "url": "https://doi.org/10.1109/SKG.2006.93", "year": "2006", "author": {"Bin Yu 0005": "27/116-5", "Wenming Guo": "89/6072", "Maozhen Li 0001": "l/MaozhenLi", "Yong-Hua Song": "87/4599", "Peter Hobson": "95/6437", "Man Qi": "17/3518"}, "abstract": " Abstract:The past few years have seen the Grid is evolving as a service-oriented computing infrastructure. It is foreseen that various resources in a Grid environment will be exposed as services for use by a wider user community. Service discovery becomes an issue of vital importance for utilising Grid facilities. This paper presents Rosse, a Rough Sets based Search Engine for service discovery that can deal with uncertainty of service properties when matching service advertisements with service requests. The evaluation results show that Rosse is more effective in service discovery compared with other mechanisms such as UDDI keyword matching and OWL-S matchmaking."}, "journals/access/GuoWH22": {"title": "Multifeature Fusion Keyword Extraction Algorithm Based on TextRank.", "url": "https://doi.org/10.1109/ACCESS.2022.3188861", "year": "2022", "author": {"Wenming Guo": "89/6072", "Zihao Wang": "148/9655", "Fang Han": "33/4395"}, "abstract": " Abstract:Keyword extraction is the predecessor of many tasks, and its results directly affect search, recommendation, classification, and other tasks. In this study, we take Chinese text as the research object and propose a multi-feature fusion keyword extraction algorithm combined with BERT semantics and K-Truss graph(BSKT). The BSKT algorithm is based on the TextRank algorithm, which combines BERT semantic features, K-Truss features, and other features. First, the BSKT algorithm obtains the word vectors from the BERT pretraining model to calculate the semantic difference, which is used to optimize the iterative process of the TextRank word graph. Then, the BSKT algorithm obtains its K-Truss graph by decomposing the TextRank word graph and obtains the truss level feature of the word. Finally, by combining the word IDF and truss level features, the BSKT algorithm scores the words to extract keywords. Experimental results show that the BSKT algorithm achieves better performance than the latest keyword extraction algorithm SCTR in the task of extracting 1–10 keywords. Furthermore, the increment in F1 increased by 11.2% when the BSKT algorithm was used to extract three keywords from the Sensor dataset."}, "journals/access/GuoLH22": {"title": "Named Entity Recognition for Chinese Electronic Medical Records Based on Multitask and Transfer Learning.", "url": "https://doi.org/10.1109/ACCESS.2022.3192866", "year": "2022", "author": {"Wenming Guo": "89/6072", "Junda Lu": "297/0832", "Fang Han": "33/4395"}, "abstract": " Abstract:Current work on named entities for Chinese electronic medical records requires training a separate model for each different type of electronic medical record, the performance of which depends on the amount of training data available for each dataset. However, different types of electronic medical records share similar semantic information with each other, while current models do not take full advantage of this potentially common knowledge. To overcome the mentioned problem, we propose a multi-task learning framework to transfer multiple types of electronic medical records through a shared encoder. Experiments demonstrate that our model achieves substantially better performance compared with the single-task model based on BERT. F1 scores improved by more than 1% on average across the four datasets, with individual datasets improving precision by more than 3.5%. Further analysis shows that our model still achieves better F1 scores on long tail datasets and small size datasets."}, "journals/tsg/ZhangMG19": {"title": "An Integrated Wide-Area Protection Scheme for Active Distribution Networks Based on Fault Components Principle.", "url": "https://doi.org/10.1109/TSG.2017.2741060", "year": "2019", "author": {"Fan Zhang 0023": "21/3626-23", "Longhua Mu": "232/9479", "Wenming Guo": "89/6072"}, "abstract": " Abstract:An integrated wide-area protection scheme for active distribution networks (ADNs) based on a fault components principle is proposed in this paper. The structure of a three-level hierarchically coordinated system that enables the proposed protection strategy is introduced. To manage communication network failures, a backup protection strategy is also proposed. In addition, the proposed scheme can overcome the problems of complex operating conditions, such as fault conditions, in islanding and non-islanding situations, fault resistance, high-impedance faults and topology change. PSCAD/EMTDC is used in simulation analysis of protection schemes for an ADN model, and simulation results have verified the correctness and effectiveness of the protection scheme."}, "journals/corr/abs-1909-04148": {"title": "ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths.", "url": "http://arxiv.org/abs/1909.04148", "year": "2019", "author": {"Yanhao Zhu": "55/4839", "Zhineng Chen": "96/7998", "Shuai Zhao": "116/8682", "Hongtao Xie": "25/588", "Wenming Guo": "89/6072", "Yongdong Zhang 0001": "z/YongdongZhang"}, "abstract": "\n      Abstract:  Nowadays U-net-like FCNs predominate various biomedical image segmentation\napplications and attain promising performance, largely due to their elegant\narchitectures, e.g., symmetric contracting and expansive paths as well as\nlateral skip-connections. It remains a research direction to devise novel\narchitectures to further benefit the segmentation. In this paper, we develop an\nACE-net that aims to enhance the feature representation and utilization by\naugmenting the contracting and expansive paths. In particular, we augment the\npaths by the recently proposed advanced techniques including ASPP, dense\nconnection and deep supervision mechanisms, and novel connections such as\ndirectly connecting the raw image to the expansive side. With these\naugmentations, ACE-net can utilize features from multiple sources, scales and\nreception fields to segment while still maintains a relative simple\narchitecture. Experiments on two typical biomedical segmentation tasks validate\nits effectiveness, where highly competitive results are obtained in both tasks\nwhile ACE-net still runs fast at inference.\n\n    "}, "journals/concurrency/GuoLD17": {"title": "Topic mining for call centers based on A-LDA and distributed computing.", "url": "https://doi.org/10.1002/cpe.3776", "year": "2017", "author": {"Wenming Guo": "89/6072", "Lihong Liang": "31/6973", "Tianlang Deng": "155/5714"}, "abstract": ""}, "journals/npl/GuoALLQ16": {"title": "A Resource Aware MapReduce Based Parallel SVM for Large Scale Image Classifications.", "url": "https://doi.org/10.1007/s11063-015-9472-z", "year": "2016", "author": {"Wenming Guo": "89/6072", "Nasullah Khalid Alham": "22/8490", "Yang Liu 0010": "51/3710-10", "Maozhen Li 0001": "l/MaozhenLi", "Man Qi": "17/3518"}, "abstract": "Machine learning techniques have facilitated image retrieval by automatically classifying and annotating images with keywords. Among them support vector machines (SVMs) are used extensively due to their generalization properties. However, SVM training is notably a computationally intensive process especially when the training dataset is large. This paper presents RASMO, a resource aware MapReduce based parallel SVM algorithm for large scale image classifications which partitions the training data set into smaller subsets and optimizes SVM training in parallel using a cluster of computers. A genetic algorithm based load balancing scheme is designed to optimize the performance of RASMO in heterogeneous computing environments. RASMO is evaluated in both experimental and simulation environments. The results show that the parallel SVM algorithm reduces the training time significantly compared with the sequential SMO algorithm while maintaining a high level of accuracy in classifications."}}}