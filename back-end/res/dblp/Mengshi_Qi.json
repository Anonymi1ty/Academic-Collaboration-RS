{"id": "191/2586", "name": "Mengshi Qi", "Article": {"conf/icmcs/Du0Q0M22": {"title": "Towards Adversarial Robust Representation Through Adversarial Contrastive Decoupling.", "url": "https://doi.org/10.1109/ICME52920.2022.9859625", "year": "2022", "author": {"Peilun Du": "250/0521", "Xiaolong Zheng 0002": "98/3341-2", "Mengshi Qi": "191/2586", "Liang Liu 0001": "10/6178-1", "Huadong Ma": "04/6217"}, "abstract": " Abstract:Adversarial training can boost the robustness of the model by aligning discriminative features between natural and generated adversarial samples. However, the generated adversarial samples tend to have more features derived from changed patterns in other categories along with the training process, which prevents better feature alignment between natural and adversarial samples. Unfortunately, existing adversarial training methods ignore such dynamicity of generated adversarial samples. In this paper, we propose Adversarial Contrastive Decoupling (ACD) to filter the features derived from changed patterns. Specificity, we decouple the changed patterns from adversarial samples and then extract robust representations from remaining features. First, we introduce a decoupling module with a dynamic labeling strategy to explore the dynamicity of generated adversarial samples. Then, we propose a siamese network with contrastive learning mechanism to align remaining robust representations between adversarial and natural samples. Extensive experimental results demonstrate the superior performance of ACD over baselines."}, "conf/mm/QiQ0S0L21": {"title": "Latent Memory-augmented Graph Transformer for Visual Storytelling.", "url": "https://doi.org/10.1145/3474085.3475236", "year": "2021", "author": {"Mengshi Qi": "191/2586", "Jie Qin": "30/5781", "Di Huang 0001": "45/780-1", "Zhiqiang Shen": "149/5273", "Yi Yang 0001": "33/4854-1", "Jiebo Luo": "25/5545"}, "abstract": "\n\t\tVisual storytelling aims to automatically generate a human-like short story given an image stream. Most existing works utilize either scene-level or object-level representations, neglecting the interaction among objects in each image and the sequential dependency between consecutive images. In this paper, we present a novel Latent Memory-augmented Graph Transformer~(LMGT ), a Transformer based framework for visual story generation. LMGT directly inherits the merits from the Transformer, which is further enhanced with two carefully designed components, i.e., a graph encoding module and a latent memory unit. Specifically, the graph encoding module exploits the semantic relationships among image regions and attentively aggregates critical visual features based on the parsed scene graphs. Furthermore, to better preserve inter-sentence coherence and topic consistency, we introduce an augmented latent memory unit that learns and records highly summarized latent information as the story line from the image stream and the sentence history. Experimental results on three widely-used datasets demonstrate the superior performance of LMGT over the state-of-the-art methods.\n\t"}, "conf/cvpr/QiQWY20": {"title": "Imitative Non-Autoregressive Modeling for Trajectory Forecasting and Imputation.", "url": "https://openaccess.thecvf.com/content_CVPR_2020/html/Qi_Imitative_Non-Autoregressive_Modeling_for_Trajectory_Forecasting_and_Imputation_CVPR_2020_paper.html", "year": "2020", "author": {"Mengshi Qi": "191/2586", "Jie Qin": "30/5781", "Yu Wu 0011": "22/0-11", "Yi Yang 0001": "33/4854-1"}, "abstract": "\nTrajectory forecasting and imputation are pivotal steps towards understanding the movement of human and objects, which are quite challenging since the future trajectories and missing values in a temporal sequence are full of uncertainties, and the spatial-temporally contextual correlation is hard to model. Yet, the relevance between sequence prediction and imputation is disregarded by existing approaches. To this end, we propose a novel imitative non-autoregressive modeling method to simultaneously handle the trajectory prediction task and the missing value imputation task. Specifically, our framework adopts an imitation learning paradigm, which contains a recurrent conditional variational autoencoder (RC-VAE) as a demonstrator, and a non-autoregressive transformation model (NART) as a learner. By jointly optimizing the two models, RC-VAE can predict the future trajectory and capture the temporal relationship in the sequence to supervise the NART learner. As a result, NART learns from the demonstrator and imputes the missing value in a non autoregressive strategy. We conduct extensive experiments on three popular datasets, and the results show that our model achieves state-of-the-art performance across all the datasets."}, "conf/mm/QiQZ0YL20": {"title": "Few-Shot Ensemble Learning for Video Classification with SlowFast Memory Networks.", "url": "https://doi.org/10.1145/3394171.3416269", "year": "2020", "author": {"Mengshi Qi": "191/2586", "Jie Qin": "30/5781", "Xiantong Zhen": "78/10651", "Di Huang 0001": "45/780-1", "Yi Yang 0001": "33/4854-1", "Jiebo Luo": "25/5545"}, "abstract": "\n\t\tIn the era of big data, few-shot learning has recently received much attention in multimedia analysis and computer vision due to its appealing ability of learning from scarce labeled data. However, it has been largely underdeveloped in the video domain, which is even more challenging due to the huge spatial-temporal variability of video data. In this paper, we address few-shot video classification by learning an ensemble of SlowFast networks augmented with memory units. Specifically, we introduce a family of few-shot learners based on SlowFast networks which are used to extract informative features at multiple rates, and we incorporate a memory unit into each network to enable encoding and retrieving crucial information instantly. Furthermore, we propose a choice controller network to leverage the diversity of few-shot learners by learning to adaptively assign a confidence score to each SlowFast memory network, leading to a strong classifier for enhanced prediction. Experimental results on two widely-adopted video datasets demonstrate the effectiveness of the proposed method, as well as its superior performance over the state-of-the-art approaches.\n\t"}, "conf/cvpr/QiLYWL19": {"title": "Attentive Relational Networks for Mapping Images to Scene Graphs.", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qi_Attentive_Relational_Networks_for_Mapping_Images_to_Scene_Graphs_CVPR_2019_paper.html", "year": "2019", "author": {"Mengshi Qi": "191/2586", "Weijian Li": "128/5278", "Zhengyuan Yang": "163/9713", "Yunhong Wang": "95/6251", "Jiebo Luo": "25/5545"}, "abstract": "\nScene graph generation refers to the task of automatically mapping an image into a semantic structural graph, which requires correctly labeling each extracted object and their interaction relationships. Despite the recent success in object detection using deep learning techniques, inferring complex contextual relationships and structured graph representations from visual data remains a challenging topic. In this study, we propose a novel Attentive Relational Network that consists of two key modules with an object detection backbone to approach this problem. The first module is a semantic transformation module utilized to capture semantic embedded relation features, by translating visual features and linguistic features into a common semantic space. The other module is a graph self-attention module introduced to embed a joint graph representation through assigning various importance weights to neighboring nodes. Finally, accurate scene graphs are produced by the relation inference module to recognize all entities and corresponding relations. We evaluate our proposed method on the widely-adopted Visual Genome Dataset, and the results demonstrate the effectiveness and superiority of our model."}, "conf/cvpr/QiWQL19": {"title": "KE-GAN: Knowledge Embedded Generative Adversarial Networks for Semi-Supervised Scene Parsing.", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qi_KE-GAN_Knowledge_Embedded_Generative_Adversarial_Networks_for_Semi-Supervised_Scene_Parsing_CVPR_2019_paper.html", "year": "2019", "author": {"Mengshi Qi": "191/2586", "Yunhong Wang": "95/6251", "Jie Qin": "30/5781", "Annan Li": "91/411"}, "abstract": "\nIn recent years, scene parsing has captured increasing attention in computer vision. Previous works have demonstrated promising performance in this task. However, they mainly utilize holistic features, whilst neglecting the rich semantic knowledge and inter-object relationships in the scene. In addition, these methods usually require a large number of pixel-level annotations, which is too expensive in practice. In this paper, we propose a novel Knowledge Embedded Generative Adversarial Networks, dubbed as KE-GAN, to tackle the challenging problem in a semi-supervised fashion. KE-GAN captures semantic consistencies of different categories by devising a Knowledge Graph from the large-scale text corpus. In addition to readily-available unlabeled data, we generate synthetic images to unveil rich structural information underlying the images. Moreover, a pyramid architecture is incorporated into the discriminator to acquire multi-scale contextual information for better parsing results. Extensive experimental results on four standard benchmarks demonstrate that KE-GAN is capable of improving semantic consistencies and learning better representations for scene parsing, resulting in the state-of-the-art performance."}, "conf/eccv/QiQLWLG18": {"title": "stagNet: An Attentive Semantic RNN for Group Activity Recognition.", "url": "https://doi.org/10.1007/978-3-030-01249-6_7", "year": "2018", "author": {"Mengshi Qi": "191/2586", "Jie Qin": "30/5781", "Annan Li": "91/411", "Yunhong Wang": "95/6251", "Jiebo Luo": "25/5545", "Luc Van Gool": "61/5017"}, "abstract": "Group activity recognition plays a fundamental role in a variety of applications, e.g. sports video analysis and intelligent surveillance. How to model the spatio-temporal contextual information in a scene still remains a crucial yet challenging issue. We propose a novel attentive semantic recurrent neural network (RNN), dubbed as stagNet, for understanding group activities in videos, based on the spatio-temporal attention and semantic graph. A semantic graph is explicitly modeled to describe the spatial context of the whole scene, which is further integrated with the temporal factor via structural-RNN. Benefiting from the ‘factor sharing’ and ‘message passing’ mechanisms, our model is capable of extracting discriminative spatio-temporal features and capturing inter-group relationships. Moreover, we adopt a spatio-temporal attention model to attend to key persons/frames for improved performance. Two widely-used datasets are employed for performance evaluation, and the extensive results demonstrate the superiority of our method.KeywordsGroup activity recognitionSpatio-temporal attentionSemantic graphScene understanding"}, "conf/mm/QiWLL18": {"title": "Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks.", "url": "https://doi.org/10.1145/3265845.3265851", "year": "2018", "author": {"Mengshi Qi": "191/2586", "Yunhong Wang": "95/6251", "Annan Li": "91/411", "Jiebo Luo": "25/5545"}, "abstract": "\n\t\tSports video captioning is a task of automatically generating a textual description for sports events (e.g. football, basketball or volleyball games). Although previous works have shown promising performance in producing the coarse and general description of a video, it is still quite challenging to caption a sports video with multiple fine-grained player's actions and complex group relationship among players. In this paper, we present a novel hierarchical recurrent neural network (RNN) based framework with an attention mechanism for sports video captioning. A motion representation module is proposed to extract individual pose attribute and group-level trajectory cluster information. Moreover, we introduce a new dataset called Sports Video Captioning Dataset-Volleyball for evaluation. We evaluate our proposed model over two public datasets and our new dataset, and the experimental results demonstrate that our method outperforms the state-of-the-art methods.\n\t"}, "conf/mm/QiWL17": {"title": "Online Cross-Modal Scene Retrieval by Binary Representation and Semantic Graph.", "url": "https://doi.org/10.1145/3123266.3123311", "year": "2017", "author": {"Mengshi Qi": "191/2586", "Yunhong Wang": "95/6251", "Annan Li": "91/411"}, "abstract": "\n\t\tIn recent years, cross-modal scene retrieval has attracted more attention. However, most existing approaches neglect the semantic relationship between objects in a scene together with the embedded spatial layouts. Moreover, these methods mostly apply the batch learning strategy, which is not suitable for processing streaming data. To address the aforementioned problems, we propose a new framework for online cross-modal scene retrieval based on binary representations and semantic graph. Specially, we adopt the cross-modal hashing based on the quantization loss of different modalities. By introducing the semantic graph, we are able to extract wealthy semantics and measure their correlation across different modalities. Further more, we propose a two-step optimization procedure based on stochastic gradient descent for online update. Experimental results on four datasets show the superiority of our approach over the state-of-the-art.\n\t"}, "conf/icip/QiW16": {"title": "DEEP-CSSR: Scene classification using category-specific salient region with deep features.", "url": "https://doi.org/10.1109/ICIP.2016.7532517", "year": "2016", "author": {"Mengshi Qi": "191/2586", "Yunhong Wang": "95/6251"}, "abstract": " Abstract:Researches in neuroscience and biological vision have shown that the bio-inspired methods have excellent recognition performance, such as the salient detection, artificial neural network and the ganglion cell inspired image feature. In this paper, we introduce a novel framework towards scene classification using category-specific salient region(CSSR) with deep CNN features, called Deep-CSSR. Firstly, by using the salient region detection algorithm, we extract a set of image patches which contain the salient regions. Also we apply DERF, a novel bio-inspired image descriptor, to represent the salient patches and clustering all of them to remove the outliers. Then we learn the CSSR filters and construct the CSSR representation. Further more, we do scene image classification using CSSR representation concatenate with the deep CNN features extracted from the whole images. By using this new pipeline, we obtain better results than recent methods over MIT Indoor 67 and Sun397 databases."}, "journals/corr/abs-2203-10856": {"title": "RGB-Depth Fusion GAN for Indoor Depth Completion.", "url": "https://doi.org/10.48550/arXiv.2203.10856", "year": "2022", "author": {"Haowen Wang": "158/7000", "Mingyuan Wang": "216/6341", "Zhengping Che": "160/5944", "Zhiyuan Xu": "42/2832", "Xiuquan Qiao": "66/4187", "Mengshi Qi": "191/2586", "Feifei Feng": "27/4916", "Jian Tang": "181/2667"}, "abstract": "\n      Abstract:  The raw depth image captured by the indoor depth sensor usually has an\nextensive range of missing depth values due to inherent limitations such as the\ninability to perceive transparent objects and limited distance range. The\nincomplete depth map burdens many downstream vision tasks, and a rising number\nof depth completion methods have been proposed to alleviate this issue. While\nmost existing methods can generate accurate dense depth maps from sparse and\nuniformly sampled depth maps, they are not suitable for complementing the large\ncontiguous regions of missing depth values, which is common and critical. In\nthis paper, we design a novel two-branch end-to-end fusion network, which takes\na pair of RGB and incomplete depth images as input to predict a dense and\ncompleted depth map. The first branch employs an encoder-decoder structure to\nregress the local dense depth values from the raw depth map, with the help of\nlocal guidance information extracted from the RGB image. In the other branch,\nwe propose an RGB-depth fusion GAN to transfer the RGB image to the\nfine-grained textured depth map. We adopt adaptive fusion modules named W-AdaIN\nto propagate the features across the two branches, and we append a confidence\nfusion head to fuse the two outputs of the branches for the final depth map.\nExtensive experiments on NYU-Depth V2 and SUN RGB-D demonstrate that our\nproposed method clearly improves the depth completion performance, especially\nin a more realistic setting of indoor environments with the help of the pseudo\ndepth map.\n\n    "}, "journals/corr/abs-2205-00214": {"title": "Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer.", "url": "https://doi.org/10.48550/arXiv.2205.00214", "year": "2022", "author": {"Wulian Yun": "319/5424", "Mengshi Qi": "191/2586", "Chuanming Wang": "266/3767", "Huiyuan Fu": "39/10696", "Huadong Ma": "04/6217"}, "abstract": "\n      Abstract:  Video denoising aims to recover high-quality frames from the noisy video.\nWhile most existing approaches adopt convolutional neural networks(CNNs) to\nseparate the noise from the original visual content, however, CNNs focus on\nlocal information and ignore the interactions between long-range regions.\nFurthermore, most related works directly take the output after spatio-temporal\ndenoising as the final result, neglecting the fine-grained denoising process.\nIn this paper, we propose a Dual-stage Spatial-Channel Transformer (DSCT) for\ncoarse-to-fine video denoising, which inherits the advantages of both\nTransformer and CNNs. Specifically, DSCT is proposed based on a progressive\ndual-stage architecture, namely a coarse-level and a fine-level to extract\ndynamic feature and static feature, respectively. At both stages, a\nSpatial-Channel Encoding Module(SCEM) is designed to model the long-range\ncontextual dependencies at spatial and channel levels. Meanwhile, we design a\nMulti-scale Residual Structure to preserve multiple aspects of information at\ndifferent stages, which contains a Temporal Features Aggregation Module(TFAM)\nto summarize the dynamic representation. Extensive experiments on four publicly\navailable datasets demonstrate our proposed DSCT achieves significant\nimprovements compared to the state-of-the-art methods.\n\n    "}, "journals/tip/QiQYWL21": {"title": "Semantics-Aware Spatial-Temporal Binaries for Cross-Modal Video Retrieval.", "url": "https://doi.org/10.1109/TIP.2020.3048680", "year": "2021", "author": {"Mengshi Qi": "191/2586", "Jie Qin": "30/5781", "Yi Yang 0001": "33/4854-1", "Yunhong Wang": "95/6251", "Jiebo Luo": "25/5545"}, "abstract": " Abstract:With the current exponential growth of video-based social networks, video retrieval using natural language is receiving ever-increasing attention. Most existing approaches tackle this task by extracting individual frame-level spatial features to represent the whole video, while ignoring visual pattern consistencies and intrinsic temporal relationships across different frames. Furthermore, the semantic correspondence between natural language queries and person-centric actions in videos has not been fully explored. To address these problems, we propose a novel binary representation learning framework, named Semantics-aware Spatial-temporal Binaries (S2\\text{S}^{2} Bin), which simultaneously considers spatial-temporal context and semantic relationships for cross-modal video retrieval. By exploiting the semantic relationships between two modalities, S2\\text{S}^{2} Bin can efficiently and effectively generate binary codes for both videos and texts. In addition, we adopt an iterative optimization scheme to learn deep encoding functions with attribute-guided stochastic training. We evaluate our model on three video datasets and the experimental results demonstrate that S2\\text{S}^{2} Bin outperforms the state-of-the-art methods in terms of various cross-modal video retrieval tasks."}, "journals/tcsv/QiWQLLG20": {"title": "stagNet: An Attentive Semantic RNN for Group Activity and Individual Action Recognition.", "url": "https://doi.org/10.1109/TCSVT.2019.2894161", "year": "2020", "author": {"Mengshi Qi": "191/2586", "Yunhong Wang": "95/6251", "Jie Qin": "30/5781", "Annan Li": "91/411", "Jiebo Luo": "25/5545", "Luc Van Gool": "61/5017"}, "abstract": " Abstract:In real life, group activity recognition plays a significant and fundamental role in a variety of applications, e.g. sports video analysis, abnormal behavior detection, and intelligent surveillance. In a complex dynamic scene, a crucial yet challenging issue is how to better model the spatio-temporal contextual information and inter-person relationship. In this paper, we present a novel attentive semantic recurrent neural network (RNN), namely, stagNet, for understanding group activities and individual actions in videos, by combining the spatio-temporal attention mechanism and semantic graph modeling. Specifically, a structured semantic graph is explicitly modeled to express the spatial contextual content of the whole scene, which is further incorporated with the temporal factor through structural-RNN. By virtue of the “factor sharing” and “message passing” mechanisms, our stagNet is capable of extracting discriminative and informative spatio-temporal representations and capturing inter-person relationships. Moreover, we adopt a spatio-temporal attention model to focus on key persons/frames for improved recognition performance. Besides, a body-region attention and a global-part feature pooling strategy are devised for individual action recognition. In experiments, four widely-used public datasets are adopted for performance evaluation, and the extensive results demonstrate the superiority and effectiveness of our method."}, "journals/tcsv/QiWLL20": {"title": "Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling.", "url": "https://doi.org/10.1109/TCSVT.2019.2921655", "year": "2020", "author": {"Mengshi Qi": "191/2586", "Yunhong Wang": "95/6251", "Annan Li": "91/411", "Jiebo Luo": "25/5545"}, "abstract": " Abstract:Sports video captioning refers to the task of automatically generating a textual description for sports events (football, basketball, or volleyball games). Although a great deal of previous work has shown promising performance in producing a coarse and a general description of a video but lack of professional sports knowledge, it is still quite challenging to caption a sports video with multiple fine-grained player's actions and complex group relationship between players. In this paper, we present a novel hierarchical recurrent neural network-based framework with an attention mechanism for sports video captioning, in which a motion representation module is proposed to capture individual pose attribute and dynamical trajectory cluster information with extra professional sports knowledge, and a group relationship module is employed to design a scene graph for modeling players' interaction by a gated graph convolutional network. Moreover, we introduce a new dataset called sports video captioning dataset-volleyball for evaluation. The proposed model is evaluated on three widely adopted public datasets and our collected new dataset, on which the effectiveness of our method is well demonstrated."}, "journals/tip/QiWLL20": {"title": "STC-GAN: Spatio-Temporally Coupled Generative Adversarial Networks for Predictive Scene Parsing.", "url": "https://doi.org/10.1109/TIP.2020.2983567", "year": "2020", "author": {"Mengshi Qi": "191/2586", "Yunhong Wang": "95/6251", "Annan Li": "91/411", "Jiebo Luo": "25/5545"}, "abstract": " Abstract:Predictive scene parsing is a task of assigning pixel-level semantic labels to a future frame of a video. It has many applications in vision-based artificial intelligent systems, e.g., autonomous driving and robot navigation. Although previous work has shown its promising performance in semantic segmentation of images and videos, it is still quite challenging to anticipate future scene parsing with limited annotated training data. In this paper, we propose a novel model called STC-GAN, Spatio-Temporally Coupled Generative Adversarial Networks for predictive scene parsing, which employ both convolutional neural networks and convolutional long short-term memory (LSTM) in the encoder-decoder architecture. By virtue of STC-GAN, both spatial layout and semantic context can be captured by the spatial encoder effectively, while motion dynamics are extracted by the temporal encoder accurately. Furthermore, a coupled architecture is presented for establishing joint adversarial training where the weights are shared and features are transformed in an adaptive fashion between the future frame generation model and predictive scene parsing model. Consequently, the proposed STC-GAN is able to learn valuable features from unlabeled video data. We evaluate our proposed STC-GAN on two public datasets, i.e., Cityscapes and CamVid. Experimental results demonstrate that our method outperforms the state-of-the-art."}, "journals/corr/abs-2012-11260": {"title": "Unsupervised Domain Adaptation with Temporal-Consistent Self-Training for 3D Hand-Object Joint Reconstruction.", "url": "https://arxiv.org/abs/2012.11260", "year": "2020", "author": {"Mengshi Qi": "191/2586", "Edoardo Remelli": "209/5911", "Mathieu Salzmann": "18/4533", "Pascal Fua": "f/PFua"}, "abstract": "\n      Abstract:  Deep learning-solutions for hand-object 3D pose and shape estimation are now\nvery effective when an annotated dataset is available to train them to handle\nthe scenarios and lighting conditions they will encounter at test time.\nUnfortunately, this is not always the case, and one often has to resort to\ntraining them on synthetic data, which does not guarantee that they will work\nwell in real situations. In this paper, we introduce an effective approach to\naddressing this challenge by exploiting 3D geometric constraints within a cycle\ngenerative adversarial network (CycleGAN) to perform domain adaptation.\nFurthermore, in contrast to most existing works, which fail to leverage the\nrich temporal information available in unlabeled real videos as a source of\nsupervision, we propose to enforce short- and long-term temporal consistency to\nfine-tune the domain-adapted model in a self-supervised fashion. We will\ndemonstrate that our approach outperforms state-of-the-art 3D hand-object joint\nreconstruction methods on three widely-used benchmarks and will make our code\npublicly available.\n\n    "}, "journals/corr/abs-1811-10696": {"title": "Attentive Relational Networks for Mapping Images to Scene Graphs.", "url": "http://arxiv.org/abs/1811.10696", "year": "2018", "author": {"Mengshi Qi": "191/2586", "Weijian Li": "128/5278", "Zhengyuan Yang": "163/9713", "Yunhong Wang": "95/6251", "Jiebo Luo": "25/5545"}, "abstract": "\n      Abstract:  Scene graph generation refers to the task of automatically mapping an image\ninto a semantic structural graph, which requires correctly labeling each\nextracted object and their interaction relationships. Despite the recent\nsuccess in object detection using deep learning techniques, inferring complex\ncontextual relationships and structured graph representations from visual data\nremains a challenging topic. In this study, we propose a novel Attentive\nRelational Network that consists of two key modules with an object detection\nbackbone to approach this problem. The first module is a semantic\ntransformation module utilized to capture semantic embedded relation features,\nby translating visual features and linguistic features into a common semantic\nspace. The other module is a graph self-attention module introduced to embed a\njoint graph representation through assigning various importance weights to\nneighboring nodes. Finally, accurate scene graphs are produced by the relation\ninference module to recognize all entities and the corresponding relations. We\nevaluate our proposed method on the widely-adopted Visual Genome Dataset, and\nthe results demonstrate the effectiveness and superiority of our model.\n\n    "}}}