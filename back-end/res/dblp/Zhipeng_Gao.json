{"id": "25/2165", "name": "Zhipeng Gao", "Article": {"conf/bmsb/GaoYGZWC22": {"title": "Network Traffic Classification Based on Domain Adaptive Migration for Multimedia Services in Smart City Networks.", "url": "https://doi.org/10.1109/BMSB55706.2022.9828638", "year": "2022", "author": {"Bowen Gao": "274/0786", "Yang Yang": "48/450", "Zhipeng Gao": "25/2165", "Longjun Zhao": "303/1580", "Zhen Wang": "78/6727", "Dandan Cui": "15/1801"}, "abstract": " Abstract:Network traffic classification is a key technology for multimedia service management in smart city networks, which is of great significance to broadcast applications to smart cities. According to the characteristics of different network traffic to ensure the full utilization of broadband resources and improve the quality of network service. Due to the wide application of dynamic port number and stream encryption technology, the traditional network traffic classification method based on port number and deep packet inspection is no longer effective. In recent years, machine learning methods can be used to accurately identify categories of network traffic applications by labeled data. However, collecting enough labeled network traffic data is a time-consuming and difficult task, and the distribution of traffic data collected in various networks is different. In order to solve the problem of insufficient labeled samples and domain distribution differences, a network traffic classification method based on domain adaptive migration is proposed in this paper. This paper designs a new domain adaptive method, which reduces distribution differences between domains, makes the distribution of samples under the same category more compact and different application categories more separated. Then migrate the feature extraction module to the target domain. And the unlabeled data and few labeled samples are utilized to jointly classify network traffic applications using a siamese sparse denoising stacked autoencoder. Experimental results show that compared with other algorithms, the network traffic classification algorithm proposed in this paper achieves the best result."}, "conf/icait/HeYZGR22": {"title": "Network Traffic Prediction Method Based on Multi-Channel Spatial-Temporal Graph Convolutional Networks.", "url": "https://doi.org/10.1109/ICAIT56197.2022.9862813", "year": "2022", "author": {"Yechen He": "320/2372", "Yang Yang 0006": "48/450-6", "Binnan Zhao": "328/4281", "Zhipeng Gao": "25/2165", "Lanlan Rui": "03/4227"}, "abstract": " Abstract:Recent years, the use of network traffic is increasing year by year, so network traffic forecasting has ushered in new challenges. Network traffic forecasting enables network operators to adjust usage demands in real-time, thereby rationally allocating network resources. Based on the characteristics of high nonlinearity and dynamic spatial-temporal correlation of network traffic data, a graph neural network and temporal extracting modules need to be used to extract temporal and spatial features separately. Existing works usually have some common problems: (1) the use of single-channel input data makes the data feature extraction relatively single. (2) construct graphs only through spatial relationships, lacking consideration of non-spatial relationships. Therefore, we propose a new graph convolutional neural network approach: Multi-Channel Spatial-Temporal Graph Convolutional Networks. Specifically, we do time slicing in the data pre-processing stage, to slice the data with different time dimensions to construct the adjacency matrix. To construct the graph from different angles, functional adjacency matrix and temporal adjacency matrix are introduced. The model demonstrates good performance on the real-world network traffic dataset Telecom Italia."}, "conf/icait/YanYGZWC22": {"title": "Entity Extraction Algorithm of Hybrid Network based on Attention Mechanism in Smart City.", "url": "https://doi.org/10.1109/ICAIT56197.2022.9862644", "year": "2022", "author": {"Zefan Yan": "328/4626", "Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165", "Longjun Zhao": "303/1580", "Zhen Wang": "78/6727", "Dandan Cui": "15/1801"}, "abstract": " Abstract:With the continuous development of information and communication network technology, the number of devices in the network and the running data are increasing geometrically, which brings certain challenges to network configuration, operation and maintenance and management. At present, with the vigorous development of artificial intelligence technology, the concept of intelligent operation and maintenance came into being. The construction of hybrid network knowledge graph is an important part of intelligent operation and maintenance, and hybrid network entity extraction is a key step in its construction. Therefore, in order to effectively identify hybrid network entities, this paper proposes hybrid network entity extraction algorithm based on attention mechanism. Using the multi-head attention mechanism combined with the bidirectional long and short-term memory network and CRF, the accuracy of the neural network model is further improved. In order to verify the effectiveness of the method, we compare the improved algorithm in this paper with the existing classical algorithms and literature algorithms. The experiments show that the algorithm in this paper has certain effectiveness and superiority."}, "conf/iwcmc/GaoLZZ022": {"title": "Short Video List Reshuffling for Minimized Wireless Resources through Video Multicast.", "url": "https://doi.org/10.1109/IWCMC55113.2022.9824211", "year": "2022", "author": {"Zhipeng Gao": "25/2165", "Chunxi Li": "35/6163", "Yongxiang Zhao": "57/836", "Baoxian Zhang": "01/986", "Cheng Li 0005": "16/6465-5"}, "abstract": " Abstract:The explosive development of short video applications has brought severe pressure on radio resources at hotspot areas. The features of short video recommendations-and-pushing techniques provide us an opportunity to relieve the radio resource pressure via wireless multicast: An edge server can be deployed at the base station, which receives short video lists recommended by remote video server and then pushes such mobile video services to local users through wireless multicast. In this paper, we study how to reshuffle the video lists received from remote server so as to facilitate wireless multicast to maximally reduce the required wireless resource while considering the fact that a user client can only buffer one short video for watching based on off-the-shelf short video APPs. We formulate the problem of video list reshuffling for minimizing the total wireless resources consumption as an integer programming problem. We design a Minimum degree of Freedom based Maximum Filling video reshuffling algorithm (MFMF) to address this problem. MFMF moves videos from the original video lists into same sized but reshuffled video lists in a greedy manner, once for a video, whose moving can satisfy the most reshuffled video lists, and if multiple such choices exist, selects the one having the least position options. This process continues until all the videos are moved. We deduce the computation complexity of MFMF. Numerical results demonstrate the significantly high performance of MFMF."}, "conf/services/RuiCWGQW22": {"title": "Multiservice Reliability Evaluation Algorithm Considering Network Congestion and Regional Failure Based on Petri Net.", "url": "https://doi.org/10.1109/SERVICES55459.2022.00042", "year": "2022", "author": {"Lanlan Rui": "03/4227", "Xushan Chen": "162/6745", "Xiaomei Wang": "32/6877", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Shangguang Wang": "73/8637"}, "abstract": " Abstract:[J1C2 Presentation Abstract at IEEE SERVICES 2022 for IEEE Transactions on Services Computing DOI 10.1109/TSC.2019.2955486]"}, "conf/wcnc/GaoDYRZ22": {"title": "FedSeC: a Robust Differential Private Federated Learning Framework in Heterogeneous Networks.", "url": "https://doi.org/10.1109/WCNC51071.2022.9771929", "year": "2022", "author": {"Zhipeng Gao": "25/2165", "Yingwen Duan": "320/8413", "Yang Yang 0006": "48/450-6", "Lanlan Rui": "03/4227", "Chen Zhao": "81/3"}, "abstract": " Abstract:Federated learning (FL) is considered to be a promising paradigm to solve data privacy disclosure in large-scale machine learning. To further enhance the privacy protection of federated learning, prior works incorporate the differentially private data perturbation into the federated system. But it is not feasible given the impairment of the model from noise, as adding Gaussian noise to achieve differential privacy (DP) deteriorates the accuracy of the model. In particular, the assumption that the sophisticated system is homogeneous is not realistic for real scenarios. Heterogeneous networks exacerbate noise disruptions. In this paper, we present FedSeC, a novel differential private federated learning (DP-FL) framework which operates with robust convergence and high-accuracy while achieving adequate privacy protection. FedSeC improves upon naive combinations of federated learning and differential privacy approaches with an updates-based optimization of relative-staleness and semi-synchronous approach for fast convergence in heterogeneous networks. Moreover, we propose a valid client selection scheme to trade-off fair resource allocation and discriminatory incentives. Through extensive experimental validation of our method in three different heterogeneities, we show that FedSeC outperforms the previous state-of-the-art method."}, "conf/bmsb/GeZWCYG21": {"title": "Multi-dimensional Data Correlation Analysis Method Based on Neighborhood Preserving Embedding Mechanism.", "url": "https://doi.org/10.1109/BMSB53066.2021.9547142", "year": "2021", "author": {"Zhongdi Ge": "277/3052", "Longjun Zhao": "303/1580", "Zhen Wang": "78/6727", "Dandan Cui": "15/1801", "Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165"}, "abstract": " Abstract:Due to the complexity of network structure and the existence of weak nodes, the possibility of abnormal or attacked in the network is greatly increased. Therefore, it is necessary to carry out feature extraction on the multi-dimensional data of network nodes and extract the key features, to improve the speed of data analysis and find the abnormal behaviors in the network in time. In the past feature extraction methods, data fusion is often used to compress and merge multidimensional data to form new data. However, this approach cannot guarantee the influence of the original attribute dimension in the whole data set, and each dimension cannot be clearly analyzed one by one. Therefore, this paper proposes a feature extraction method based on multi-core canonical correlation analysis. The improved multi-core canonical correlation analysis algorithm is introduced into the neighborhood preserving embedding algorithm to obtain the MultiCCA-NPE feature extraction algorithm. According to the characteristics of the network operating data, in order to better extract the features, this paper first uses the improved multi-core learning CCA algorithm to calculate the correlation between the attributes of each dimension. Taking the correlation size as the basis of the NPE weight value, the improved MultiCCA-NPE algorithm is obtained, and then effective feature extraction is realized. Compared with the KCCA algorithm and the NPE-GNN algorithm, this method has a better feature extraction effect."}, "conf/bmsb/SunZWCYG21": {"title": "Fault Root Rank Algorithm Based on Random Walk Mechanism in Fault Knowledge Graph.", "url": "https://doi.org/10.1109/BMSB53066.2021.9547194", "year": "2021", "author": {"Yindong Sun": "303/1348", "Longjun Zhao": "303/1580", "Zhen Wang": "78/6727", "Dandan Cui": "15/1801", "Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165"}, "abstract": " Abstract:As the software architecture becomes more and more complex, the software architecture changes from a monolithic architecture to a microservice architecture, ensuring the continuous and stable operation of software services directly affects the income of Internet companies. Therefore, in the microservice architecture, it is important to quickly locate possible anomalies in a large number of services and provide a list of possible diagnostic paths. The most important thing is to ensure that the prediction list given is as few as possible. But it must be accurate, so as to help engineers quickly locate service problems. In this article, the problem of complex and ambiguous call relationships between microservices is addressed. Propose an algorithm for automatically constructing a microservice dependency graph based on multiple factors, and on this basis, a fault troubleshooting path generation algorithm is proposed and arranged in descending order according to the probability. The generated result is a list of troubleshooting paths for engineers to use. Finally, a typical microservice architecture website is used as a data collection site to verify the effectiveness of the algorithm in generating the failure prediction list, and the accuracy is significantly higher than that of the comparison algorithm."}, "conf/hpcc/LiWCRYG21": {"title": "Distributed pseudonym mechanism based on Consortium Blockchain.", "url": "https://doi.org/10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00084", "year": "2021", "author": {"Baohui Li": "145/9884", "Miaomiao Wang": "67/3317", "Shiyou Chen": "233/0862", "Lanlan Rui": "03/4227", "Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165"}, "abstract": " Abstract:Crowd sensing is a way to obtain multiple sensing data onto users or mobile devices and is widely used in industrial Internet, smart city, smart medical, etc. However, when users upload sensing data involving sensitive information such as a user identity and location, there will be risks of data privacy leakage and user tracking. For the problem of this scenario, we propose a blockchain-based pseudonym generation mechanism, which uses pseudonyms to realize user anonymity and user identity authentication. The average speed of pseudonym generation and authentication in the simulation experiment is about 0.1s, the experimental results verify the availability and effectiveness of the distributed pseudonym mechanism."}, "conf/ica3pp/GaoSZMZ21": {"title": "EdgeSP: Scalable Multi-device Parallel DNN Inference on Heterogeneous Edge Clusters.", "url": "https://doi.org/10.1007/978-3-030-95388-1_21", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Shan Sun": "83/8595", "Yinghan Zhang": "11/8868", "Zijia Mo": "240/6361", "Chen Zhao": "81/3"}, "abstract": "Edge computing has emerged as a promising line of research for processing large-scale data and providing low-latency services. Unfortunately, deploying deep neural networks (DNNs) on resource-limited edge devices presents unacceptable latency, hindering artificial intelligence from empowering edge devices. Prior solutions attempted to address this issue by offloading workload to the remote cloud. However, the cloud-assisted approach ignores that devices in the edge environment tend to exist as clusters. In this paper, we propose EdgeSP, a scalable multi-device parallel DNN inference framework that maximizes resource utilization of heterogeneous edge device clusters. We design a multiple fused-layer blocks parallelization strategy to reduce inter-device communication during parallel inference. Further, we add early exit branches to DNNs, empowering the device to trade-off latency and accuracy for a variety of sophisticated tasks. Experimental results show that EdgeSP enables inference latency acceleration of 2.3×−3.7×2.3×−3.7×2.3\\times -3.7\\times  for DNN inference tasks of various scales and outperforms the existing naive parallel inference method. Additionally, EdgeSP can provide high accuracy inference services under various latency requirements.KeywordsEdge computingEdge intelligenceParallel inferenceDeep neural networksEarly-exitInternet of Things"}, "conf/icc/LinGXWM0RGW21": {"title": "A Model Training Mechanism based on Onchain and Offchain Collaboration for Edge Computing.", "url": "https://doi.org/10.1109/ICC42927.2021.9500604", "year": "2021", "author": {"Yijing Lin": "275/1523", "Zhipeng Gao": "25/2165", "Kaile Xiao": "194/4868", "Qian Wang": "75/5723", "Zijia Mo": "240/6361", "Yang Yang 0006": "48/450-6", "Lanlan Rui": "03/4227", "Haisheng Guo": "299/0189", "Dezheng Wang": "198/7365"}, "abstract": " Abstract:Blockchain as a new decentralized chain structure can be used in edge computing to solve the security issue caused by edge nodes in model training. However, large amounts of data exchanges in the process of model training of edge computing reduce the performance of blockchain, and meanwhile, the block needed to be saved in the edge node challenges storage capacity of the edge node. Therefore, in the paper we propose a safe and efficient model training mechanism based on onchain and offchain collaboration. In the mechanism, edge nodes train models locally, store the model parameters in offchain and only return identifiers for model aggregation. By the method, the storage pressure of the edge node is reduced and the efficiency of executing consensus algorithms are increased. Moreover, in the mechanism we design a reputation evaluation model based on confidence factors to avoid the uploading of random and wrong data of edge nodes. Evaluation results show that our schemes can reduce the average delay and resources consumption, increase transaction throughput and maintain security compared with a state-of-the-art scheme."}, "conf/iciai/GaoHZ21": {"title": "A double-phase search algorithm for sub-optimal path finding.", "url": "https://doi.org/10.1145/3461353.3461374", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Junmeng Huang": "252/1018", "Chen Zhao": "81/3"}, "abstract": "\n\t\t Traditional optimal path finding algorithms are usually too complex for real world problems, motivating the need to find path with sub-optimality. Typically suboptimal algorithms use a single admissible heuristic value to decide how to find a path and bound the cost. Algorithms like Weighted A*(WA*), Convex upward parabola(XUP) and Convex downward parabola(XDP) have overcome the node re-expansion problem during search. However, this re-incur a balance between the quality of path and the speed of search. In this paper, we research the process of extending and put forward an algorithm that would more efficiently operate the search while on the same time not lower the quality of path. This algorithm includes two phase of search, the first phase is to fasten the process of path finding, while the second phase is to guarantee the quality of path. In most maps we choose from Dragon Age Origins(DAO), our algorithm performs better than WA*.\n\t"}, "conf/im/ZhangY00021": {"title": "A Computation Offloading Mechanism Based on Sharable Cache in Smart Community.", "url": "https://ieeexplore.ieee.org/document/9464032", "year": "2021", "author": {"Yixin Zhang": "14/5888", "Yong Yan": "70/374", "Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:In the smart community, many smart devices often offload computation tasks that use related data to edge servers. In this case, unnecessary data transmissions will occur, resulting in waste of energy resources and increased latency. Therefore, we consider adding a shareable cache to the edge server, allowing different computing tasks to share the original data file of computation task, and further reducing the energy consumption and latency. In this paper, we propose a computation offloading mechanism based on edge server shareable cache. We first propose a four-tier architecture. Subsequently, a resource constraint optimization model combining communication, computing, and caching is established. Then, we design a multi-factor-based edge server sharable cache management and update mechanism (MFBM), and propose a two-scenario computation offloading mechanism (TSCOM). Finally, simulation results show that the mechanism proposed in this paper is effective."}, "conf/kbse/HuG00Y21": {"title": "Automating User Notice Generation for Smart Contract Functions.", "url": "https://doi.org/10.1109/ASE51524.2021.9678552", "year": "2021", "author": {"Xing Hu 0008": "49/10052-8", "Zhipeng Gao": "25/2165", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "Xiaohu Yang": "95/6761"}, "abstract": " Abstract:Smart contracts have obtained much attention and are crucial for automatic financial and business transactions. For end-users who have never seen the source code, they can read the user notice shown in end-user client to understand what a transaction does of a smart contract function. However, due to time constraints or lack of motivation, user notice is often missing during the development of smart contracts. For end-users who lack the information of the user notices, there is no easy way for them to check the code semantics of the smart contracts. Thus, in this paper, we propose a new approach SMARTDOC to generate user notice for smart contract functions automatically. Our tool can help end-users better understand the smart contract and aware of the financial risks, improving the users’ confidence on the reliability of the smart contracts. SMARTDOC exploits the Transformer to learn the representation of source code and generates natural language descriptions from the learned representation. We also integrate the Pointer mechanism to copy words from the input source code instead of generating words during the prediction process. We extract 7,878 〈function, notice〉 pairs from 54,739 smart contracts written in Solidity. Due to the limited amount of collected smart contract functions (i.e., 7,878 functions), we exploit a transfer learning technique to utilize the learned knowledge to improve the performance of SMARTDOC. The learned knowledge obtained by the pre-training on a corpus of Java code, that has similar characteristics as Solidity code. The experimental results show that our approach can effectively generate user notice given the source code and significantly outperform the state-of-the-art approaches. To investigate human perspectives on our generated user notice, we also conduct a human evaluation and ask participants to score user notice generated by different approaches. Results show that SMARTDOC outperforms baselines from three aspects, naturalness, inf..."}, "conf/sigsoft/Gao00G021": {"title": "Automating the removal of obsolete TODO comments.", "url": "https://doi.org/10.1145/3468264.3468553", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John C. Grundy": "g/JohnCGrundy", "Thomas Zimmermann 0001": "19/3622-1"}, "abstract": "\n\t\tTODO comments are very widely used by software developers to describe their pending tasks during software development. However, after performing the task developers sometimes neglect or simply forget to remove the TODO comment, resulting in obsolete TODO comments. These obsolete TODO comments can confuse development teams and may cause the introduction of bugs in the future, decreasing the software's quality and maintainability. Manually identifying obsolete TODO comments is time-consuming and expensive. It is thus necessary to detect obsolete TODO comments and remove them automatically before they cause any unwanted side effects. In this work, we propose a novel model, named TDCleaner, to identify obsolete TODO comments in software projects. TDCleaner can assist developers in just-in-time checking of TODO comments status and avoid leaving obsolete TODO comments. Our approach has two main stages: offline learning and online prediction. During offline learning, we first automatically establish <code_change, todo_comment, commit_msg> training samples and leverage three neural encoders to capture the semantic features of TODO comment, code change and commit message respectively. TDCleaner then automatically learns the correlations and interactions between different encoders to estimate the final status of the TODO comment. For online prediction, we check a TODO comment's status by leveraging the offline trained model to judge the TODO comment's likelihood of being obsolete. We built our dataset by collecting TODO comments from the top-10,000 Python and Java Github repositories and evaluated TDCleaner on them. Extensive experimental results show the promising performance of our model over a set of benchmarks. We also performed an in-the-wild evaluation with real-world software projects, we reported 18 obsolete TODO comments identified by TDCleaner to Github developers and 9 of them have already been confirmed and removed by the developers, demonstrating the practical usage of our approach.\n\t"}, "conf/sigsoft/Gao00GL21": {"title": "Code2Que: a tool for improving question titles from mined code snippets in stack overflow.", "url": "https://doi.org/10.1145/3468264.3473114", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John C. Grundy": "g/JohnCGrundy", "Yuan-Fang Li": "20/2537"}, "abstract": "\n\t\tStack Overflow is one of the most popular technical Q&A sites used by software developers. Seeking help from Stack Overflow has become an essential part of software developers’ daily work for solving programming-related questions. Although the Stack Overflow community has provided quality assurance guidelines to help users write better questions, we observed that a significant number of questions submitted to Stack Overflow are of low quality. In this paper, we introduce a new web-based tool, Code2Que, which can help developers in writing higher quality questions for a given code snippet. Code2Que consists of two main stages: offline learning and online recommendation. In the offline learning phase, we first collect a set of good quality ⟨code snippet, question⟩ pairs as training samples. We then train our model on these training samples via a deep sequence-to-sequence approach, enhanced with an attention mechanism, a copy mechanism and a coverage mechanism. In the online recommendation phase, for a given code snippet, we use the offline trained model to generate question titles to assist less experienced developers in writing questions more effectively. To evaluate Code2Que, we first sampled 50 low quality ⟨code snippet, question⟩ pairs from the Python and Java datasets on Stack Overflow. Then we conducted a user study to evaluate the question titles generated by our approach as compared to human-written ones using three metrics: Clearness, Fitness and Willingness to Respond. Our experimental results show that for a large number of low-quality questions in Stack Overflow, Code2Que can improve the question titles in terms of Clearness, Fitness and Willingness measures.\n\t"}, "conf/trustcom/GaoZLR0ZM21": {"title": "Select-Storage: A New Oracle Design Pattern on Blockchain.", "url": "https://doi.org/10.1109/TrustCom53373.2021.00159", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Zijian Zhuang": "315/6081", "Yijing Lin": "275/1523", "Lanlan Rui": "03/4227", "Yang Yang 0006": "48/450-6", "Chen Zhao": "81/3", "Zijia Mo": "240/6361"}, "abstract": " Abstract:The blockchain system allows various trans-actions and information storage to be executed in a decentralized manner, while smart contracts require multiple nodes to be executed in the local sandbox environment according to preset settings to ensure the consistency of each node, which makes smart contracts unable to proactively obtain data from the outside world. Decentralized oracle can realize the acquisition of off-chain data with a low speed under the premise of ensuring the decentralization of the blockchain. Some oracles use on-chain data storage and maintenance to speed up data acquisition, but this will face higher costs of data storage and maintenance, so current oracles cannot simultaneously ensure privacy and security while taking into account execution cost and processing speed. In this article, we propose Select-Storage, a new oracle design pattern to achieve low operating cost and high processing speed without compromising security. Through experimental analysis, and comparison with other design patterns in processing time and on-chain and off-chain call costs, we have proved the superiority of the Select-Storage design pattern."}, "conf/trustcom/GaoQZYML21": {"title": "FedIM: An Anti-attack Federated Learning Based on Agent Importance Aggregation.", "url": "https://doi.org/10.1109/TrustCom53373.2021.00205", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Chenhao Qiu": "315/5981", "Chen Zhao": "81/3", "Yang Yang 0006": "48/450-6", "Zijia Mo": "240/6361", "Yijing Lin": "275/1523"}, "abstract": " Abstract:Federated learning (FL) is a distributed framework for machine learning (ML) model training. Training agents upload local model parameters rather than original training data, and the central server performs parameter aggregation. FL can protect user data privacy and break the information island when training the ML model. Federated Average (FedAvg) is an aggregation method commonly used in the FL training task. The central server calculates the mean value of the local model parameters to obtain the new global parameters. FedAvg assumes that all the training agents are honest, which means the central server lacks terminal agents' knowability. When there are attackers in the training agents, the global model's performance may be deeply affected, and the training task cannot be completed normally. To solve this problem, we propose a Federated Learning method with aggregation based on the Importance of training agent (FedIM), in which the central server performs pre-evaluation on the agent parameters before aggregation, calculates the weights of parameters according to the historical behavior records of training terminals and performs federated aggregation to improve the anti-poisoning ability of learning task. Experiments show that our method can effectively improve the global model's anti-poisoning ability and accelerate the training speed compared with the FedAvg method when malicious agents are involved."}, "conf/wcnc/GaoMZMQY21": {"title": "Triple-partition Network: Collaborative Neural Network based on the 'End Device-Edge-Cloud'.", "url": "https://doi.org/10.1109/WCNC49053.2021.9417243", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Dong Miao": "50/5069", "Langcheng Zhao": "291/7149", "Zijia Mo": "240/6361", "Guangpeng Qi": "291/7127", "Liang Yan": "69/2422"}, "abstract": " Abstract:The traditional centralized data processing model represented by cloud computing cannot meet the data processing requirements that are gradually tending to the edge. Therefore, a new distributed computing model coordinated by the end devices, edges and cloud has become the main development direction. However, artificial intelligence algorithms that are widely used in cloud-only approach are difficult to embed in resource-constrained distributed frameworks. To address this issue, we propose Triple-partition Network, a neural network model augment with three exit points. The structure of three exit points allows to segment the traditional neural network and deploying them on the end devices, edges, and cloud. By setting up suitable exit points through the Entropy Topsis comprehensive evaluation model, part of the data can exit the network in advance to improve the efficiency of computing services. In this experiment, the classic neural networks (Alexnet, Resnet) are used to study the Triple-partition Network on a state-of-art platform and show that trained Triple-partition Network can greatly reduce the end-to-end latency by over 3x while achieving high accuracy."}, "conf/icdcs/GaoLXW20": {"title": "Cross-chain Oracle Based Data Migration Mechanism in Heterogeneous Blockchains.", "url": "https://doi.org/10.1109/ICDCS47774.2020.00162", "year": "2020", "author": {"Zhipeng Gao": "25/2165", "Hongli Li": "54/7821", "Kaile Xiao": "194/4868", "Qian Wang": "75/5723"}, "abstract": " Abstract:As things currently stand, the blockchain industry is siloed among many different platforms and protocols resulting in various islands of blockchains. Restrictions regarding assets transfers and data migration between different blockchains reduce the usability and comfort of users, and hinder novel developments within the blockchain ecosystem. Interoperability will be the main topics of next-generation blockchain technologies. In this paper, we focus on how to enable interoperability between two heterogeneous blockchains in the context of data migration. We first build an cross-chain data migration architecture based on data migration oracle. Second, we design a data migration mechanism based on former architecture. By employing the proposed data migration architecture, it is equivalent to opening a secure channel between two heterogeneous blockchains allowing secure data migration. By applying data migration mechanism, the confidentiality, integrity and security of migrated data can be well guaranteed."}, "conf/kbse/Gao20a": {"title": "When Deep Learning Meets Smart Contracts.", "url": "https://doi.org/10.1145/3324884.3418918", "year": "2020", "author": {"Zhipeng Gao": "25/2165"}, "abstract": "\n\t\tEthereum has become a widely used platform to enable secure, Blockchain-based financial and business transactions. However, many identified bugs and vulnerabilities in smart contracts have led to serious financial losses, which raises serious concerns about smart contract security. Thus, there is a significant need to better maintain smart contract code and ensure its high reliability. In this research: (1) Firstly, we propose an automated deep learning based approach to learn structural code embeddings of smart contracts in Solidity, which is useful for clone detection, bug detection and contract validation on smart contracts. We apply our approach to more than 22K solidity contracts collected from the Ethereum blockchain, results show that the clone ratio of solidity code is at around 90%, much higher than traditional software. We collect a list of 52 known buggy smart contracts belonging to 10 kinds of common vulnerabilities as our bug database. Our approach can identify more than 1000 clone related bugs based on our bug databases efficiently and accurately. (2) Secondly, according to developers' feedback, we have implemented the approach in a web-based tool, named SmartEmbed, to facilitate Solidity developers for using our approach. Our tool can assist Solidity developers to efficiently identify repetitive smart contracts in the existing Ethereum blockchain, as well as checking their contract against a known set of bugs. which can help to improve the users' confidence in the reliability of the contract. We optimize the implementations of SmartEmbed which is sufficient in supporting developers in real-time for practical uses. The Ethereum ecosystem as well as the individual Solidity developer can both benefit from our research. SmartEmbed website: http://www.smartembed.tools Demo video: https://youtu.be/o9ylyOpYFq8 Replication package: https://github.com/beyondacm/SmartEmbed\n\t"}, "conf/ficloud/GaoYXM0019": {"title": "A Real-Time Task Offloading Strategy Based on Double Auction for Optimal Resource Allocation in Edge Computing.", "url": "https://doi.org/10.1109/FiCloud.2019.00010", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Congcong Yao": "252/1219", "Kaile Xiao": "194/4868", "Zijia Mo": "240/6361", "Qian Wang 0015": "75/5723-15", "Yang Yang 0006": "48/450-6"}, "abstract": " Abstract:Task offloading in edge computing becomes an effective method to extend the computation ability of user equipments (UEs), via migrating computation-intensive applications from UEs to edge servers. However, not only locality-aware resource allocation for UEs and various edge computing services providers (ESPs) but also network economics for profit-driven ESPs and UEs is still a big challenge in task offloading. In this paper, we propose an edge computing resource allocation model based on the continuous-cycle double auction mechanism (RABDA). Considering the emergency of task offloaded, we also propose real-time offloading strategy (RTOS) to ensure tasks are processed efficiently. We use genetic algorithm to determine the winner ESPs which are responsible for providing computational resources to UEs, and verify the performance of our algorithm by contrast experiment. The simulation results show that our algorithm can improve satisfaction between UEs and ESPs, and it has higher resource utilization than the existing algorithm."}, "conf/globecom/GaoXWH0R19": {"title": "Multi-Source Feedback Based Light-Weight Trust Mechanism for Edge Computing.", "url": "https://doi.org/10.1109/GLOBECOM38437.2019.9014281", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Chenxi Xia": "223/3542", "Qian Wang": "75/5723", "Junmeng Huang": "252/1018", "Yang Yang 0006": "48/450-6", "Lanlan Rui": "03/4227"}, "abstract": " Abstract:To alleviate the security concerns caused by the openness of the edge computing network and meet the time-sensitive requirements of the edge devices' collaborative tasks, an effective trust evaluation mechanism is needed urgently to resist multi-attacks from various malicious devices. In this work, a light- weight trust mechanism based on multi-source feedback is proposed for edge computing. First, we design a light-weight data-processing algorithm executed in edge brokers and edge devices, which could reduce the data transmission pressure in communication networks effectively and work efficiently in large-scale edge networks. Then, a comprehensive evaluation method is designed for edge brokers based on the Dempster Shafer theory and multi-source feedback mechanism, which makes our mechanism more reliable and pluralistic when resisting various multi-attacks at the same time. At last, we originally develop a neural network in the centralized cloud to update edge brokers' hyper-parameters and weights of the key factors by auditing trust evaluation results uploaded from the edge network according to deep Q-learning algorithm, which are usually weighted manually and subjectively in traditional schemes. The experimental results show the proposed trust mechanism outperforms existing methods in reliability and calculation efficiency when resisting various malicious attacks."}, "conf/ica3pp/GaoZX0M19": {"title": "A Data Uploading Strategy in Vehicular Ad-hoc Networks Targeted on Dynamic Topology: Clustering and Cooperation.", "url": "https://doi.org/10.1007/978-3-030-38961-1_21", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Xinyue Zheng": "248/8035", "Kaile Xiao": "194/4868", "Qian Wang 0015": "75/5723-15", "Zijia Mo": "240/6361"}, "abstract": "Vehicular Ad-hoc Network (VANET) is a special network composed of driving vehicles with dynamic topology. Data uploading from the VANET to the computation server is a challenging issue due to the high mobility of vehicles. By introducing the Mobile Edge Computing (MEC) server deployed on the roadside, this paper proposes a stable clustering strategy based on adjacency screening and designs an Intra-Cluster Data Uploading (ICDU) algorithm to improve the efficiency of data uploading in a dynamic environment. The connection lifetime between vehicles is taken as a key indicator for our proposed clustering strategy to form stable clusters. After the formation of clusters, the ICDU algorithm plans a stable path for vehicles in a cluster to upload data in a cooperative method. Extensive simulation results show that the proposed clustering strategy performs better in terms of the clustering stability compared with Vehicular Multi-hop algorithm for Stable Clustering (VMaSC) and the greedy clustering strategy. The results also prove that our proposed ICDU algorithm outperforms the self-uploading algorithm and can achieve a larger data uploading throughput in the dense scenario compared with the greedy-uploading algorithm.KeywordsVehicular Ad-hoc NetworkDynamic topologyMobile Edge ComputingClusteringData uploading"}, "conf/iciot/GaoZXXM0019": {"title": "A Credible and Lightweight Multidimensional Trust Evaluation Mechanism for Service-Oriented IoT Edge Computing Environment.", "url": "https://doi.org/10.1109/ICIOT.2019.00035", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Wensi Zhao": "248/3027", "Chenxi Xia": "223/3542", "Kaile Xiao": "194/4868", "Zijia Mo": "240/6361", "Qian Wang 0015": "75/5723-15", "Yang Yang 0006": "48/450-6"}, "abstract": " Abstract:To meet the increasing need of a reliable and service-oriented IoT edge environment, a credible and lightweight trust evaluation mechanism is expected urgently. Due to the task openness, device heterogeneity and resource dynamics of the IoT edge computing, the lack of trust is inherent, which hides the public from excellent computing services. Therefore, in this paper, a service-driven collaboration mechanism based on multidimensional trust evaluation of IoT devices is proposed. Due to the design of the multidimensional trust evaluation mechanism, the service's quality affected by low-capability devices can be improved and the performance against malicious attacks could be well improved as well, which means this trust evaluation scheme is robust. With the double-filtering design based on K-means clustering algorithm in this mechanism, the feedback with low similarity of current task context and the feedback from malicious devices could be filtered effectively, which makes the trust evaluation mechanism more credible. In addition, owing to the application of low-overhead algorithms, our evaluation mechanism is lightweight, which could have an outstanding efficiency in large-scale IoT edge computing. According to the experiment results under the simulation environment, the credibility and robustness of the IoT edge environment could be obviously improved with the application of our trust evaluation mechanism."}, "conf/icnp/GaoXJWH0R19": {"title": "A Light-weight Trust Mechanism for Cloud-Edge Collaboration Framework.", "url": "https://doi.org/10.1109/ICNP.2019.8888037", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Chenxi Xia": "223/3542", "Zhuojun Jin": "252/1063", "Qian Wang 0015": "75/5723-15", "Junmeng Huang": "252/1018", "Yang Yang 0006": "48/450-6", "Lanlan Rui": "03/4227"}, "abstract": " Abstract:With the development of the edge computing and cloud computing technology, the cloud-edge collaboration framework is proposed as a new effective computing architecture and applied in many fields. However, due to the openness of the edge networks, the security of cloud-edge framework is an unavoidable problem and most recent trust mechanism could not resist mixed malicious attacks at the same time. In this work, a light-weight and reliable trust mechanism based on the improved LightGBM algorithm is originally proposed to evaluate the credibility of edge devices. First, we design a light-weight trust mechanism for edge devices to process raw interaction data and extract trust features, which reduces the amount of data transmission and the pressure on the communication networks. In addition, an evaluation algorithm based on the entropy weight method (EWM) and punishment factors is designed for edge brokers to distinguish the malicious devices from the normal ones, which performs great against mixed malicious attacks. At last, we propose an improved LightGBM algorithm developed in the centralized cloud to learn other researchers' evaluation methods and check the evaluation uploaded from edge brokers, which could make the punishment factors of edge networks weighted adaptively with the change of edge networks. The experimental results show the proposed trust mechanism outperforms existing methods in the accuracy and discriminating speed under mixed malicious attacks."}, "conf/icsm/GaoJJ00G19": {"title": "SmartEmbed: A Tool for Clone and Bug Detection in Smart Contracts through Structural Code Embedding.", "url": "https://doi.org/10.1109/ICSME.2019.00067", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Vinoj Jayasundara": "211/4067", "Lingxiao Jiang": "82/3572", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John C. Grundy": "g/JohnCGrundy"}, "abstract": " Abstract:Ethereum has become a widely used platform to enable secure, Blockchain-based financial and business transactions. However, a major concern in Ethereum is the security of its smart contracts. Many identified bugs and vulnerabilities in smart contracts not only present challenges to the maintenance of blockchain, but also lead to serious financial loses. There is a significant need to better assist developers in checking smart contracts and ensuring their reliability. In this paper, we propose a web service tool, named SmartEmbed, which can help Solidity developers to find repetitive contract code and clone-related bugs in smart contracts. Our tool is based on code embeddings and similarity checking techniques. By comparing the similarities among the code embedding vectors for existing solidity code in the Ethereum blockchain and known bugs, we are able to efficiently identify code clones and clone-related bugs for any solidity code given by users, which can help to improve the users' confidence in the reliability of their code. In addition to the uses by individual developers, SmartEmbed can also be applied to studies of smart contracts in a large scale. When applied to more than 22K solidity contracts collected from the Ethereum blockchain, we found that the clone ratio of solidity code is close to 90%, much higher than traditional software, and 194 clone-related bugs can be identified efficiently and accurately based on our small bug database with a precision of 96%."}, "conf/sose/GaoJXWMY19": {"title": "Deep Reinforcement Learning Based Service Migration Strategy for Edge Computing.", "url": "https://doi.org/10.1109/SOSE.2019.00025", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Qidong Jiao": "240/6366", "Kaile Xiao": "194/4868", "Qian Wang 0015": "75/5723-15", "Zijia Mo": "240/6361", "Yang Yang 0006": "48/450-6"}, "abstract": " Abstract:Edge Computing (EC) is an emerging technology to cope with the unprecedented growth of user demands for access to low-latency computation and content data. However, user mobility and limited coverage of Edge Computing Server (ECS) result in service discontinuity and reduce Quality of Service (QoS). Service migration has a great potential to address this issue. In the scenario of service migration, how to choose the optimal migration strategy and communication strategy is a key challenge. In this paper, we innovatively propose solving the service migration using reinforcement learning based model which can take a long-term goal into consideration and make service migration and communication decisions more efficient. we consider a single-user EC system with exploiting predefined movement of user, where user passes through many ECSs and its corresponding Virtual Machine (VM) in ECS decides the migration strategy and communication strategy. We design a Reinforcement Learning (RL)-based framework for a single-user EC service migration system. Q-learning based and Deep Q Network (DQN) based themes are analyzed in detail respectively. Simulation results shows that our RL-based system can achieve the optimal result compared with other two methods under different system parameters."}, "conf/wcnc/XiaoGYWM019": {"title": "Task Offloading and Resources Allocation based on Fairness in Edge Computing.", "url": "https://doi.org/10.1109/WCNC.2019.8885960", "year": "2019", "author": {"Kaile Xiao": "194/4868", "Zhipeng Gao": "25/2165", "Congcong Yao": "252/1219", "Qian Wang 0015": "75/5723-15", "Zijia Mo": "240/6361", "Yang Yang 0006": "48/450-6"}, "abstract": " Abstract:Task offloading has been a hot topic in the field of edge computing. Resources fairness of edge computing servers which is the destination of task offloading directly impacts life of server and the process quality of task. In this paper, we propose a subtask-virtual machine mapping model (subtask-VM mapping model) to complete task offloading from the terminals to the servers. Considering the reasonable allocation of server resources, we also propose stack-based cache mechanism (SCM) to ensure the fairness of server resources allocation. We transform the problem of mapping model solution into the problem of optimal matching in the bipartite graph, and verify the performance of our algorithm by contrast experiment. In particular, the fair performance of our algorithm for server-side is over 84%."}, "conf/bigcomp/GaoFNY18": {"title": "MR-Mafia: Parallel Subspace Clustering Algorithm Based on MapReduce for Large Multi-dimensional Datasets.", "url": "https://doi.org/10.1109/BigComp.2018.00045", "year": "2018", "author": {"Zhipeng Gao": "25/2165", "Yidan Fan": "181/6491", "Kun Niu": "14/1289", "Zhenyi Ying": "220/8794"}, "abstract": " Abstract:The mission of subspace clustering is to find hidden clusters exist in different subspaces within a dataset. In recent years, with the exponential growth of data size and data dimensions, traditional subspace clustering algorithms become inefficient as well as ineffective while extracting knowledge in the big data environment, resulting in an emergent need to design efficient parallel distributed subspace clustering algorithms to handle large multi-dimensional data with an acceptable computational cost. In this paper, we introduce MR-Mafia: a parallel mafia subspace clustering algorithm based on MapReduce. The algorithm takes advantage of MapReduce's data partitioning and task parallelism and achieves a good tradeoff between the cost for disk accesses and communication cost. The experimental results show near linear speedups and demonstrate the high scalability and great application prospects of the proposed algorithm."}, "conf/bigcomp/JinJ0GQ18": {"title": "An Evaluation Mechanism Based on HDFS in Unstable Network Environment.", "url": "https://doi.org/10.1109/BigComp.2018.00047", "year": "2018", "author": {"Xiaoyu Jin": "37/6331", "Xilin Ji": "220/8736", "Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:Reliability of distributed storage is a key issue for storage of big data. This paper presents an evaluation mechanism for the popular distributed file system (HDFS). The evaluation mechanism (RST) is based on HDFS in unstable network environment. The RST can be divided into two parts, one part (NE) is that the NameNode evaluates the storage reliability of DataNode, the other part (CE) is that clients evaluate the communication quality between the client and DataNodes. This paper also proposes strategies of data redundancy and file reading and writing based on RST. The experimental results show that the strategy of data redundancy improves the availability of data and the process of data access improves the success rate of communication in unstable network environment."}, "conf/bigcomp/DengGN18": {"title": "A Novel Data Dependent Similarity Measure Algorithm Based on Attribute Selection.", "url": "https://doi.org/10.1109/BigComp.2018.00105", "year": "2018", "author": {"Nanjie Deng": "181/6446", "Zhipeng Gao": "25/2165", "Kun Niu": "14/1289"}, "abstract": " Abstract:Similarity measure is an inseparable part of many data mining tasks, it possesses of great research value. Recently proposed data dependent dissimilarity measure has been proved to be more efficient than distance similarity measure in clustering, anomaly detection and multi-label classification. In this paper, we introduce a data dependent similarity measure algorithm based on attribute selection. This algorithm takes the attribute significance into consideration when building the partition model which is used to identify the similarity. Our experimental results show that this algorithm has better performance than other algorithms in anomaly detection task and is able to effectively handling high-dimensional data."}, "conf/bigcomp/ZhangJYCGQ18": {"title": "Data Fusion Method Based on Improved D-S Evidence Theory.", "url": "https://doi.org/10.1109/BigComp.2018.00145", "year": "2018", "author": {"Wei Zhang": "10/4661", "Xilin Ji": "220/8736", "Yang Yang 0006": "48/450-6", "Jianwen Chen": "32/3685", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:As the rapid growth of user-generated data from social networks, wikis and social tagging systems, it is necessary to understand the high-level semantics and user subjective perceptions from such a large volume of data. In the era of big data flooding, how to fuse the emotional computing results from massive data to obtain effective conclusions and decisions has become a problem. This paper combines D-S evidence theory with data fusion and effectively solves the conflict of evidence evidence in D-S evidence theory by introducing the Bhattacharyya distance, the confidence level of evidence and the modified combination rule. The experimental results show that the improved data fusion method can get the data fusion result, and the result has a high accuracy and credibility."}, "conf/edge/XiaoGW018": {"title": "A Heuristic Algorithm Based on Resource Requirements Forecasting for Server Placement in Edge Computing.", "url": "https://doi.org/10.1109/SEC.2018.00043", "year": "2018", "author": {"Kaile Xiao": "194/4868", "Zhipeng Gao": "25/2165", "Qian Wang 0015": "75/5723-15", "Yang Yang 0006": "48/450-6"}, "abstract": " Abstract:The placement of edge computing server is the key to the rapid development of edge computing. We propose prediction-mapping-optimization heuristic based on resource requirements forecasting for server placement in edge computing. Through this algorithm, we divide the task into multiple subtasks, and then realize the mapping of subtask-location of server, and finish the information interaction between the servers and the data source through the data naming mechanism proposed by us. With the goal of the lowest cost of service providers, we propose a cross-region resource optimization model and obtained the final server placement strategy."}, "conf/icccsec/0006GQGY18": {"title": "A Mixed Mobile Charging Strategy in Rechargeable Wireless Sensor Networks.", "url": "https://doi.org/10.1007/978-3-030-00015-8_53", "year": "2018", "author": {"Yang Yang 0006": "48/450-6", "Xiang yang Gong": "226/3673", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165", "Haitao Yu": "75/6588"}, "abstract": "Based on the mobile charging scheduling strategy, the Mobile Charger (MC) complements the power supply for the sensor nodes. We proposed a Mixed Charging Schedule Algorithm (MCS) based on the periodic charging scheme and the on-demand charging scheme. We add a node’s rate grouping mechanism to determine which nodes need to be recharged based on a dynamic threshold mechanism. Based on the periodic charging loops and the service station deployment mechanism, a charging loop is constructed according to guide the MC to wirelessly charge the sensor nodes. The proposed algorithm reduces the power consumption of the sensor network, improves the efficiency of MC mobile billing, and ensures the normal operation of the network.KeywordsRechargeable wireless sensor networkOn-demandMixed charging strategyService stationPre-charging"}, "conf/intellisys/GaoLNY18": {"title": "Tag Genome Aware Collaborative Filtering Based on Item Clustering for Recommendations.", "url": "https://doi.org/10.1007/978-3-030-01054-6_77", "year": "2018", "author": {"Zhipeng Gao": "25/2165", "Bo Li": "50/3402", "Kun Niu": "14/1289", "Yang Yang 0006": "48/450-6"}, "abstract": "Item-based collaborative filtering has become the most popular algorithm in recommender systems which exploits relationships among items to predict users’ preferences for items. However, this algorithm suffers from data sparsity and poor varieties of information sources. It only contains information from rating data, lacking the items’ meta information such as classifications and characteristics, which make similarities measurement among items imprecise, consequently reduce accuracy and interpretability of results. In this paper, we incorporate tag genome information into item-based collaborative filtering using item clustering techniques, which provides a more objective and comprehensive description of items. In particular, we utilize relevance between items and tags to calculate similarities in a more precise way and integrate the results with item-based collaborative filtering through an optimization process. An innovative improved stochastic gradient descent algorithm is adopted to accelerate the optimization process, which lowers down the time complexity without sacrificing the accuracy of the model. Our experimental results demonstrate the effectiveness of our proposed algorithm by gaining lower rating prediction error than currently popular item-based collaborative filtering and SVD latent factor model, and also outperforms them in a more practical top K recommender system.KeywordsItem clusteringCollaborative filteringTag genomeRecommender system"}, "conf/ipccc/WangGDZ18": {"title": "An Optimal LTE-U Access Method for Throughput Maximization and Fairness Assurance.", "url": "https://doi.org/10.1109/PCCC.2018.8710909", "year": "2018", "author": {"Qian Wang 0015": "75/5723-15", "Zhipeng Gao": "25/2165", "Xiaojiang Du": "22/5535", "Liehuang Zhu": "26/2546"}, "abstract": " Abstract:To solve the issue of scarce spectrum resources of the existing cellular network, LTE-U that expands LTE service to the unlicensed 5GHz spectrum is proposed. However, the centralized medium access control protocol of LTE largely decreases the performance of Wi-Fi networks operating in the same unlicensed spectrum. In the paper, to solve the problem, we propose a new mechanism based on the duty-cycle method. It can adaptively adjust the percentage of the airtime used by a LTE Small cell Base Station (SBS) according to the bandwidth of the licensed spectrum of the SBS and downlink data rate demands of the SBS users to maximize throughput of the SBS network on the unlicensed spectrum while ensuring fairness between the Wi-Fi and SBS network. The fairness is based on 3GPP proposed fairness coexistence criterion. To ensure the fairness, we propose a method to construct a W-Fi network offering the same level of the SBS traffic load, and throughput maximization of the SBS is formulated as a constrained non-linear optimization problem solved by an optimal algorithm. We evaluate the proposed mechanism from two aspects. The first is to prove the proposed Wi-Fi network construction method is valid. The second is to evaluate the performance of our proposed method. Simulation results show that our approach is valid and it can maximize the throughput of the SBS network and ensure the fairness criterion."}, "conf/services/GaoMWY18": {"title": "Service Migration for Deadline-Varying User-Generated Data in Mobile Edge-Clouds.", "url": "https://doi.org/10.1109/SERVICES.2018.00039", "year": "2018", "author": {"Zhipeng Gao": "25/2165", "Jie Meng": "80/485", "Qian Wang 0015": "75/5723-15", "Yang Yang 0006": "48/450-6"}, "abstract": " Abstract:Mobile edge computing is a promising paradigm to compensate for the lack of traditional cloud computing, which has a variety of application scenarios. However, the migration of user-generated data in edge networks is a key issue which involves in transmission costs, the mobility of users, transmission resources, etc. In this paper, we focus on migrating deadline-varying user-generated data to edge servers, considering the tasks characteristics and contact patterns between nodes. We design a heuristic algorithm and propose the online algorithm using real-time information to save the cost of transmission. Further, we conduct the extensive simulations to demonstrate the effectiveness of our algorithms."}, "conf/uic/GaoMW018": {"title": "Data Offloading for Deadline-Varying Tasks in Mobile Edge Computing.", "url": "https://doi.org/10.1109/SmartWorld.2018.00256", "year": "2018", "author": {"Zhipeng Gao": "25/2165", "Jie Meng": "80/485", "Qian Wang 0015": "75/5723-15", "Yang Yang 0006": "48/450-6"}, "abstract": " Abstract:Opportunistic mobile networks are efficient complementary forms of data transmission in Mobile Edge Computing, which consist of many mobile users and could reduce the costs. In this paper, we focus on offloading deadline-varying tasks, mainly depending on the contact frequency and durations. Different from existing work, we consider multi-tasks and the influence of contact situation on probabilities. However, the problem is NP-hard. Thus, we design the offline data offloading heuristic algorithm to schedule a series of tasks to encountering mobile nodes or Wi-Fi APs. To fully utilize the real-time contact information, we further propose the online offloading algorithm, which adjusts the offloading scheme depending on the new contact parameters. Finally, we conduct simulations and evaluate the performance of our algorithms, comparing to other algorithms. Moreover, we demonstrate that our algorithms perform better than the compared algorithms through detailed comparison."}, "conf/dasc/GaoZNF17": {"title": "A High-Dimensional Outlier Detection Algorithm Base on Relevant Subspace.", "url": "https://doi.org/10.1109/DASC-PICom-DataCom-CyberSciTec.2017.165", "year": "2017", "author": {"Zhipeng Gao": "25/2165", "Yang Zhao": "50/2082", "Kun Niu": "14/1289", "Yidan Fan": "181/6491"}, "abstract": " Abstract:Outlier detection in high-dimensional big data is an important data mining task to distinguish outliers from regular objects. In tradition, outlier detection approaches miss outliers which hide in full data space. However, these methods are deteriorated due to the notorious \"curse of dimensionality\" which leads to distance cannot express the deviation of outlier and normal objects, and the exponential computation leads to low efficiency. In this paper, we propose an outlier detection method based on relevant subspace, which can effectively describe the local distribution of objects and detect outliers hidden in subspaces of the data. In thorough experiments on synthetic data and real data, it shows that the method outperforms competing outlier ranking approaches by detecting outliers in subspace."}, "conf/dasc/GaoFNW17": {"title": "An Adaptive Initial Cluster Centers Selection Algorithm for High-Dimensional Partition Clustering.", "url": "https://doi.org/10.1109/DASC-PICom-DataCom-CyberSciTec.2017.181", "year": "2017", "author": {"Zhipeng Gao": "25/2165", "Yidan Fan": "181/6491", "Kun Niu": "14/1289", "Ting Wang": "12/2633"}, "abstract": " Abstract:Cluster analysis is the process of partitioning a set of data objects into subsets, each subset is a cluster, so that objects within a cluster have high similarity, but are very dissimilar to objects in other clusters. Partitioning methods in clustering start from an initial partitioning and gain the optimal partition by applying the iterative relocation technique. Partition clustering results depend heavily on the selection of initial cluster centers. Traditional distance-based initialization methods become inefficient because of the inherent sparsity in high-dimensional data and the curse of dimensionality, while existing improved methods are very sensitive to parameters. Based on these, we propose a new initialization method for high-dimensional partition clustering, which can choose high-density and low-similarity initial cluster centers and identify outliers according to its local structure in high-dimensional space adaptively. The experiments on both synthetic and real-world datasets show that the proposed algorithm can achieve better performance."}, "conf/im/MaYQGL17": {"title": "Fault-tolerant topology control for heterogeneous wireless sensor networks using Multi-Routing Tree.", "url": "https://doi.org/10.23919/INM.2017.7987344", "year": "2017", "author": {"Guizhen Ma": "180/9659", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165", "He Li 0004": "05/4746-4"}, "abstract": " Abstract:Fault-tolerant topology control is a critical problem in WSNs. It is important for improving network lifetime and reliability. In this paper, we present a novel algorithm FTMRT, which ensures Fault Tolerance by constructing a Multi-Routing Tree. We firstly construct a multi-routing tree of the initial topology, which ensures there are at least k-disjoint paths from each sensor to the set of supernodes. And then each sensor adjusts its transmission power according to the multi-routing tree to form the fault-tolerant network topology. In the topology maintenance phase, topology reconstruction is invoked each time there are some node fail and the supernode connectivity is broken. The effectiveness of the proposed algorithm is validated through simulation experiments."}, "conf/uic/NiuJGJYC17": {"title": "A developed feature selection method for classification based on united information gain.", "url": "https://doi.org/10.1109/UIC-ATC.2017.8397477", "year": "2017", "author": {"Kun Niu": "14/1289", "Haizhen Jiao": "188/7908", "Zhipeng Gao": "25/2165", "Guannan Jia": "222/6150", "Guangyu Yang": "04/2076", "Cheng Cheng": "66/332"}, "abstract": " Abstract:In big data times, effective feature selection methods become more and more important because of high dimensional character. In this paper, we propose a developed feature selection method UIG, what effectively improves the accuracy of classifiers. Firstly, all the attributes are systemized pairwise to calculate their united information gain to form a United Information Gain Matrix. Secondly, the United Information Gain Matrix is discretized by a predefined parameter. Finally, UIG searches the biggest full-1 sub-matrix to find the maximum valid subset of attributes. Then classifier can be built by the selected attributes. Experiments on several public data sets show that the proposed method is effective and robust for different situations."}, "conf/vtc/Li0QGM17": {"title": "Cooperative Relay Selection and Forwarding in Vehicle-to-Infrastructure Communications.", "url": "https://doi.org/10.1109/VTCSpring.2017.8108457", "year": "2017", "author": {"He Li 0004": "05/4746-4", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165", "Guizhen Ma": "180/9659"}, "abstract": " Abstract:The wireless sensors deployed at the highway can ensure the safety of the traveling vehicles. However, the ribbon deployed wireless sensor network in the roadside infrastructure can easily to generate energy hole. Cooperative communication between sensors and vehicles is an effective way to improve this situation and reduce the energy consumption of the sensors. A cooperative relay selection algorithm based on residence time (CRSR) and cooperative relay selection algorithms based on prediction (CRSP) are proposed in this paper. In order to improve the data transfer amount of the vehicle, residence time is considered in CRSR when sensors select the vehicles. To further improve energy efficiency CRSP considers arrival time of the vehicle to store collected data in the delay tolerance situation. Energy is also considered in CRSP to reduce the energy consumption and ease energy hole. Simulation results show that the CRSR and CRSP methods can reduce the energy consumption and prolong sensor network lifetime than the traditional algorithm."}, "conf/ccis/NiuGJD16": {"title": "K-means+: A developed clustering algorithm for big data.", "url": "https://doi.org/10.1109/CCIS.2016.7790241", "year": "2016", "author": {"Kun Niu": "14/1289", "Zhipeng Gao": "25/2165", "Haizhen Jiao": "188/7908", "Nanjie Deng": "181/6446"}, "abstract": " Abstract:Clustering is one of the most important task in data mining. But for big data application, clustering models are faced with the problem of high complexity for low respond time requirement. This paper focuses on velocity criterion of big data modeling, presents a developed k-means algorithm, k-means+, which effectively reduces time costs of clustering modeling through block operation and redesigning of distance function. Block operation aggregates instances as blocks to cluster afterwards. Manhattan distance is used instead of common Euclidean distance to simplify calculation. Experimental results show that k-means+ works well on most testing datasets and executes much faster than original k-means."}, "conf/ccis/GaoQN16": {"title": "An effective merge strategy based hierarchy for improving small file problem on HDFS.", "url": "https://doi.org/10.1109/CCIS.2016.7790278", "year": "2016", "author": {"Zhipeng Gao": "25/2165", "Yinghao Qin": "192/1022", "Kun Niu": "14/1289"}, "abstract": " Abstract:Hadoop Distributed File System (HDFS) is designed for reliable storage and management of very large file and low-cost storage capability. As HDFS architecture based on master (NameNode) to handle metadata for multiple slaves (DataNode), NameNode often becomes the bottleneck, especially when handing large number of small files. It is a common solution to merge many small files into one big file about this problem. To solve the large small files problem and improve the efficiency of accessing small files, in this paper, we define Logic File Name (LFN) and propose the Small file Merge Strategy Based LFN (SMSBL). SMSBL is a new idea and a new perspective on hierarchy, it improves the correlation of small files in the same block of HDFS effectively based different file system hierarchy, so the performance is amazing facing large small files when HDFS adopted SMSBL with prefetching mechanism. The system efficiency analysis model is established and experimental results demonstrate that SMSBL can solve small file problem in HDFS and has appreciable high hit rate of prefetching files."}, "conf/icce-tw/GaoLDN16": {"title": "A novel collaborative filtering recommendation algorithm based on user location.", "url": "https://doi.org/10.1109/ICCE-TW.2016.7520906", "year": "2016", "author": {"Zhipeng Gao": "25/2165", "Zehui Lu": "307/3690", "Nanjie Deng": "181/6446", "Kun Niu": "14/1289"}, "abstract": " Abstract:With information technology and the Internet developing fast, people gradually walk out of the time of information deficient and enter the era of information overload. Whether information consumers or information producers are faced with big challenge: how to obtain or sell the information. Recommendation system is a key to this problem. Traditional recommendation system focuses on connecting user interest and items, and recommends items which match user interest. However, all these algorithms ignore the context which users are in. In terms of this problem, this paper presents a novel collaborative filtering recommendation algorithm based on user location context. Firstly, this algorithm defines user location attenuation function to calculate the relations between user locations, then combines this function with traditional Pearson similarity method to get similarity between users, finally, uses the traditional collaborative filtering recommendation algorithm to realize preference prediction and recommendation. Experiments show that this algorithm which has location information taken into account can improve recommendation quality for traditional collaborative filtering recommendation algorithms."}, "conf/icce-tw/GaoLN16": {"title": "Solutions for problems of existing E-Commerce recommendation system.", "url": "https://doi.org/10.1109/ICCE-TW.2016.7520907", "year": "2016", "author": {"Zhipeng Gao": "25/2165", "Zhixing Li": "68/8126", "Kun Niu": "14/1289"}, "abstract": " Abstract:With the development of E-Commerce, it's harder and harder for consumers to find the product they want, and the recommendation systems are applied more and more widely. A recommendation system includes user model, the recommended model and recommendation algorithm. The improvement considered in this paper mainly refer to the recommendation algorithm. Limited resource, data valid time and cold start problems are not well considered in existing E-Commerce recommendation system. According to the problems descripted above, a algorithm based on limited resource and a solution to cold start problem are proposed."}, "conf/icycsee/NiuGXDJ16": {"title": "A Real-Time Fraud Detection Algorithm Based on Usage Amount Forecast.", "url": "https://doi.org/10.1007/978-981-10-2053-7_9", "year": "2016", "author": {"Kun Niu": "14/1289", "Zhipeng Gao": "25/2165", "Kaile Xiao": "194/4868", "Nanjie Deng": "181/6446", "Haizhen Jiao": "188/7908"}, "abstract": "Real-time Fraud Detection has always been a challenging task, especially in financial, insurance, and telecom industries. There are mainly three methods, which are rule set, outlier detection and classification to solve the problem. But those methods have some drawbacks respectively. To overcome these limitations, we propose a new algorithm UAF (Usage Amount Forecast). Firstly, Manhattan distance is used to measure the similarity between fraudulent instances and normal ones. Secondly, UAF gives real-time score which detects the fraud early and reduces as much economic loss as possible. Experiments on various real-world datasets demonstrate the high potential of UAF for processing real-time data and predicting fraudulent users.KeywordsReal-time Fraud DetectionUsage Amount ForecastTelecom industry"}, "conf/nana/NiuJDG16": {"title": "A Real-Time Fraud Detection Algorithm Based on Intelligent Scoring for the Telecom Industry.", "url": "https://doi.org/10.1109/NaNA.2016.11", "year": "2016", "author": {"Kun Niu": "14/1289", "Haizhen Jiao": "188/7908", "Nanjie Deng": "181/6446", "Zhipeng Gao": "25/2165"}, "abstract": " Abstract:Fraud detection is one of the biggest challenges in the telecom industry. Commonly used approaches, such as rule sets, outlier detection, and classification, have high computational cost, so they don't work well on mass data in terms of accuracy and speed. Besides, those algorithms are not good at detecting new fraud patterns. In this paper, we propose an UIS (United Intelligent Scoring) algorithm for fraud detection which has three merits. First, it has lower computational complexity. We use Manhattan distance instead of Euclidean distance to measure similarity between fraud samples and ordinaries. Second, new fraud patterns can be detected effectively by joint fraud probability. Finally, UIS is able to generate and update real-time scores, which detects early-time fraud and minimizes economic losses. Integrated experiments on real datasets of the telecom industry demonstrate that UIS is real-time, effective, and robust in different situations."}, "conf/icaci/0006LGQR15": {"title": "Data clustering-based fault detection in WSNs.", "url": "https://doi.org/10.1109/ICACI.2015.7184725", "year": "2015", "author": {"Yang Yang 0006": "48/450-6", "Qian Liu": "33/85", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Lanlan Rui": "03/4227"}, "abstract": " Abstract:Sensors easily become faulty and unreliable subject to limited battery and insecurity. Data Fault is one of traditional faults in the wireless sensor networks. Data fault mainly uses distributed method through exchanging neighbors' measurements and voting for decision. But the detection accuracy performance is easily influenced by unbalanced fault distribution. Based on this, we propose the k-means clustering-based fault detection algorithm (k-CFD), which uses clustering view to replace tendency values for fault decision, in addition, and adopts ant colony optimization algorithm to promote the results of k-means mechanism. The simulation results demonstrate the efficiency and superiority of k-CFD mechanisms."}, "conf/icce-tw/HongGHL15": {"title": "A novel SCNDR rumor propagation model on online social networks.", "url": "https://doi.org/10.1109/ICCE-TW.2015.7216829", "year": "2015", "author": {"Weijun Hong": "82/7830", "Zhipeng Gao": "25/2165", "Yuwen Hao": "149/8368", "Xiaoxue Li": "84/7448"}, "abstract": " Abstract:With the rapid development of online social networks, the spread of online rumors will result in serious social problems. How to correctly understand the propagation rules of online rumors, and effectively control the spread of online rumors, has very important research. Based on the detailed analysis of the classic epidemic spreading model, this paper divides the infected states into credulous(C), neutrals(N) and denies(D) according to the characteristics of online social networks, then proposes the new SCNDR rumor propagation model on online social networks. Based on the SCNDR model, derives the dynamic differential equations and designs the algorithm for SCNDR."}, "conf/icycsee/LiGQHHZ15": {"title": "An Improved Frag-Shells Algorithm for Data Cube Construction Based on Irrelevance of Data Dispersion.", "url": "https://doi.org/10.1007/978-3-662-46248-5_36", "year": "2015", "author": {"Dong Li": "47/4826", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Ran He": "61/6198", "Yuwen Hao": "149/8368", "Jingchen Zheng": "149/8447"}, "abstract": "On-Line Analytical Processing (OLAP) is based on pre-computation of data cubes, which greatly reduces the response time and improves the performance of OLAP. Frag-Shells algorithm is a common method of pre-computation. However, it relies too much on the data dispersion that it performs poorly, when confronts large amount of highly disperse data. As the amount of data grows fast nowadays, the efficiency of data cube construction is increasingly becoming a significant bottleneck. In addition, with the popularity of cloud computing and big data, MapReduce framework proposed by Google is playing an increasingly prominent role in parallel processing. It is an intuitive idea that MapReduce framework can be used to enhance the efficiency of parallel data cube construction. In this paper, by improving the Frag-Shells algorithm based on the irrelevance of data dispersion, and taking advantages of the high parallelism of MapReduce framework, we propose an improved Frag-Shells algorithm based on MapReduce framework. The simulation results prove that the proposed algorithm greatly enhances the efficiency of cube construction.KeywordsOLAPMapReduceData cube constructionFrag-ShellsData dispersion"}, "conf/iscc/ChengGZH15": {"title": "An energy-efficient prediction-based algorithm for object tracking in sensor networks.", "url": "https://doi.org/10.1109/ISCC.2015.7405628", "year": "2015", "author": {"Weijing Cheng": "176/0909", "Zhipeng Gao": "25/2165", "Jingchen Zheng": "149/8447", "Yuwen Hao": "149/8368"}, "abstract": " Abstract:Object Tracking Sensor Network (OTSN) is considered one of the most energy consuming applications of wireless sensor network. OTSN is used to track moving objects and report their newest location which consumes a large amount of energy. However, energy of sensor node is limited and the movement of objects generally follows some definite patterns. We can reduce the energy consuming by predicting the next location of an object to keep irrelevant sensor nodes sleepy as long as possible. In this paper, we propose an energy-efficient prediction-based tracking algorithm called Improved Mining Pattern (IMP). This algorithm predicts the next active sensor node based on the backward dependence. The predicted paths can be updated partly fast through clustering. Besides, IMP reduces the long distance communication between sensor nodes and the base station. In addition, missing objects can be tracked again quickly through recovery algorithm which is based on prediction results. Moreover, this algorithm can track multi-species simultaneously. Experimental results show that IMP behaves better than other algorithms in reducing the energy consumption and the missing rate."}, "conf/apnoms/GaoLYZH14": {"title": "A load balance algorithm based on nodes performance in Hadoop cluster.", "url": "https://doi.org/10.1109/APNOMS.2014.6996555", "year": "2014", "author": {"Zhipeng Gao": "25/2165", "Dangpeng Liu": "156/3733", "Yang Yang 0006": "48/450-6", "Jingchen Zheng": "149/8447", "Yuwen Hao": "149/8368"}, "abstract": " Abstract:MapReduce is an important distributed programming model for large-scale data-parallel applications like web indexing, data mining, and scientific simulation. Hadoop is an open-source implementation of MapReduce and it is often applied to short jobs for which low response time is critical. When the cluster nodes are homogeneous, Hadoop has a good performance. In practice, the homogeneity assumptions do not always hold. In heterogeneous environment, there are various devices which vary greatly in the capacities of computation, communication, architectures, memories and power. When different nodes process the same amount of data, load balancing problem occurs. In this paper we address the problem of how to assign data after Map phase to balance the execution time of each Reduce task by proposing a novel load balancing algorithm based on nodes performance (LBNP), in which the input data of poor performance nodes are decreased. Simulation results indicate that all the Reduce tasks can be completed in the same time which shortens the whole Reduce phase. Thus the efficiency of MapReduce is improved."}, "conf/apnoms/ZhouYQG14": {"title": "The strategy of probe station selection of active probing in WSNs.", "url": "https://doi.org/10.1109/APNOMS.2014.6996549", "year": "2014", "author": {"Hang Zhou 0002": "26/3707-2", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165"}, "abstract": " Abstract:In the management of WSNs, the mechanism of fault detection and location based on active probing has been widely applied. The main optimal direction of active probing is to maximize the coverage of nodes in the network by sending minimal set of probes from probe stations. Therefore, before probing, selecting optimizational station set to improve reachable rates of probed nodes has significant influence on detection effect of active probing. In this paper, we propose an optimizational strategy of probe station selection (PSS) by Genetic Algorithm (GA) and achieve the improvement of confirmed achievable rates of probed nodes. Meanwhile, lower runtime cost and more reasonable usage of energy are reached."}, "conf/apnoms/ZhuYQG14": {"title": "Sensor failure detection and recovery mechanism based on support vector and genetic algorithm.", "url": "https://doi.org/10.1109/APNOMS.2014.6996565", "year": "2014", "author": {"Jiehui Zhu": "156/3672", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165"}, "abstract": " Abstract:The main role of wireless sensor networks is to collect environmental data. As the sensor nodes are vulnerable and work in unpredictable environments, sensors are possible to fail and return unexpected response. Therefore, fault detection and recovery are important in wireless sensor networks. In this paper, we propose a fault detection algorithm based on support vector regression, which predicts the measurements of sensor nodes by using historical data. Credit levels of sensor nodes will be determined by a contrast between predictions and actual measured values. In this paper we also propose a fault recovery algorithm according to the node credit levels combined with genetic algorithm. The simulation results demonstrate that the algorithms we propose work well in failure detection rate, fault recovery speed and energy consumption."}, "conf/iih-msp/GaoCCHL14": {"title": "K-Extended Constrain Independent Relay Node Placement with Base Stations in Two-Tiered Wireless Sensor Network.", "url": "https://doi.org/10.1109/IIH-MSP.2014.208", "year": "2014", "author": {"Zhipeng Gao": "25/2165", "Kan Chen": "33/2607", "Weijing Cheng": "176/0909", "Yuwen Hao": "149/8368", "Xiaoxue Li": "84/7448"}, "abstract": " Abstract:In 2tWSN(two-tiered wireless sensor network), relay nodes are responsible for delivering the sensed data from sensor nodes to base stations. From the perspective of business, it needs more relay nodes to ensure network connectivity. From the perspective of economy, relay nodes are relatively expensive, so people are intended to place a minimum number of relay nodes while guaranteeing the data collecting and delivering. In this paper, we firstly model fault tolerant RNP(relay node placement) problem in 2tWSN as a graphic problem, denoted by DBY-HCG. It distinguishes relay nodes from base stations. Secondly, we figure out keCi-RNPB algorithm. Moreover, an approximation algorithm aiming at finding the minimum length of k-vertex disjoint paths in connected sub-graph is raised up to support our algorithm's extensibility. Our algorithm is also the first solution for constrained version of fault tolerant relay node placement problem in 2tWSN. Extensive experiments have been executed in both unconstrained and constrained situations and the numeric results show that our solution is close to optimal solution."}, "conf/iov/GaoCZHYQ14": {"title": "Crossroads Optimal Geographic Routing for Vehicular Ad Hoc Networks in City Scenario.", "url": "https://doi.org/10.1007/978-3-319-11167-4_20", "year": "2014", "author": {"Zhipeng Gao": "25/2165", "Kan Chen": "33/2607", "Jingchen Zheng": "149/8447", "Yuwen Hao": "149/8368", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342"}, "abstract": "As it is a big challenge to adapt routing protocol to different applications and dynamic network topology in Mobile Ad Hoc Networks, geographic routing protocols such as Greedy Perimeter Stateless Routing protocol (GPSR) have attracted significant attention. However, unintelligent routing path selecting strategy and outdated neighbor information lead to unwanted performance decline. In this paper, we propose a routing protocol called Crossroads Optimal Geographic Routing protocol (COGR) for vehicular ad hoc networks (VANET) in city scenario, which features on intelligent routing path planning, efficient recovery strategy from dead holes and feasible neighbor position distribution protocol without map information ahead. Through simulation experiments, we prove that COGR does substantially improve performance such as packet delivery ratio, delay and throughput in highly dynamic network environment as VANET in city scenario.KeywordsDelivery RatioPacket Delivery RatioPacket Loss RateForwarding StrategyGreedy ForwardingThese keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves."}, "conf/iscc/ChenXYGQ12": {"title": "The contract net based task allocation algorithm for wireless sensor network.", "url": "https://doi.org/10.1109/ISCC.2012.6249362", "year": "2012", "author": {"Lin Chen": "13/3479", "Xuesong Qiu 0001": "13/342", "Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165", "Zheng Qu": "13/9687"}, "abstract": " Abstract:Since wireless sensor network has limited resources, it's important to design its task allocation algorithm reasonably to reduce energy consumption. The contract net is simple and flexible so that it can meet the needs of the wireless sensor network. In this paper, we introduce the improved C-MEANS algorithm to cluster nodes to decrease the number of bidders, and at the same time, the LMS algorithm is adopted to predict the bid value of the nodes. The simulation results show that the energy consumption and traffic flow are reduced, and the bid value more accurately reflects the status of the node when allocated tasks, which increased the complete rate of network tasks."}, "conf/msn/JinGHCQ11": {"title": "An Incomplete Coverage Control Based on Target Tracking Wireless Sensor Network.", "url": "https://doi.org/10.1109/MSN.2011.18", "year": "2011", "author": {"Meng Jin": "33/6937", "Zhipeng Gao": "25/2165", "Rimao Huang": "57/10696", "Xiao Chang": "44/1831", "Feng Qi 0004": "42/1054-4"}, "abstract": " Abstract:Coverage control is one of the most important technologies in Wireless Sensor Network (WSN). In the precondition of better coverage quality, how to format optimal coverage with least sensors is a significant problem to be solved. A new incomplete coverage control based on target tracking sensor network which called mobile-constrained optimal target tracking coverage algorithm (MCOTT) is presented. In our approach, static sensors will be pre-deployed, collaborating with mobile sensors to achieve an optimal coverage which based on a target trajectory prediction model. Simulation results show that, MCOTT has more advantages like good robustness, high level of target coverage, low energy consumption. The algorithm can save the number of sensors and prolong the network lifetime effectively."}, "conf/msn/YangGHQ11": {"title": "A Data Correlation-Based Virtual Clustering Algorithm for Wireless Sensor Network.", "url": "https://doi.org/10.1109/MSN.2011.2", "year": "2011", "author": {"Shuchun Yang": "90/10695", "Zhipeng Gao": "25/2165", "Rimao Huang": "57/10696", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:In WSN, Clustering Routing Algorithm can effectively reduce network energy consumption and prolong network lifetime well. But existed Clustering Routing Algorithms are usually location-based, where data correlations are not considered. There is still data redundancy in the terminal. This paper proposes a data correlation-based virtual clustering approach. It integrates the advantages of clustering technique and data correlation. Nodes that are good data correlated will be partitioned in the same virtual cluster. The experimental results show that the proposed algorithm can reduce the amount of messages sent by the nodes, and reduce the energy consumption. The network lifetime is prolonged as well."}, "conf/msn/ZhaoGHWW11": {"title": "A Fault Detection Algorithm Based on Cluster Analysis in Wireless Sensor Networks.", "url": "https://doi.org/10.1109/MSN.2011.59", "year": "2011", "author": {"Xiaodong Zhao": "97/5507", "Zhipeng Gao": "25/2165", "Rimao Huang": "57/10696", "Zhuoqi Wang": "28/10696", "Teng Wang": "49/9"}, "abstract": " Abstract:In this paper, we present a distributed fault detection algorithm based on k-means clustering for WSN. The nodes within a cluster are divided into three sub-clustering according to their measurements' similarity. We conclude the sensor nodes' working state from the N recent states of sub-clustering, so as to detect, locate, and get rid of the fault nodes. Simulation results show that the k-means cluster fault detection algorithm has a better performance than the distributed Bayesian algorithms. Moreover, the computational complexity of the proposed algorithm is low."}, "conf/cnsm/LiGYGCQ10": {"title": "A cluster-based negotiation model for task allocation in Wireless Sensor Network.", "url": "https://doi.org/10.1109/CNSM.2010.5691326", "year": "2010", "author": {"Yukun Li": "73/7536", "Zhipeng Gao": "25/2165", "Yang Yang 0006": "48/450-6", "Zhili Guan": "29/9064", "Xingyu Chen": "59/7651", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:This paper studies task allocation in cluster-based Wireless Sensor Network (WSN) using negotiation model. We study both the negotiation reasoning model and the negotiation protocol for task allocation to achieve energy efficiency while balancing nodes energy. Reasoning model determines the offer generate scheme and gives control over negotiation process. A time depending Boulware function is used as the concession strategy in reasoning model to balancing efficiency and utility. Contract net protocol is used as negotiation protocol to regulate the interaction style of nodes. The goals of this study are: 1) energy efficiency task allocation; 2) maintaining energy balance of nodes in WSN after the task to prolong the network life cycle. Experimental results using this cluster-based negotiation model task allocation approach verify its performance."}, "conf/cnsm/LiuQGCCP10": {"title": "An Iterative Information-Theoretic approach to estimate traffic matrix.", "url": "https://doi.org/10.1109/CNSM.2010.5691194", "year": "2010", "author": {"Ke Liu": "32/2948", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165", "Xingyu Chen": "59/7651", "Shuying Chang": "58/665", "Yuan Pang": "92/9063"}, "abstract": " Abstract:Traffic matrices are very essential for many network engineering tasks: for instance, load balancing, capacity planning, routing protocol configuration. However, measuring these traffic matrices directly is difficult and costly. Hence many methods have been proposed to estimate these traffic matrices based on link load measurements and other more easily available data. This paper presents an iterative algorithm to estimate traffic matrix without differentiating the access links and the peering links and our algorithm get a similar performance with the Minimal Mutual Information method which requires that difference. Experiments on real backbone network data have also demonstrated that our algorithm is accurate and robust to measurement noise."}, "conf/cnsm/ChangQGLQ10": {"title": "A flow-based anomaly detection method using sketch and combinations of traffic features.", "url": "https://doi.org/10.1109/CNSM.2010.5691206", "year": "2010", "author": {"Shuying Chang": "58/665", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165", "Ke Liu": "32/2948", "Feng Qi 0004": "42/1054-4"}, "abstract": " Abstract:With the development of high-speed networks, the challenge of effectively analyzing the massive data source for anomaly detection and diagnosis is yet to be resolved. This paper proposes a new flow-based anomaly detection method based on summary data structures and combinations of traffic features. Using IPFIX flow records as input, parallel sketches are established for chosen traffic features respectively. For each sketch, we use Holt-Winters forecasting technique to achieve their forecast sketches and deviation matrixes. When the deviation exceeds a certain threshold, sub-alarms will be generated. According to the characteristics of various attacks and combinations of traffic features, sub-alarms can be merged into final alarms. While sketches of flows are being constructed, destination addresses are recorded in linked lists which are used to locate victims by a series of set operations. This method can not only detect the existence of anomalies in near real time, but can roughly indicate the anomaly types and locate abnormal addresses."}, "conf/iscc/GuoGZGCQ10": {"title": "A hotspot attraction driven user mobility model and direction deciding algorithm.", "url": "https://doi.org/10.1109/ISCC.2010.5546726", "year": "2010", "author": {"Hao Guo": "97/3499", "Zhipeng Gao": "25/2165", "Heng Zhang": "55/826", "Zhili Guan": "29/9064", "Xingyu Chen": "59/7651", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:Mobility Model for mobile users is an important part of simulation of the wireless mobile network's resource management and optimization. The Manhattan Mobility Model proposed by ETSI is highly versatile. However, its random direction selecting strategy lacks of objectives, which means the scene of hotspot attracting user mobility could not be simulated accurately. In order to simulate the user mobility attracted by hotspots, a novel Hotspot Attraction Driven User Mobility Model (HADUMM) is proposed based on the Manhattan Mobility Model, and a Direction Deciding Algorithm (DDA) is designed. The HADUMM's realistic significance is evaluated."}, "conf/csie/ZhangWGL09": {"title": "Design and Implementation of Mapping Rules from OWL to Relational Database.", "url": "https://doi.org/10.1109/CSIE.2009.290", "year": "2009", "author": {"Huiji Zhang": "51/7721", "Zhili Wang": "15/2643", "Zhipeng Gao": "25/2165", "Wenjing Li 0001": "08/6548-1"}, "abstract": " Abstract:The paper is to solve how to map OWL information model to database which is very important for sharing information. First, relatively complete and exact mapping rules with the consideration of OWL syntax and semantics is designed. Then, the mapping system from OWL to relational database is implemented based on Jena using Java. Finally, the mapping system is tested by some OWL examples. The mapping rules in this paper are lossless and canonical, and every element in OWL can be stored in the database. The mapping system is advanced, simply operable and robust."}, "conf/icumt/YangCDMGQ09": {"title": "A self-configuration management model for clustering-based MANETs.", "url": "https://doi.org/10.1109/ICUMT.2009.5345636", "year": "2009", "author": {"Yang Yang 0006": "48/450-6", "Jian Chen": "49/6002", "Leiling Duan": "22/8281", "Luoming Meng": "52/382", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:Self-configuration management of mobile ad-hoc networks (MANETs) depends upon the hierarchical architecture and topology change of the network. By analyzing role shift and behavior changes of nodes in the clustering-based MANETs, a self-configuration management model is proposed in this paper. In view of unpredictability of the node behavior, a cluster unit provides the auto-configuration mechanisms with the node entry-exit and the clusterhead replacement control loops to adjust to changes of the network. The node entry-exit control loop resolves configuration as a result of nodes' movements. Configuration processes such as parameter reconfiguration, service publication will be executed once again in the clusterhead replacement control loop when the original clusterhead loses effectiveness. Based on this, two scenarios are depicted to give an analysis in model commonality aspects. Results of simulation prove a better performance of our model. In this paper, we describe the interoperate actions between cluster members and the cluster head to enable each cluster system to self-maintain in its life cycle."}, "conf/asiams/GaoMQF07": {"title": "The Interoperability and Shared Management Information Model.", "url": "https://doi.org/10.1109/AMS.2007.102", "year": "2007", "author": {"Zhipeng Gao": "25/2165", "Luoming Meng": "52/382", "Xuesong Qiu 0001": "13/342", "Xiaocui Fu": "87/4115"}, "abstract": " Abstract:The interoperability problem of different operational support systems (OSS) is analyzed and the problem is divided into two parts: system-level interoperability and model-level interoperability problems. This paper presents an ontology-based model: shared management information model, which is noted as SMIM, to solve the model-level interoperability problem. Then an improved similarity-based ontology mapping method is given to realize the one-more concept mapping. The similarity-based ontology mapping method divides the mapping process into five steps: ontology acquisition, ontology formalization, concept similarity estimation, ontology mapping, and result presentation. At last, a SMIM-based prototype system is introduced to show how system works with the help of SMIM mapping"}, "conf/ifip8/ShangguanGZ07": {"title": "Ontology-Based Process Modeling Using eTOM and ITIL.", "url": "https://doi.org/10.1007/978-0-387-76312-5_24", "year": "2007", "author": {"Zhenning Shangguan": "21/1614", "Zhipeng Gao": "25/2165", "Kai Zhu": "75/4078"}, "abstract": "With recently accelerated shifting trends from device-oriented to process-oriented service management, large portions of business organizations are now on their way of adopting standards and best-practices such as the eTOM and the ITIL to model and manage their diverse business and operational processes. Although there are various process/workflow definition formalisms to orchestrate these processes mentioned above, the rapid development and emerging demands for process automation and interoperability requires systematic modeling methodology and increased semantic information. To achieve this goal, a comprehensive analysis about two commonly accepted process frameworks — the eTOM and the ITIL — is presented in this paper first. Then, further studies concerning their mapping relationship and integrated application of these two process frameworks are carried out, which forms the basis of our process modeling methodology. We also propose an ontology-based process information model based on the preceding methodology, for the purpose of semantic richness. Finally, a novel and ontology-based business process management architecture is given to provide an application scenario and thus, to demonstrate the correctness and feasibility of the proposal in this paper.KeywordsBusiness process modelingeTOMITILOntologyOWLS"}, "journals/iotj/ZhaoGWXM22": {"title": "AFL: An Adaptively Federated Multitask Learning for Model Sharing in Industrial IoT.", "url": "https://doi.org/10.1109/JIOT.2021.3125989", "year": "2022", "author": {"Chen Zhao": "81/3", "Zhipeng Gao": "25/2165", "Qian Wang 0015": "75/5723-15", "Kaile Xiao": "194/4868", "Zijia Mo": "240/6361"}, "abstract": " Abstract:In the Industrial Internet of Things (IIoT), model and computing power sharing among devices can improve resource utilization and work efficiency. However, data privacy and security issues hinder the sharing process. Besides, in the process of model sharing, due to the customization of industrial equipment functions and the high separation of model and task types between devices, it is difficult to share model and optimize models among devices with different task requirements. In this article, we propose an adaptively federated multitask learning (AFL) for IIoT devices efficiently model sharing. Inspired by the parameter sharing mechanism, AFL builds a sparse sharing structure by designing an iterative pruning network and generating subnets for each task. Moreover, for better share relevant information, we further propose tailored task mask layers for effectively training specialized subnets, and an adaptive loss function to dynamically adjust the priority between tasks. Extensive experiments show that AFL can successfully fit hundreds of tasks from different devices into one model, which preserves both high accuracy and system scalability, and outperforms other related approaches that naively combine federated learning with multitask learning."}, "journals/isci/ZhangNGWG22": {"title": "Second-order information bottleneck based spiking neural networks for sEMG recognition.", "url": "https://doi.org/10.1016/j.ins.2021.11.065", "year": "2022", "author": {"Anguo Zhang": "233/3960", "Yuzhen Niu": "28/7442", "Yueming Gao": "152/6738", "Junyi Wu": "125/0678", "Zhipeng Gao": "25/2165"}, "abstract": "The pattern recognition of surface electromyography (sEMG) signal is an important application in the realization of human-machine interface. However, due to the disturbance of human body, sensors and environment, sEMG signal usually contains lots of noise, which brings great challenges to the high-accuracy sEMG pattern recognition. In addition, embedded human wearable devices are becoming more and more popular nowadays. How to realize the sEMG recognition method with low power consumption and high noise immunity has also become a difficult and very meaningful research topic. In this paper, a spiking neural network (SNN) classification method based on second-order information bottleneck training is proposed. Firstly, the training loss function for classification neural networks is constructed based on the proposed second-order information bottleneck. The method is used to train the conventional continuous-valued neural network and convert it into an SNN model with equivalent structure and connection weights. Then, the converted SNN is used to classify the sEMG signal patterns. Through a series of theoretical analysis and experimental results, it is proved that this method has significant advantages in terms of generalization of network determination and computational efficiency. The experimental code can be accessed from  https://github.com/anvien/2OIB-for-sEMG-Recognition."}, "journals/jnca/RuiYGLQM22": {"title": "Smart network maintenance in edge cloud computing environment: An allocation mechanism based on comprehensive reputation and regional prediction model.", "url": "https://doi.org/10.1016/j.jnca.2021.103298", "year": "2022", "author": {"Lanlan Rui": "03/4227", "Siqi Yang": "159/1399", "Zhipeng Gao": "25/2165", "Wenjing Li 0001": "08/6548-1", "Xuesong Qiu 0001": "13/342", "Luoming Meng": "52/382"}, "abstract": "With the rapid development of communication networks, there are more stringent requirements for its maintenance management. This paper uses edge cloud computing technology and wearable technology to improve the inconvenient information flow of on-site maintenance of communication networks. We propose an Emergent Task Allocation Mechanism based on Comprehensive Reputation and Regional Prediction Model (ETARR) in the edge cloud computing environment to solve emergency task allocation in smart network maintenance. Firstly, based on the basic reputation value, we add the work enthusiasm and work activity as the indicators to measure the work efficiency of the maintenance personnel. By using Long Short-Term Memory (LSTM) model to predict the historical reputation value sequence, the work enthusiasm of the maintenance personnel is quantified. Furthermore, based on the historical movement route of the maintenance personnel, the location of maintenance personnel is predicted by using the modified Workers’ Movement Patterns (WMP) model. Finally, combined with the reputation value requirements of the emergency task and the geographical location of the maintenance personnel, the maintenance emergency task is assigned. Simulation results show that the ETARR mechanism proposed in this paper reduces the cost of task allocation, improves the efficiency of the completion of the emergency task, and can be better applied to the emergency task scenario in the smart network maintenance environment."}, "journals/peerj-cs/GaoWWHZZ22": {"title": "Robust clothing-independent gait recognition using hybrid part-based gait features.", "url": "https://doi.org/10.7717/peerj-cs.996", "year": "2022", "author": {"Zhipeng Gao": "25/2165", "Junyi Wu": "125/0678", "Tingting Wu": "20/815", "Renyu Huang": "326/3556", "Anguo Zhang": "233/3960", "Jianqiang Zhao": "147/1474"}, "abstract": ""}, "journals/pr/WuHGHZD22": {"title": "Inter-Attribute awareness for pedestrian attribute recognition.", "url": "https://doi.org/10.1016/j.patcog.2022.108865", "year": "2022", "author": {"Junyi Wu": "125/0678", "Yan Huang 0023": "75/6434-23", "Zhipeng Gao": "25/2165", "Yating Hong": "228/5093", "Jianqiang Zhao": "147/1474", "Xinsheng Du": "59/5880"}, "abstract": "The task of pedestrian attribute recognition (PAR) is to distinguish a series of person semantic attributes. Generally, existing methods adopt multi-label classification algorithms to tackle the PAR task by utilizing multiple attribute labels. Despite remarkable progress, this kind of method normally ignores relations between different attributes. In order to be aware of relations between attributes, we propose an inter-attribute aware network via vector-neuron capsule for PAR (IAA-Caps). Our IAA-Caps method replaces traditional one-dimensional scalar neurons with two-dimensional vector-neuron capsules by embedding them in IAA-Caps. Specifically, during IAA-Caps training, one dimension in capsules is used to recognize different attributes, and the other dimension is used to strengthen the relations of different attributes. Through considering inter-attribute relations, compared with previous methods that use a heavyweight backbone (e.g., ResNet50 or BN-Inception), a more lightweight backbone (i.e., OSNet) can be adopted in our proposed IAA-Caps to achieve better performance. Experiments are conducted on several PAR benchmark datasets, including PETA, PA-100K, RAPv1, and RAPv2, demonstrating the effectiveness of the proposed IAA-Caps. In addition, experiments also show that the proposed method can improve the performance of PAR on different backbones, showing its generalization ability."}, "journals/tsc/RuiCWGQW22": {"title": "Multiservice Reliability Evaluation Algorithm Considering Network Congestion and Regional Failure Based on Petri Net.", "url": "https://doi.org/10.1109/TSC.2019.2955486", "year": "2022", "author": {"Lanlan Rui": "03/4227", "Xushan Chen": "162/6745", "Xiaomei Wang": "32/6877", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Shangguang Wang": "73/8637"}, "abstract": " Abstract:With the development of complex networks and with increasing service demands, service use is becoming more complex and the composition of services is becoming more complicated. In the XaaS (X as a Service) environment, users only care about the QoE of a service and do not care about the composition process of the service. Therefore, it is important to evaluate the reliability of the entire service. In this article, we use Petri Net as a basis for modeling the composition of services. In addition, we consider the problems of shared resources and common cause faults. Both of these problems can cause network congestion and regional failures. We use distance to assess the effects of regional faults and queuing theory to simulate the network congestion process. Moreover, in the simulation, we verify the impacts of regional failures and network congestion on service reliability. We choose the Tree-Based Search algorithm and the Semi-Markov Model as comparison algorithms. The results of our algorithm are related to service time. Our algorithm can timely reflect the impact of regional failure or network congestion, and it can feedback different evaluation results according to environmental changes. Therefore, our algorithm is more comprehensive and has better performance."}, "journals/corr/abs-2204-01964": {"title": "BcMON: Blockchain Middleware for Offline Networks.", "url": "https://doi.org/10.48550/arXiv.2204.01964", "year": "2022", "author": {"Yijing Lin": "275/1523", "Zhipeng Gao": "25/2165", "Qian Wang": "75/5723", "Lanlan Rui": "03/4227", "Yang Yang 0006": "48/450-6"}, "abstract": "\n      Abstract:  Blockchain is becoming a new generation of information infrastructures.\nHowever, the current blockchain solutions rely on a continuous connectivity\nnetwork to query and modify the state of the blockchain. The emerging satellite\ntechnology seems to be a good catalyst to forward offline transactions to the\nblockchain. However, this approach suffers expensive costs, difficult\ninteroperability, and limited computation problems. Therefore, we propose\nBcMON, the first blockchain middleware for offline networks. BcMON incorporates\nthree innovative designs: 1) it reduces the costs of offline transactions\naccessing the blockchain through Short Message Service (SMS), 2) it validates\nthe authenticity of offline cross-chain transactions by two-phase consensus, 3)\nit supports offline clients to perform complex queries and computations on the\nblockchains. The prototype of BcMON has been implemented to evaluate the\nperformance of the proposed middleware, which can show its stability,\nefficiency, and scalability.\n\n    "}, "journals/jei/YangWSGHS21": {"title": "Unidirectional information-interaction network for person re-identification.", "url": "https://doi.org/10.1117/1.jei.30.4.043023", "year": "2021", "author": {"Qingqing Yang": "49/10304", "Junyi Wu": "125/0678", "Qishan Song": "301/3360", "Zhipeng Gao": "25/2165", "Liqin Huang": "45/9302", "Zhigang Song": "47/5816"}, "abstract": ""}, "journals/jnca/RuiZGQWX21": {"title": "Service migration in multi-access edge computing: A joint state adaptation and reinforcement learning mechanism.", "url": "https://doi.org/10.1016/j.jnca.2021.103058", "year": "2021", "author": {"Lanlan Rui": "03/4227", "Menglei Zhang": "156/2390", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Zhili Wang": "15/2643", "Ao Xiong": "119/8978"}, "abstract": "With the development of the internet of things (IoT), the concept of an edge network has been gradually expanding to other fields including internet of vehicles, mobile communication networks and smart grids. Because the resources of terminals are limited, the long-distance movements of users will increase the running costs of the services that are offloaded to edge servers, and even the services on terminals will stop running. Another problem is that resource shortages or hardware failures of these edge networks can affect the service migration policy. In this paper, a novel service migration method based on state adaptation and deep reinforcement learning is proposed to efficiently overcome network failures. Before migration, we define four edge network states to discuss the migration policy and adopt the two-dimensional movement around the edge servers to adapt to the applications scenarios of our work. Then, we use the satisfiability modulo theory (SMT) method to solve the candidate space of migration policies based on cost constraints, delay constraints and available resource capacity constraints to shorten the interruption time. Finally, the service migration problem can be transformed into the optimal destination server and low-cost migration path problem based on the Markov decision process by the deep Q-learning (DQN) algorithm. Moreover, we theoretically prove the rate of convergence in the learning rate function of our algorithm to improve the convergence rate. Our experimental results demonstrate that our proposed service migration mechanism can effectively shorten the delays from service interruptions, and better avoid the impact of edge network failure on the migration results and, thus, improve the users’ satisfaction."}, "journals/jnca/RuiZGQWX21a": {"title": "Corrigendum to \"Service migration in multi-access edge computing: A joint state adaptation and reinforcement learning mechanism\" [J. Netw. Comput. Appl. 183-184 (2021) 103058].", "url": "https://doi.org/10.1016/j.jnca.2021.103085", "year": "2021", "author": {"Lanlan Rui": "03/4227", "Menglei Zhang": "156/2390", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Zhili Wang": "15/2643", "Ao Xiong": "119/8978"}, "abstract": ""}, "journals/tii/RuiZGQ21": {"title": "CLPM: A Cooperative Link Prediction Model for Industrial Internet of Things Using Partitioned Stacked Denoising Autoencoder.", "url": "https://doi.org/10.1109/TII.2020.2999318", "year": "2021", "author": {"Lanlan Rui": "03/4227", "Yu Zhu": "38/5267", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:With the development of Industry 4.0, an increasing number of industrial Internet of Things (IIoT) mobile devices (MD), which constantly transmit data at any time, are working on the production line. However, due to node movement, signal attenuation, or physical obstacles, data must rely on the transmission of relay nodes to finally reach the destination node. Based on this scenario, in this article, we propose a cooperative link prediction model (CLPM) using a stacked denoising autoencoder (SDAE) to predict links of the IIoT-based MDs at the next moment through historical link information. The layer structure of the SDAE model is partitioned so that the local MD and edge servers can cooperatively process the link prediction tasks. Experimental results show that our proposed CLPM outperforms others in terms of prediction performance and execution delay."}, "journals/tii/RuiCGQ21": {"title": "MLPRA: An MCDS and Link-Priority-Based Network Repair Algorithm for Smart Grid.", "url": "https://doi.org/10.1109/TII.2020.3012407", "year": "2021", "author": {"Lanlan Rui": "03/4227", "Xushan Chen": "162/6745", "Zhipeng Gao": "25/2165", "Xue-song Qiu 0001": "13/342"}, "abstract": " Abstract:The power system is an infrastructure for industrial manufacturing, and its availability is relevant to industrial systems. The smart grid combines communication systems with sensing devices to provide intelligent management tools for fault monitoring and processing of the power grid. Regional failures caused by natural disasters can have a large impact on the power system. To reduce the impact of disasters, an efficient network repair strategy is needed. This article focuses on the emergency repair strategy of a power communication network under disaster conditions and seeks to ensure the operation of the power system with fewer repairs. Combined with the analysis of cascading failures and regional failures, a communication network fast repair algorithm is proposed. The coupled network is constructed in the simulation part to verify the algorithm. The results show that with a limited number of node repairs, the algorithm can ensure the highest percentage of workable nodes."}, "journals/tnsm/RuiCGLQM21": {"title": "Petri Net-Based Reliability Assessment and Migration Optimization Strategy of SFC.", "url": "https://doi.org/10.1109/TNSM.2020.3045705", "year": "2021", "author": {"Lanlan Rui": "03/4227", "Xushan Chen": "162/6745", "Zhipeng Gao": "25/2165", "Wenjing Li 0001": "08/6548-1", "Xuesong Qiu 0001": "13/342", "Luoming Meng": "52/382"}, "abstract": " Abstract:With the development of information technology, the network consists of various proprietary hardware devices, and the use of these devices brings problems. To solve problems, network function virtualization is proposed, which decouples the software and hardware in the network, and deploys the existing network function devices to a common physical platform. However, network virtualization needs will inevitably face reliability problems during resource virtualization and service function chain deployment. This article proposes a service function chain reliability evaluation method and reliability optimization algorithm. The composition relationship and reliability influencing factors of service function chain were analyzed, including resource preemption, common cause failure, fault recovery and redundant backup. The service function chain was modeled as a Petri net model, and reliability evaluation results related to execution time were obtained. Based on the reliability assessment results, a VNF migration strategy is designed, with reliability as the optimization goal while considering costs. Simulation results show that, compared with the reliability optimization strategy based on backup, our algorithm costs less and reduces the impact of resource preemption on service reliability."}, "journals/tomccap/WuHWGZH21": {"title": "Dual-Stream Guided-Learning via ", "url": "https://doi.org/10.1145/3447715", "year": "2021", "author": {"Junyi Wu": "125/0678", "Yan Huang 0023": "75/6434-23", "Qiang Wu 0001": "87/2533-1", "Zhipeng Gao": "25/2165", "Jianqiang Zhao": "147/1474", "Liqin Huang": "45/9302"}, "abstract": "The task of person re-identification (re-ID) is to find the same pedestrian across non-overlapping camera views. Generally, the performance of person re-ID can be affected by background clutter. However, existing segmentation algorithms cannot obtain perfect foreground masks to cover the background information clearly. In addition, if the background is completely removed, some discriminative ID-related cues (i.e., backpack or companion) may be lost. In this article, we design a dual-stream network consisting of a Provider Stream (P-Stream) and a Receiver Stream (R-Stream). The R-Stream performs an a priori optimization operation on foreground information. The P-Stream acts as a pusher to guide the R-Stream to concentrate on foreground information and some useful ID-related cues in the background. The proposed dual-stream network can make full use of the a priori optimization and guided-learning strategy to learn encouraging foreground information and some useful ID-related information in the background. Our method achieves Rank-1 accuracy of 95.4% on Market-1501, 89.0% on DukeMTMC-reID, 78.9% on CUHK03 (labeled), and 75.4% on CUHK03 (detected), outperforming state-of-the-art methods."}, "journals/tosem/GaoXLG21": {"title": "Technical Q8A Site Answer Recommendation via Question Boosting.", "url": "https://doi.org/10.1145/3412845", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John Grundy 0001": "g/JohnCGrundy"}, "abstract": "Software developers have heavily used online question-and-answer platforms to seek help to solve their technical problems. However, a major problem with these technical Q8A sites is “answer hungriness,” i.e., a large number of questions remain unanswered or unresolved, and users have to wait for a long time or painstakingly go through the provided answers with various levels of quality. To alleviate this time-consuming problem, we propose a novel DEEPANS neural network–based approach to identify the most relevant answer among a set of answer candidates. Our approach follows a three-stage process: question boosting, label establishment, and answer recommendation. Given a post, we first generate a clarifying question as a way of question boosting. We automatically establish the positive, neutral+, neutral-, and negative training samples via label establishment. When it comes to answer recommendation, we sort answer candidates by the matching scores calculated by our neural network–based model. To evaluate the performance of our proposed model, we conducted a large-scale evaluation on four datasets, collected from the real-world technical Q8A sites (i.e., Ask Ubuntu, Super User, Stack Overflow Python, and Stack Overflow Java). Our experimental results show that our approach significantly outperforms several state-of-the-art baselines in automatic evaluation. We also conducted a user study with 50 solved/unanswered/unresolved questions. The user-study results demonstrate that our approach is effective in solving the answer-hungry problem by recommending the most relevant answers from historical archives."}, "journals/tse/GaoJXLG21": {"title": "Checking Smart Contracts With Structural Code Embedding.", "url": "https://doi.org/10.1109/TSE.2020.2971482", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Lingxiao Jiang": "82/3572", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John Grundy 0001": "g/JohnCGrundy"}, "abstract": " Abstract:Smart contracts have been increasingly used together with blockchains to automate financial and business transactions. However, many bugs and vulnerabilities have been identified in many contracts which raises serious concerns about smart contract security, not to mention that the blockchain systems on which the smart contracts are built can be buggy. Thus, there is a significant need to better maintain smart contract code and ensure its high reliability. In this paper, we propose an automated approach to learn characteristics of smart contracts in Solidity, which is useful for clone detection, bug detection and contract validation on smart contracts. Our new approach is based on word embeddings and vector space comparison. We parse smart contract code into word streams with code structural information, convert code elements (e.g., statements, functions) into numerical vectors that are supposed to encode the code syntax and semantics, and compare the similarities among the vectors encoding code and known bugs, to identify potential issues. We have implemented the approach in a prototype, named S\nmart\nE\nmbed\n,\n1\n1.The anonymous replication packages can be accessed at: https://drive.google.com/file/d/1kauLT3y2IiHPkUlVx4FSTda-dVAyL4za/view?usp=sharing. \n and evaluated it with more than 22,000 smart contracts collected from the Ethereum blockchain. Results show that our tool can effectively identify many repetitive instances of Solidity code, where the clone ratio is around 90 percent. Code clones such as type-III or even type-IV semantic clones can also be detected accurately. Our tool can identify more than 1000 clone related bugs based on our bug databases efficiently and accurately. Our tool can also help to efficiently validate any given smart contract against a known set of bugs, which can help to improve the users’ confidence in the reliability of the contract."}, "journals/corr/abs-2104-12086": {"title": "FedSup: A Communication-Efficient Federated Learning Fatigue Driving Behaviors Supervision Framework.", "url": "https://arxiv.org/abs/2104.12086", "year": "2021", "author": {"Chen Zhao": "81/3", "Zhipeng Gao": "25/2165", "Qian Wang": "75/5723", "Kaile Xiao": "194/4868", "Zijia Mo": "240/6361", "M. Jamal Deen": "08/2733"}, "abstract": "\n      Abstract:  With the proliferation of edge smart devices and the Internet of Vehicles\n(IoV) technologies, intelligent fatigue detection has become one of the\nmost-used methods in our daily driving. To improve the performance of the\ndetection model, a series of techniques have been developed. However, existing\nwork still leaves much to be desired, such as privacy disclosure and\ncommunication cost. To address these issues, we propose FedSup, a\nclient-edge-cloud framework for privacy and efficient fatigue detection.\nInspired by the federated learning technique, FedSup intelligently utilizes the\ncollaboration between client, edge, and cloud server to realizing dynamic model\noptimization while protecting edge data privacy. Moreover, to reduce the\nunnecessary system communication overhead, we further propose a Bayesian\nconvolutional neural network (BCNN) approximation strategy on the clients and\nan uncertainty weighted aggregation algorithm on the cloud to enhance the\ncentral model training efficiency. Extensive experiments demonstrate that the\nFedSup framework is suitable for IoV scenarios and outperforms other mainstream\nmethods.\n\n    "}, "journals/corr/abs-2108-05846": {"title": "Automating the Removal of Obsolete TODO Comments.", "url": "https://arxiv.org/abs/2108.05846", "year": "2021", "author": {"Zhipeng Gao": "25/2165", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John C. Grundy": "g/JohnCGrundy", "Thomas Zimmermann 0001": "19/3622-1"}, "abstract": "\n      Abstract:  TODO comments are very widely used by software developers to describe their\npending tasks during software development. However, after performing the task\ndevelopers sometimes neglect or simply forget to remove the TODO comment,\nresulting in obsolete TODO comments. These obsolete TODO comments can confuse\ndevelopment teams and may cause the introduction of bugs in the future,\ndecreasing the software's quality and maintainability. In this work, we propose\na novel model, named TDCleaner (TODO comment Cleaner), to identify obsolete\nTODO comments in software projects. TDCleaner can assist developers in\njust-in-time checking of TODO comments status and avoid leaving obsolete TODO\ncomments. Our approach has two main stages: offline learning and online\nprediction. During offline learning, we first automatically establish\n<code_change, todo_comment, commit_msg> training samples and leverage three\nneural encoders to capture the semantic features of TODO comment, code change\nand commit message respectively. TDCleaner then automatically learns the\ncorrelations and interactions between different encoders to estimate the final\nstatus of the TODO comment. For online prediction, we check a TODO comment's\nstatus by leveraging the offline trained model to judge the TODO comment's\nlikelihood of being obsolete. We built our dataset by collecting TODO comments\nfrom the top-10,000 Python and Java Github repositories and evaluated TDCleaner\non them. Extensive experimental results show the promising performance of our\nmodel over a set of benchmarks. We also performed an in-the-wild evaluation\nwith real-world software projects, we reported 18 obsolete TODO comments\nidentified by TDCleaner to Github developers and 9 of them have already been\nconfirmed and removed by the developers, demonstrating the practical usage of\nour approach.\n\n    "}, "journals/access/YangDQG20": {"title": "FGGAN: Feature-Guiding Generative Adversarial Networks for Text Generation.", "url": "https://doi.org/10.1109/ACCESS.2020.2993928", "year": "2020", "author": {"Yang Yang 0006": "48/450-6", "Xiaodong Dan": "268/1087", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165"}, "abstract": " Abstract:Text generation is a basic work of natural language processing, which plays an important role in dialogue system and intelligent translation. As a kind of deep learning framework, Generative Adversarial Networks (GAN) has been widely used in text generation. In combination with reinforcement learning, GAN uses the output of discriminator as reward signal of reinforcement learning to guide generator training, but the reward signal is a scalar and the guidance is weak. This paper proposes a text generation model named Feature-Guiding Generative Adversarial Networks (FGGAN). To solve the problem of insufficient feedback guidance from the discriminator network, FGGAN uses a feature guidance module to extract text features from the discriminator network, convert them into feature guidance vectors and feed them into the generator network for guidance. In addition, sampling is required to complete the sequence before feeding it into the discriminator to get feedback signal in text generation. However, the randomness and insufficiency of the sampling method lead to poor quality of generated text. This paper formulates text semantic rules to restrict the token of the next time step in the sequence generation process and remove semantically unreasonable tokens to improve the quality of generated text. Finally, text generation experiments are performed on different datasets and the results verify the effectiveness and superiority of FGGAN."}, "journals/amc/GaoLW20": {"title": "Rainbow domination numbers of generalized Petersen graphs.", "url": "https://doi.org/10.1016/j.amc.2020.125341", "year": "2020", "author": {"Zhipeng Gao": "25/2165", "Hui Lei": "181/2828", "Kui Wang": "55/6416"}, "abstract": "Domination and its variations in graphs are natural models for the location problems in operations research. In this paper, we investigate the rainbow domination number of graphs, which was introduced by Brešar, Henning and Rall. Given a graph G and a positive integer t, a t-rainbow dominating function of G is a function f from vertex set to the set of all subsets of {1, 2, ⋅⋅⋅, t} such that for any vertex v with f(v)=ϕ, we have ⋃u∈N(v)f(u)={1,2,⋯,t}. The t-rainbow domination problem is to determine the t-rainbow domination number γrt(G) of G, that is the minimum value of ∑v ∈ V(G)|f(v)|, where f runs over all t-rainbow dominating functions of G. The domination number and its variations of generalized Petersen graphs P(n, k) are widely investigated. The exact values of γr2(P(n, 1)) and γr3(P(n, 1)) are already determined in [11, 12]. In this paper, we determine the exact values of γrt(P(n, 1)) for any t ≥ 8 and t=4 and prove that γrt(P(2k,k))=4k for t ≥ 6, where P(2k, k) is a special graph."}, "journals/fgcs/GuoRG20": {"title": "A zone-based content pre-caching strategy in vehicular edge networks.", "url": "https://doi.org/10.1016/j.future.2019.12.050", "year": "2020", "author": {"Hui Guo": "70/2221", "Lanlan Rui": "03/4227", "Zhipeng Gao": "25/2165"}, "abstract": "Content pre-caching is a kind of significant technology to lower response delay and improve network performance, especially for the delay-sensitive services in dynamic vehicular edge networks. Therefore, in this paper, we propose a zone-based content pre-caching strategy, which aims to implement an active content caching through two algorithms: pre-caching zone selecting algorithm- PCZS and pre-caching node selecting algorithm- PCNS. Firstly, we organize the edge servers (ESs) with a zone-based way at the edge, and assign a Manager node to collect the information of each zone; with the help of road topology and tables information recorded in Manager nodes, PCZS can predict the vehicle motion and zone sojourn time with a high accuracy, and further get a content pre-caching zone by comparing estimated request delay and zone sojourn time; then, PCNS checks whether the content has been cached in the CST of the zone selected by PCZS, if CST hits, the pre-caching process is over, otherwise, by combining ES centrality, load degree with content popularity, we perform PCNS to select a specific ES node to pre-cache the content; simulation results show that our strategy has a higher prediction accuracy and dynamic adaptability, it also outperforms in terms of average response delay and cache hit ratio."}, "journals/fgcs/XiaoGSQYR20": {"title": "EdgeABC: An architecture for task offloading and resource allocation in the Internet of Things.", "url": "https://doi.org/10.1016/j.future.2020.02.026", "year": "2020", "author": {"Kaile Xiao": "194/4868", "Zhipeng Gao": "25/2165", "Weisong Shi": "s/WeisongShi", "Xuesong Qiu 0001": "13/342", "Yang Yang 0006": "48/450-6", "Lanlan Rui": "03/4227"}, "abstract": "The evolving Internet of Things (IoT) faces considerable challenges in terms of sensitive delay requirements for tasks, reasonable allocation requirements of resources, and reliability requirements of resource transactions. In the paper, considering these problems, we propose an emerging IoT architecture EdgeABC introduced the blockchain to ensure the integrity of resource transaction data and the profits of service provider, and we also propose a Task Offloading and Resource Allocation (TO-RA) algorithm, where the TO-RA algorithm is implemented on the blockchain in the form of smart contracts. In other words, the architecture proposed optimizes the resource allocation in IoT based on the advantages of the blockchain. Specifically, we first propose a subtask-virtual machine mapping strategy to complete the Task Offloading (TO) and the first allocation of resources; then aiming at the possible load imbalance problem of the system, we propose a stack cache supplement mechanism to complete the Resource Allocation (RA) based on the TO strategy. Finally, simulation experiments verify that the fairness, user satisfaction, and system utility of the TO-RA algorithm are superior to traditional algorithms."}, "journals/iotj/XiaoSGYQ20": {"title": "DAER: A Resource Preallocation Algorithm of Edge Computing Server by Using Blockchain in Intelligent Driving.", "url": "https://doi.org/10.1109/JIOT.2020.2984553", "year": "2020", "author": {"Kaile Xiao": "194/4868", "Weisong Shi": "s/WeisongShi", "Zhipeng Gao": "25/2165", "Congcong Yao": "252/1219", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:The introduction of edge computing (EC) in intelligent driving allows the vehicle to offload tasks to the EC server closer to the vehicle side, creating a new paradigm for task offloading and resource allocation. The movement of the vehicle, the time sensitivity of the processing data, and the resource allocation of the EC server have become bottlenecks of the rapid development of intelligent driving. In this article, we jointly considered the problems of the network economy and resource allocation. In order to eliminate dependence on third parties, we propose a resource transaction architecture based on the blockchain. Moreover, we propose the dynamic allocation algorithm of edge resources (DAERs) based on the double auction mechanism to maximize the satisfaction of users and service providers of edge computing (SPs), where the DAER algorithm is implemented in the form of smart contracts in the blockchain architecture. In particular, we propose the state search algorithm that can improve the prediction accuracy of the staged destination of the vehicle to help allocate resources reasonably. Through simulation experiments, we verify the superior performance of the DAER algorithm in terms of resource utilization rate and the satisfaction of both parties participating in the auction."}, "journals/jis/YangBGGQ20": {"title": "LAZY R-tree: The R-tree with lazy splitting algorithm.", "url": "https://doi.org/10.1177/0165551519828616", "year": "2020", "author": {"Yang Yang 0006": "48/450-6", "Pengwei Bai": "259/0280", "Ningling Ge": "259/0190", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342"}, "abstract": ""}, "journals/ppna/WangGLDG20": {"title": "An optimal uplink traffic offloading algorithm via opportunistic communications based on machine learning.", "url": "https://doi.org/10.1007/s12083-020-00904-7", "year": "2020", "author": {"Qian Wang 0015": "75/5723-15", "Zhipeng Gao": "25/2165", "Zifan Li": "125/3616", "Xiaojiang Du": "22/5535", "Mohsen Guizani": "15/1123"}, "abstract": "Opportunistic communications as an efficient traffic offloading method can be used to offload uplink traffic of cellular networks to Wi-Fi networks. However, because of its contact pattern (contact frequency and contact duration) the offloading method could not ensure the data to be successfully offloaded to Wi-Fi Access Points (APs) within a time constraint. In this paper, we focus on maximizing the probability of offloading data to Wi-Fi APs by fragmenting the data and assigning the fragments to different direct or indirect paths generated by opportunistic contacts. Firstly, we propose two methods based on mobility prediction, which is realized by machine learning, to separately calculate the probability of offloading data to Wi-Fi APs by the direct offloading path considering multiple opportunistic contacts and contact duration, and the probability of indirectly offloading data to Wi-Fi APs by the indirect offloading path. Then, based on the probability calculation methods the offloading probability maximization is formulated as a non-linear integer programming problem, and we propose a distributed heuristic algorithm to solve it considering complexity of the probability calculation and limited computation capacities of devices. Simulation results prove the data offloading probability of our proposed algorithm outperforms other algorithms under different simulation environment."}, "journals/tosem/Gao000L20": {"title": "Generating Question Titles for Stack Overflow from Mined Code Snippets.", "url": "https://doi.org/10.1145/3401026", "year": "2020", "author": {"Zhipeng Gao": "25/2165", "Xin Xia 0001": "06/2072-1", "John Grundy 0001": "g/JohnCGrundy", "David Lo 0001": "89/6793-1", "Yuan-Fang Li": "20/2537"}, "abstract": "Stack Overflow has been heavily used by software developers as a popular way to seek programming-related information from peers via the internet. The Stack Overflow community recommends users to provide the related code snippet when they are creating a question to help others better understand it and offer their help. Previous studies have shown that a significant number of these questions are of low-quality and not attractive to other potential experts in Stack Overflow. These poorly asked questions are less likely to receive useful answers and hinder the overall knowledge generation and sharing process. Considering one of the reasons for introducing low-quality questions in SO is that many developers may not be able to clarify and summarize the key problems behind their presented code snippets due to their lack of knowledge and terminology related to the problem, and/or their poor writing skills, in this study we propose an approach to assist developers in writing high-quality questions by automatically generating question titles for a code snippet using a deep sequence-to-sequence learning approach. Our approach is fully data-driven and uses an attention mechanism to perform better content selection, a copy mechanism to handle the rare-words problem and a coverage mechanism to eliminate word repetition problem. We evaluate our approach on Stack Overflow datasets over a variety of programming languages (e.g., Python, Java, Javascript, C# and SQL) and our experimental results show that our approach significantly outperforms several state-of-the-art baselines in both automatic and human evaluation. We have released our code and datasets to facilitate other researchers to verify their ideas and inspire the follow up work."}, "journals/wcl/GaoWLZW20": {"title": "FFDNet-Based Channel Estimation for Massive MIMO Visible Light Communication Systems.", "url": "https://doi.org/10.1109/LWC.2019.2954511", "year": "2020", "author": {"Zhipeng Gao": "25/2165", "Yuhao Wang 0001": "54/518-1", "Xiaodong Liu 0006": "65/622-6", "Fuhui Zhou": "155/5190", "Kai-Kit Wong": "35/5506"}, "abstract": " Abstract:Channel estimation is of crucial importance in massive multiple-input multiple-output (m-MIMO) visible light communication (VLC) systems. In order to tackle this problem, a fast and flexible denoising convolutional neural network (FFDNet)-based channel estimation scheme for m-MIMO VLC systems was proposed. The channel matrix of the m-MIMO VLC channel is identified as a two-dimensional natural image since the channel has the characteristic of sparsity. A deep learning-enabled image denoising network FFDNet is exploited to learn from a large number of training data and to estimate the m-MIMO VLC channel. Simulation results demonstrate that our proposed channel estimation based on the FFDNet significantly outperforms the benchmark scheme based on minimum mean square error."}, "journals/corr/abs-2001-07125": {"title": "Checking Smart Contracts with Structural Code Embedding.", "url": "https://arxiv.org/abs/2001.07125", "year": "2020", "author": {"Zhipeng Gao": "25/2165", "Lingxiao Jiang": "82/3572", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John C. Grundy": "g/JohnCGrundy"}, "abstract": "\n      Abstract:  Smart contracts have been increasingly used together with blockchains to\nautomate financial and business transactions. However, many bugs and\nvulnerabilities have been identified in many contracts which raises serious\nconcerns about smart contract security, not to mention that the blockchain\nsystems on which the smart contracts are built can be buggy. Thus, there is a\nsignificant need to better maintain smart contract code and ensure its high\nreliability. In this paper, we propose an automated approach to learn\ncharacteristics of smart contracts in Solidity, which is useful for clone\ndetection, bug detection and contract validation on smart contracts. Our new\napproach is based on word embeddings and vector space comparison. We parse\nsmart contract code into word streams with code structural information, convert\ncode elements (e.g., statements, functions) into numerical vectors that are\nsupposed to encode the code syntax and semantics, and compare the similarities\namong the vectors encoding code and known bugs, to identify potential issues.\nWe have implemented the approach in a prototype, named SmartEmbed. Results show\nthat our tool can effectively identify many repetitive instances of Solidity\ncode, where the clone ratio is around 90\\%. Code clones such as type-III or\neven type-IV semantic clones can also be detected accurately. Our tool can\nidentify more than 1000 clone related bugs based on our bug databases\nefficiently and accurately. Our tool can also help to efficiently validate any\ngiven smart contract against a known set of bugs, which can help to improve the\nusers' confidence in the reliability of the contract.\nThe anonymous replication packages can be accessed at:\nthis https URL,\nand evaluated it with more than 22,000 smart contracts collected from the\nEthereum blockchain.\n\n    "}, "journals/corr/abs-2005-10157": {"title": "Generating Question Titles for Stack Overflow from Mined Code Snippets.", "url": "https://arxiv.org/abs/2005.10157", "year": "2020", "author": {"Zhipeng Gao": "25/2165", "Xin Xia 0001": "06/2072-1", "John C. Grundy": "g/JohnCGrundy", "David Lo 0001": "89/6793-1", "Yuan-Fang Li": "20/2537"}, "abstract": "\n      Abstract:  Stack Overflow has been heavily used by software developers as a popular way\nto seek programming-related information from peers via the internet. The Stack\nOverflow community recommends users to provide the related code snippet when\nthey are creating a question to help others better understand it and offer\ntheir help. Previous studies have shown that} a significant number of these\nquestions are of low-quality and not attractive to other potential experts in\nStack Overflow. These poorly asked questions are less likely to receive useful\nanswers and hinder the overall knowledge generation and sharing process.\nConsidering one of the reasons for introducing low-quality questions in SO is\nthat many developers may not be able to clarify and summarize the key problems\nbehind their presented code snippets due to their lack of knowledge and\nterminology related to the problem, and/or their poor writing skills, in this\nstudy we propose an approach to assist developers in writing high-quality\nquestions by automatically generating question titles for a code snippet using\na deep sequence-to-sequence learning approach. Our approach is fully\ndata-driven and uses an attention mechanism to perform better content\nselection, a copy mechanism to handle the rare-words problem and a coverage\nmechanism to eliminate word repetition problem. We evaluate our approach on\nStack Overflow datasets over a variety of programming languages (e.g., Python,\nJava, Javascript, C# and SQL) and our experimental results show that our\napproach significantly outperforms several state-of-the-art baselines in both\nautomatic and human evaluation. We have released our code and datasets to\nfacilitate other researchers to verify their ideas and inspire the follow-up\nwork.\n\n    "}, "journals/corr/abs-2007-10851": {"title": "Code2Que: A Tool for Improving Question Titles from Mined Code Snippets in Stack Overflow.", "url": "https://arxiv.org/abs/2007.10851", "year": "2020", "author": {"Zhipeng Gao": "25/2165", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John C. Grundy": "g/JohnCGrundy", "Yuan-Fang Li": "20/2537"}, "abstract": "\n      Abstract:  Stack Overflow is one of the most popular technical Q&A sites used by\nsoftware developers. Seeking help from Stack Overflow has become an essential\npart of software developers' daily work for solving programming-related\nquestions. Although the Stack Overflow community has provided quality assurance\nguidelines to help users write better questions, we observed that a significant\nnumber of questions submitted to Stack Overflow are of low quality. In this\npaper, we introduce a new web-based tool, Code2Que, which can help developers\nin writing higher quality questions for a given code snippet. Code2Que consists\nof two main stages: offline learning and online recommendation. In the offline\nlearning phase, we first collect a set of good quality <code snippet, question>\npairs as training samples. We then train our model on these training samples\nvia a deep sequence-to-sequence approach, enhanced with an attention mechanism,\na copy mechanism and a coverage mechanism. In the online recommendation phase,\nfor a given code snippet, we use the offline trained model to generate question\ntitles to assist less experienced developers in writing questions more\neffectively. At the same time, we embed the given code snippet into a vector\nand retrieve the related questions with similar problematic code snippets.\n\n    "}, "journals/corr/abs-2008-04093": {"title": "When Deep Learning Meets Smart Contracts.", "url": "https://arxiv.org/abs/2008.04093", "year": "2020", "author": {"Zhipeng Gao": "25/2165"}, "abstract": "\n      Abstract:  Ethereum has become a widely used platform to enable secure, Blockchain-based\nfinancial and business transactions. However, many identified bugs and\nvulnerabilities in smart contracts have led to serious financial losses, which\nraises serious concerns about smart contract security. Thus, there is a\nsignificant need to better maintain smart contract code and ensure its high\nreliability. In this research: (1) Firstly, we propose an automated deep\nlearning based approach to learn structural code embeddings of smart contracts\nin Solidity, which is useful for clone detection, bug detection and contract\nvalidation on smart contracts. We apply our approach to more than 22K solidity\ncontracts collected from the Ethereum blockchain, results show that the clone\nratio of solidity code is at around 90%, much higher than traditional software.\nWe collect a list of 52 known buggy smart contracts belonging to 10 kinds of\ncommon vulnerabilities as our bug database. Our approach can identify more than\n1000 clone related bugs based on our bug databases efficiently and accurately.\n(2) Secondly, according to developers' feedback, we have implemented the\napproach in a web-based tool, named SmartEmbed, to facilitate Solidity\ndevelopers for using our approach. Our tool can assist Solidity developers to\nefficiently identify repetitive smart contracts in the existing Ethereum\nblockchain, as well as checking their contract against a known set of bugs,\nwhich can help to improve the users' confidence in the reliability of the\ncontract. We optimize the implementations of SmartEmbed which is sufficient in\nsupporting developers in real-time for practical uses. The Ethereum ecosystem\nas well as the individual Solidity developer can both benefit from our\nresearch.\n\n    "}, "journals/access/YangHHCG19": {"title": "Entity Alignment Algorithm Based on Dual-Attention and Incremental Learning Mechanism.", "url": "https://doi.org/10.1109/ACCESS.2019.2951785", "year": "2019", "author": {"Yang Yang 0006": "48/450-6", "Maojie Hao": "253/1606", "Yonghua Huo": "189/8253", "Liandong Chen": "231/4740", "Zhipeng Gao": "25/2165"}, "abstract": " Abstract:With the development of artificial intelligence and big data technology, large-scale general knowledge map construction is becoming increasingly important. One of the most efficient methods is undoubtedly the integration of existing knowledge maps, and entity alignment is the key in the process of knowledge map fusion. The merits of the entity alignment algorithm directly affect the efficiency and accuracy of the knowledge map fusion. However, there are some problems with the current Chinese knowledge map entity alignment algorithm, such as its low accuracy, difficulty in generating solid vectors, and difficulty in obtaining a priori alignment data. In this paper, the entity alignment algorithm is understood to be a neural network binary classification model, and we propose an entity alignment algorithm based on the dual-attention mechanism. The algorithm improves the entity vector training process, proposes a dual-attention mechanism, and applies an incremental learning mechanism. The experiments show that the improvements proposed in this paper effectively improve the classification accuracy of the algorithm, and the overall effect of the algorithm is better than that of the existing physical alignment algorithm."}, "journals/iotj/RuiYGQ19": {"title": "Computation Offloading in a Mobile Edge Communication Network: A Joint Transmission Delay and Energy Consumption Dynamic Awareness Mechanism.", "url": "https://doi.org/10.1109/JIOT.2019.2939874", "year": "2019", "author": {"Lanlan Rui": "03/4227", "Yingtai Yang": "255/2198", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:Various problems arise in the maintenance of communication networks. For example, on-site maintenance personnel have insufficient work experience. Devices used for maintenance work have limited computing resources and battery life. Moreover, most maintenance systems still use the centralized single processing mode of traditional cloud computing, which increases the data center computing pressure and slows the data flow. To overcome these problems, we propose a communication network edge maintenance system based on smart wearable technology and introduce computation offloading technology for mobile edge computing (MEC). Before offloading, we propose a multimerged computing sorting segmentation (MCSS) algorithm to divide a part of the task to offload. When making an offloading decision, we access a suitable MEC service node for each user with the lowest transmission cost and establish a related model. We use an improved Kuhn-Munkras (KM) algorithm that considers fairness among users to solve this model. After that, we propose a dynamic energy-efficiency awareness strategy. When tasks are processed locally, we optimize the CPU clock frequency. When tasks are offloaded, we adaptively allocate the transmission power. Finally, we conduct a simulation experiment. The results demonstrate that the proposed scheme can reduce the transmission cost and improve the performance, thereby increasing the level of on-site maintenance work."}, "journals/tvt/WangDGG19": {"title": "An Optimal Channel Occupation Time Adjustment Method for LBE in Unlicensed Spectrum.", "url": "https://doi.org/10.1109/TVT.2019.2940123", "year": "2019", "author": {"Qian Wang 0015": "75/5723-15", "Xiaojiang Du": "22/5535", "Zhipeng Gao": "25/2165", "Mohsen Guizani": "15/1123"}, "abstract": " Abstract:Load Based Equipment (LBE) mechanism is a category of Listen-Before-Talk (LBT) protocol adopted by LTE to access unlicensed channels to realize fair coexistence between LTE and Wi-Fi networks. However, most LBE optimization methods fix Channel Occupation Time (COT) in LBE neglecting its influence on network throughput and fairness. In the paper, considering the influence, we propose an optimal COT adjustment method for the LBE mechanism to maximize the throughput of LTE while satisfying the LTE users data rate demands and ensuring the fairness coexistence between LTE and Wi-Fi networks. To this end, we first propose a method to separately calculate the throughput of LTE network and Wi-Fi network on the coexistence unlicensed spectrum considering the synchronous of LTE on licensed and unlicensed spectrum, and then utilize a virtual Wi-Fi network construction method to assure the used fairness criterion. Based on the aforementioned methods the problem of throughput maximization of LTE is formulated as a constrained non-linear optimization problem solved by an optimal algorithm. Simulation results prove our proposed throughput calculation method is valid, and then demonstrate the LBE mechanism using our proposed method optimizes the throughput of the SBS network and ensures fairness."}, "journals/corr/abs-1908-08615": {"title": "SmartEmbed: A Tool for Clone and Bug Detection in Smart Contracts through Structural Code Embedding.", "url": "http://arxiv.org/abs/1908.08615", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Vinoj Jayasundara": "211/4067", "Lingxiao Jiang": "82/3572", "Xin Xia 0001": "06/2072-1", "David Lo 0001": "89/6793-1", "John C. Grundy": "g/JohnCGrundy"}, "abstract": "\n      Abstract:  Ethereum has become a widely used platform to enable secure, Blockchain-based\nfinancial and business transactions. However, a major concern in Ethereum is\nthe security of its smart contracts. Many identified bugs and vulnerabilities\nin smart contracts not only present challenges to maintenance of blockchain,\nbut also lead to serious financial loses. There is a significant need to better\nassist developers in checking smart contracts and ensuring their this http URL\nthis paper, we propose a web service tool, named SmartEmbed, which can help\nSolidity developers to find repetitive contract code and clone-related bugs in\nsmart contracts. Our tool is based on code embeddings and similarity checking\ntechniques. By comparing the similarities among the code embedding vectors for\nexisting solidity code in the Ethereum blockchain and known bugs, we are able\nto efficiently identify code clones and clone-related bugs for any solidity\ncode given by users, which can help to improve the users' confidence in the\nreliability of their code. In addition to the uses by individual developers,\nSmartEmbed can also be applied to studies of smart contracts in a large scale.\nWhen applied to more than 22K solidity contracts collected from the Ethereum\nblockchain, we found that the clone ratio of solidity code is close to 90\\%,\nmuch higher than traditional software, and 194 clone-related bugs can be\nidentified efficiently and accurately based on our small bug database with a\nprecision of 96\\%. SmartEmbed can be accessed at\n\\url{this http URL}. A demo video of SmartEmbed is at\n\\url{this https URL}\n\n    "}, "journals/corr/abs-1911-07404": {"title": "FFDNet-Based Channel Estimation for Massive MIMO Visible Light Communication Systems.", "url": "http://arxiv.org/abs/1911.07404", "year": "2019", "author": {"Zhipeng Gao": "25/2165", "Yuhao Wang 0001": "54/518-1", "Xiaodong Liu 0006": "65/622-6", "Fuhui Zhou": "155/5190", "Kai-Kit Wong": "35/5506"}, "abstract": "\n      Abstract:  Channel estimation is of crucial importance in massive multiple-input\nmultiple-output (m-MIMO) visible light communication (VLC) systems. In order to\ntackle this problem, a fast and flexible denoising convolutional neural network\n(FFDNet)-based channel estimation scheme for m-MIMO VLC systems was proposed.\nThe channel matrix of the m-MIMO VLC channel is identified as a two-dimensional\nnatural image since the channel has the characteristic of sparsity. A deep\nlearning-enabled image denoising network FFDNet is exploited to learn from a\nlarge number of training data and to estimate the m-MIMO VLC channel.\nSimulation results demonstrate that our proposed channel estimation based on\nthe FFDNet significantly outperforms the benchmark scheme based on minimum mean\nsquare error.\n\n    "}, "journals/wc/YuLZFYGGQ18": {"title": "Capacity Enhancement for 5G Networks Using MmWave Aerial Base Stations: Self-Organizing Architecture and Approach.", "url": "https://doi.org/10.1109/MWC.2018.1700393", "year": "2018", "author": {"Peng Yu 0001": "50/2599-1", "Wenjing Li 0001": "08/6548-1", "Fanqin Zhou": "137/4178", "Lei Feng 0001": "76/847-1", "Mengjun Yin": "152/1083", "Shaoyong Guo": "150/6843", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342"}, "abstract": " Abstract:In 5G networks, an mAeBS can overcome on-ground constraints and enable rapid and flexible deployment, providing an ideal solution for CE in hotspot areas. However, how to control multiple mAeBSs for efficient CE is a challenging issue, and we refer to it as mACE. To tame the trivial management practices involved in mACE, we resort to the SON methodology and propose a self-organizing architecture. A closed-loop management process is correspondingly designed for it, which consists of four phases: analyzing, planning, devaluating, and executing. Thereafter, solutions to the key technical issues, including hotspot identification and mAeBS deployment, are investigated. The effects of the overall mACE approach are intuitively illustrated in a small-scale example case. A few more challenging issues related to mACE are also identified for future research."}, "journals/wicomm/WangGNYQ18": {"title": "An Efficient Forwarding Capability Evaluation Method for Opportunistic Offloading in Mobile Edge Computing.", "url": "https://doi.org/10.1155/2018/4801465", "year": "2018", "author": {"Qian Wang 0015": "75/5723-15", "Zhipeng Gao": "25/2165", "Kun Niu": "14/1289", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342"}, "abstract": ""}, "journals/access/LiYQGG17": {"title": "Gravitation-Based 3-D Redeployment Schemes for the Mobile Sensors and Sink in Gas Leakage Monitoring.", "url": "https://doi.org/10.1109/ACCESS.2017.2695232", "year": "2017", "author": {"He Li 0004": "05/4746-4", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165", "Ma Guizhen": "180/9659"}, "abstract": " Abstract:Gas leakage is one of the most frequent types of accidents in the petrochemical industry. It is imperative to use mobile sensors to monitor such an accident area. The new and effective way is to use drones and helicopters that spray wireless sensors from the air to monitor harmful gases and to locate the gas leaking source. However, the sprayed wireless sensors will be distributed randomly around the accident area, and it is a challenge to obtain effective coverage. This paper proposes a gravitation-based redeployment algorithm for sensors (GRSS) that considers the virtual boundary forces and the gas concentration in a 3-D accident monitoring area. A priority-based redeployment algorithm for sensors (PRSS) is proposed to further improve the coverage and simplify the 3-D redeployment problem. PRSS considers the layer priorities of the monitoring area to control the movements of the mobile sensors. The simulation results show that the GRSS and PRSS methods can achieve better coverage and utilize less distance compared with the random algorithm and 3-D self-deployment."}, "journals/tkde/NieWZWGY17": {"title": "Data-Driven Answer Selection in Community QA Systems.", "url": "https://doi.org/10.1109/TKDE.2017.2669982", "year": "2017", "author": {"Liqiang Nie": "92/8277", "Xiaochi Wei": "131/2938", "Dongxiang Zhang": "89/6013", "Xiang Wang 0010": "31/2864-10", "Zhipeng Gao": "25/2165", "Yi Yang 0001": "33/4854-1"}, "abstract": " Abstract:Finding similar questions from historical archives has been applied to question answering, with well theoretical underpinnings and great practical success. Nevertheless, each question in the returned candidate pool often associates with multiple answers, and hence users have to painstakingly browse a lot before finding the correct one. To alleviate such problem, we present a novel scheme to rank answer candidates via pairwise comparisons. In particular, it consists of one offline learning component and one online search component. In the offline learning component, we first automatically establish the positive, negative, and neutral training samples in terms of preference pairs guided by our data-driven observations. We then present a novel model to jointly incorporate these three types of training samples. The closed-form solution of this model is derived. In the online search component, we first collect a pool of answer candidates for the given question via finding its similar questions. We then sort the answer candidates by leveraging the offline trained model to judge the preference orders. Extensive experiments on the real-world vertical and general community-based question answering datasets have comparatively demonstrated its robustness and promising performance. Also, we have released the codes and data to facilitate other researchers."}, "journals/ijdsn/LiYQGG16": {"title": "Cooperative Downloading in Mobile Ad Hoc Networks: A Cost-Energy Perspective.", "url": "https://doi.org/10.1155/2016/3028642", "year": "2016", "author": {"He Li 0004": "05/4746-4", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165", "Ma Guizhen": "180/9659"}, "abstract": ""}, "journals/ijdsn/GuizhenYQG16": {"title": "Obstacle Aware Connectivity Restoration for Disjoint Wireless Sensor Networks Using a Mix of Stationary and Mobile Nodes.", "url": "https://doi.org/10.1155/2016/6469341", "year": "2016", "author": {"Ma Guizhen": "180/9659", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342", "Zhipeng Gao": "25/2165"}, "abstract": ""}, "journals/corr/WangGNYQ16": {"title": "A Time-constraint Satisfying and Cost-reducing node evaluation metric for Message Routing in Mobile Crowd Sensing Networks.", "url": "http://arxiv.org/abs/1606.08936", "year": "2016", "author": {"Qian Wang 0015": "75/5723-15", "Zhipeng Gao": "25/2165", "Kun Niu": "14/1289", "Yang Yang 0006": "48/450-6", "Xuesong Qiu 0001": "13/342"}, "abstract": "\n      Abstract:  In mobile crowd sensing networks data forwarding through opportunistic\ncontacts between participants. Data is replicated to encountered participants.\nFor optimizing data delivery ratio and reducing redundant data a lot of data\nforwarding schemes, which selectively replicate data to encountered\nparticipants through node's data forwarding metric are proposed. However most\nof them neglect a kind of redundant data whose Time-To-Live is expired. For\nreducing this kind of redundant data we proposed a new method to evaluate\nnode's data forwarding metric, which is used to measure the node's probability\nof forwarding data to destination within data's constraint time. The method is\ndivided into two parts. The first is evaluating nodes whether have possibility\nto contact destination within time constraint based on transient cluster. We\npropose a method to detect node's transient cluster, which is based on node's\ncontact rate. Only node, which has possibility to contact destination, has\nchances to the second step. It effectively reduces the computational\ncomplexity. The second is calculating data forwarding probability of node to\ndestination according to individual ICT (inter contact time) distribution.\nEvaluation results show that our proposed transient cluster detection method is\nmore simple and quick. And from two aspects of data delivery ratio and network\noverhead our approach outperforms other existing data forwarding approach.\n\n    "}, "journals/ejwcn/YangLGQRL15": {"title": "A data dissemination mechanism for motorway environment in VANETs.", "url": "https://doi.org/10.1186/s13638-015-0310-9", "year": "2015", "author": {"Yang Yang 0006": "48/450-6", "Qian Liu": "33/85", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Lanlan Rui": "03/4227", "Xin Li": "09/1365"}, "abstract": ""}, "journals/ijdsn/YangGQLHZ15": {"title": "A Hierarchical Reputation Evidence Decision System in VANETs.", "url": "https://doi.org/10.1155/2015/341579", "year": "2015", "author": {"Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Qian Liu": "33/85", "Yuwen Hao": "149/8368", "Jingchen Zheng": "149/8447"}, "abstract": ""}, "journals/ijdsn/GaoCQM15": {"title": "A Missing Sensor Data Estimation Algorithm Based on Temporal and Spatial Correlation.", "url": "https://doi.org/10.1155/2015/435391", "year": "2015", "author": {"Zhipeng Gao": "25/2165", "Weijing Cheng": "176/0909", "Xuesong Qiu 0001": "13/342", "Luoming Meng": "52/382"}, "abstract": ""}, "journals/sensors/0006LGQM15": {"title": "Data Fault Detection in Medical Sensor Networks.", "url": "https://doi.org/10.3390/s150306066", "year": "2015", "author": {"Yang Yang 0006": "48/450-6", "Qian Liu": "33/85", "Zhipeng Gao": "25/2165", "Xuesong Qiu 0001": "13/342", "Luoming Meng": "52/382"}, "abstract": ""}, "journals/sensors/0006G0Q14": {"title": "An Uncertainty-Based Distributed Fault Detection Mechanism for Wireless Sensor Networks.", "url": "https://doi.org/10.3390/s140507655", "year": "2014", "author": {"Yang Yang 0006": "48/450-6", "Zhipeng Gao": "25/2165", "Hang Zhou 0002": "26/3707-2", "Xuesong Qiu 0001": "13/342"}, "abstract": ""}}}